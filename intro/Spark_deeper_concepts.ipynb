{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Deeper Concepts\n",
    "© Explore Data Science Academy\n",
    "\n",
    "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/f7a433fa65521cca0016a42da6cffbebc6e65c40/data_engineering/transform/spark_logo.png?raw=True\"\n",
    "     alt=\"Spark Diagram\"\n",
    "     style=\"padding-bottom=0.5em\"\n",
    "     width=600px/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "In this train, you will learn how to:\n",
    "\n",
    "- understand and use SparkSQL to manipulate datasets;\n",
    "- understand and use unions, joins, windowing and other modifications in Spark;\n",
    "- manipulate complex data types using built-in Spark functions; \n",
    "- apply UDF and Pandas UDFs to datasets; and\n",
    "- have knowledge of JDBC and SQL Databases with Spark.\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "By now, you should be quite familiar with Spark, the benefits of using in-memory processing, and be able to do quite a few transformations using the PySpark implementation of Apache Spark. \n",
    "\n",
    "In this train, we go deeper. We move past only using the PySpark interface, and go one level of abstraction lower – the Spark SQL API – to directly interface with Spark. Using Spark SQL, it is possible to perform transformations commonly available in SQL, like joins, unions, and working with complex data types. These transformations are very powerful, and because they're being implemented in Spark SQL, they are very fast and computationally efficient. \n",
    "\n",
    "Using Spark SQL is sometimes not sufficient, and we want to perform more complex or custom transformations on our data. In this case, Spark provides us with user-defined functions, which are functions defined programmatically in Python (or another Spark-compatible language). These functions can combine columns in a unique way, perform complex mathematical operations, or use logic to manipulate the dataset. \n",
    "\n",
    "Finally, we look at how we can use Spark to output data to databases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dependencies\n",
    "\n",
    "To start, let's set up Spark and read in the Bitcoin dataset.\n",
    "\n",
    "The dataset can be accessed by downloading the accompanying zip folder. \n",
    "The file to access is: `spark_deeper_concepts_transformed_data.snappy.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the below cell, make sure that you download and unzip the required file and place it in your current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_data = spark.read.parquet('./spark_deeper_concepts_transformed_data.snappy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- volume_btc: float (nullable = true)\n",
      " |-- volume_currency: float (nullable = true)\n",
      " |-- weighted_price: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bitcoin_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+----+----+-----+----------+---------------+--------------+\n",
      "|timestamp          |open|high|low |close|volume_btc|volume_currency|weighted_price|\n",
      "+-------------------+----+----+----+-----+----------+---------------+--------------+\n",
      "|2011-12-31 09:52:00|4.39|4.39|4.39|4.39 |0.45558086|2.0            |4.39          |\n",
      "|2011-12-31 09:53:00|NaN |NaN |NaN |NaN  |NaN       |NaN            |NaN           |\n",
      "|2011-12-31 09:54:00|NaN |NaN |NaN |NaN  |NaN       |NaN            |NaN           |\n",
      "|2011-12-31 09:55:00|NaN |NaN |NaN |NaN  |NaN       |NaN            |NaN           |\n",
      "|2011-12-31 09:56:00|NaN |NaN |NaN |NaN  |NaN       |NaN            |NaN           |\n",
      "|2011-12-31 09:57:00|NaN |NaN |NaN |NaN  |NaN       |NaN            |NaN           |\n",
      "|2011-12-31 09:58:00|NaN |NaN |NaN |NaN  |NaN       |NaN            |NaN           |\n",
      "|2011-12-31 09:59:00|NaN |NaN |NaN |NaN  |NaN       |NaN            |NaN           |\n",
      "|2011-12-31 10:00:00|NaN |NaN |NaN |NaN  |NaN       |NaN            |NaN           |\n",
      "|2011-12-31 10:01:00|NaN |NaN |NaN |NaN  |NaN       |NaN            |NaN           |\n",
      "+-------------------+----+----+----+-----+----------+---------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bitcoin_data.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SparkSQL\n",
    "*Spark SQL is a base layer in Spark which we can use to interface with data. It provides a similar interface and level of abstraction as the Python API, but in ANSI-compliant SQL.*\n",
    "\n",
    "Spark SQL has various advantages:\n",
    "\n",
    "1. It provides an engine on which higher-level APIs are built.\n",
    "2. It can read and write to many structured formats (for example, JSON, Hive tables, Parquet, Avro, ORC, and CSV).\n",
    "3. It can connect to databases using ODBC/JDBC connectors or other systems such as PowerBI, Tableau, Talend, or from RDBMSs such as MySQL or PostgreSQL.\n",
    "4. It offers an interactive shell to issue SQL queries on your structured data.\n",
    "5. It supports ANSI SQL:2003-compliant commands and HiveQL.\n",
    "\n",
    "As mentioned already, the `SparkSession` is the main entry point into programming Spark with the Structured APIs. To run a SQL query, use the `sql()` method on the `SparkSession` instance, `spark`. For example, `spark.sql(\"SELECT * FROM table\")`.\n",
    "\n",
    "### Starting out – basic queries\n",
    "\n",
    "In this train, we'll be using the Bitcoin dataset which we've worked with previously. Here we aim to do some basic exploration and manipulation of the data in Spark SQL.\n",
    "\n",
    "Before we can do this, we need to create a temporary table. Temporary tables and views can be created using the `createOrReplaceTempView()` method on the DataFrame, taking one argument – the name of the temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_data.createOrReplaceTempView('bitcoin_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can execute SQL queries on the table. \n",
    "\n",
    "As a demonstration for this functionality, let's write a brief query that returns the `timestamp`, `high`, and `low` fields for entries that have a `high` value larger than 5000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+\n",
      "|          timestamp|    high|     low|\n",
      "+-------------------+--------+--------+\n",
      "|2020-12-31 02:00:00|28928.49|28893.21|\n",
      "|2020-12-31 01:59:00|28911.52| 28867.6|\n",
      "|2020-12-31 01:58:00|28900.52|28850.49|\n",
      "|2020-12-31 01:57:00| 28863.9|28829.42|\n",
      "|2020-12-31 01:56:00|28829.42|28785.64|\n",
      "|2020-12-31 01:55:00| 28825.5|28800.01|\n",
      "|2020-12-31 01:54:00|28832.79| 28800.0|\n",
      "|2020-12-31 01:53:00|28822.71| 28800.0|\n",
      "|2020-12-31 01:52:00|28844.25|28816.09|\n",
      "|2020-12-31 01:51:00|28849.67|28807.78|\n",
      "+-------------------+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT timestamp, high, low\n",
    "          FROM bitcoin_tbl \n",
    "          WHERE high > 5000\n",
    "          ORDER BY timestamp DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has a very familiar SQL-like interface, and queries are simple and easy to write if you are well versed in SQL.\n",
    "Here we:\n",
    " - used the `SELECT` command to define which fields to select; \n",
    " - used the `FROM` command to define which table to select from; \n",
    " - used the `WHERE` command to define a condition to select; and \n",
    " - used the `ORDER BY` command to define which field to order on and the `DESC` command to define that order should be descending.\n",
    "\n",
    "Note that we call the `.show()` method to limit the number of printed results in our notebook to 10.\n",
    "\n",
    "To try another query, let's look at the timestamps (minute resolution) for days in which there was no Bitcoin trading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------+---------------+-------+\n",
      "|          timestamp|   high|    low|volume_currency|  close|\n",
      "+-------------------+-------+-------+---------------+-------+\n",
      "|2017-08-14 03:08:00|4068.49|4068.49|            0.0|4068.49|\n",
      "|2014-03-25 11:50:00| 573.91| 573.91|            0.0| 573.91|\n",
      "|2014-01-09 08:37:00| 834.89| 834.89|            0.0| 834.89|\n",
      "|2013-10-03 12:02:00| 110.52| 110.52|            0.0| 110.52|\n",
      "|2013-07-04 08:58:00|  74.42|  74.42|            0.0|  74.42|\n",
      "|2013-06-30 16:40:00|   89.2|   89.2|            0.0|   89.2|\n",
      "|2013-06-27 11:13:00|   98.4|   98.4|            0.0|   98.4|\n",
      "|2013-06-26 22:50:00|  99.17|  99.17|            0.0|  99.17|\n",
      "|2013-06-24 22:36:00|  98.88|  98.88|            0.0|  98.88|\n",
      "|2013-06-22 21:39:00| 101.16| 101.16|            0.0| 101.16|\n",
      "+-------------------+-------+-------+---------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's select all days where there was no trading.\n",
    "\n",
    "spark.sql(\"\"\"SELECT timestamp, high, low, volume_currency, close\n",
    "          FROM bitcoin_tbl WHERE volume_currency <= 0\n",
    "          ORDER BY timestamp DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last time no Bitcoin traded was back in 2017, and only for one minute. It seems to have been more common before 2017. However, let's look at the total number of times (minutes) on which there was no trading using the `COUNT` command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Occurrences|\n",
      "+-----------+\n",
      "|         16|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's use a count of all the days where there was no trading.\n",
    "\n",
    "spark.sql(\"\"\"SELECT COUNT(*) as Occurrences\n",
    "          FROM bitcoin_tbl WHERE volume_currency <= 0\n",
    "          \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\[Exercise 1\\]\n",
    "\n",
    "As an exercise, using Spark SQL, select all entries from the bitcoin_tbl that have a close value larger than 10 000 USD.\n",
    "\n",
    "> **Note** 💡\n",
    " >\n",
    " > Spark has a special way of handling **NaNs** while performing computations. Have a look at the [NaN semantics](https://spark.apache.org/docs/3.0.0-preview/sql-ref-nan-semantics.html) provided by Spark.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing complexity – using additional SQL functionality\n",
    "\n",
    "Next, we create more complex SQL statements, using `CASE`.\n",
    "\n",
    "CASE in SQL defines a list of `WHEN` statements to go through and executes the true condition.\n",
    "This is very similar to an if-then-else conditional found in most programming languages. \n",
    "\n",
    "Here we want to define (somewhat arbitrary) categories for when to either sell or acquire Bitcoin based on the open value for Bitcoin on a specific day. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use CASE with multiple WHEN statements to define some categories.\n",
    "\n",
    "bitcoin_advice = spark.sql(\"\"\"SELECT timestamp, high, low, open,\n",
    "            CASE\n",
    "            WHEN open > 19000 THEN 'Sell now'\n",
    "            WHEN open > 17000 THEN 'You should really sell'\n",
    "            WHEN open > 15000 THEN 'Should have already sold'\n",
    "            WHEN open > 0 THEN 'Buy NOW!'\n",
    "            WHEN open = 0 THEN 'Lost it all!'\n",
    "            ELSE 'Something is wrong here...'\n",
    "            END AS sell_guidance\n",
    "            FROM bitcoin_tbl\n",
    "            ORDER BY timestamp DESC\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the spark.sql() method returns a spark DataFrame, which has methods available like `show()` or other operators, for instance, creating new views. \n",
    "\n",
    "Before we can run more Spark SQL commands on the DataFrame we just created above, we have to create a view from the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_advice.createOrReplaceTempView('bitcoin_advice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+--------+-------------+\n",
      "|timestamp          |high    |low     |open    |sell_guidance|\n",
      "+-------------------+--------+--------+--------+-------------+\n",
      "|2020-12-31 02:00:00|28928.49|28893.21|28893.21|Sell now     |\n",
      "|2020-12-31 01:59:00|28911.52|28867.6 |28910.54|Sell now     |\n",
      "|2020-12-31 01:58:00|28900.52|28850.49|28850.49|Sell now     |\n",
      "|2020-12-31 01:57:00|28863.9 |28829.42|28829.42|Sell now     |\n",
      "|2020-12-31 01:56:00|28829.42|28785.64|28801.47|Sell now     |\n",
      "|2020-12-31 01:55:00|28825.5 |28800.01|28809.07|Sell now     |\n",
      "|2020-12-31 01:54:00|28832.79|28800.0 |28800.0 |Sell now     |\n",
      "|2020-12-31 01:53:00|28822.71|28800.0 |28814.36|Sell now     |\n",
      "|2020-12-31 01:52:00|28844.25|28816.09|28826.49|Sell now     |\n",
      "|2020-12-31 01:51:00|28849.67|28807.78|28836.97|Sell now     |\n",
      "+-------------------+--------+--------+--------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT *\n",
    "          FROM bitcoin_advice\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the Python API, we can aggregate in Spark SQL using the `GROUP BY` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------+\n",
      "|sell_guidance           |count(high)|\n",
      "+------------------------+-----------+\n",
      "|Sell now                |43895      |\n",
      "|Should have already sold|17811      |\n",
      "|Buy NOW!                |437267     |\n",
      "|You should really sell  |25307      |\n",
      "+------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping by the categories created above, and aggregating by count.\n",
    "\n",
    "spark.sql(\"\"\"SELECT sell_guidance, COUNT(high)\n",
    "          FROM bitcoin_advice\n",
    "          WHERE timestamp > '2020-01-02'\n",
    "          GROUP BY sell_guidance\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there were numerous occasions when you should have bought in 2020, with the value being below the threshold, but also ample times when you could have made a buck by selling. At least it looks like the price never dipped below 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark SQL interface undergoes the same optimisations as the Python, Scala, and other APIs, and you can effectively perform all of the transformations and operations defined in the previous chapter. You can also perform all operations that you normally write in SQL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\[Exercise 2\\]\n",
    "\n",
    "As an exercise, using the Spark SQL filter for dates, which month had the most occurrences when it was ideal to buy Bitcoin in 2018, according to the above criteria?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables and views\n",
    "\n",
    "Tables hold data, but each table also has associated metadata, which includes information on the table schema, description, table name, database name, column names, partitions, physical location, and more. This is by default stored in a Hive metastore, located at `/user/hive/warehouse` within the host file system. You can change the location of this metastore to an external metastore to leverage more advanced features of Spark and Hive. Generally, such an external metastore will be hosted centrally, where multiple teams can use it.  Many examples exist to implement both on [Microsoft Azure](https://docs.microsoft.com/en-us/azure/databricks/data/metastores/external-hive-metastore) and [AWS](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html).\n",
    "\n",
    "It is also important to know that Spark has two types of tables: **managed** and **unmanaged**. \n",
    "\n",
    "*Managed* tables have their metadata managed by Spark, as well as the data in the file store. This could be locally in HDFS or in an object store such as Azure Storage or Amazon S3. In *unmanaged*, Spark only manages the metadata, while you are responsible for managing the data storage and location yourself.\n",
    "\n",
    "The command `DROP TABLE tbl` will drop the table data and metadata with a managed table, whereas the same command will only delete the metadata with an unmanaged table.\n",
    "\n",
    "Managed tables are created by using the `saveAsTable()` method, whereas unmanaged tables include the `path` option.\n",
    "\n",
    "As you have seen, you can create a view using the `createOrReplaceTempView()` method. This creates a view that will exist as long as the SparkSession is active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_advice.createOrReplaceTempView('bitcoin_advice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databases\n",
    "\n",
    "All tables have to reside within a database. By default, Spark creates tables under a `default` schema, but databases can also be manually created and specified. \n",
    "\n",
    "This will mean that most novice users of Spark would not be aware of databases, and may be ignorant of the concept.\n",
    "\n",
    "Users of Databricks may already be very familiar with this concept:\n",
    "\n",
    "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/65a436ec1393464b511f3fff4e274f035eb503a1/data_engineering/transform/spark_deeper_concepts/default_database.png?raw=True\"\n",
    "     alt=\"Default Database\"\n",
    "     style=\"padding-bottom=1em\"\n",
    "     width=400px\n",
    "     />\n",
    "     <p><em>Databases and tables Microsoft documentation <a href=\"https://docs.microsoft.com/en-us/azure/databricks/data/tables\">here</a>.</em></p>\n",
    "</div>\n",
    "<br/><br/>\n",
    "The following commands create and set which schema to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Spark, it's called a DATABASE, but this is equivalent to schemas in relational databases.\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bitcoin\")\n",
    "spark.sql(\"USE bitcoin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a managed table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_advice.write.saveAsTable('bitcoin_advice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create the table `bitcoin_advice` in the database `bitcoin`. \n",
    "\n",
    "You can find this table in:\n",
    "\n",
    "`$WORKING_DIR/spark-warehouse/bitcoin.db/bitcoin_advice/`\n",
    "\n",
    "The table will be stored as a collection of Parquet files in this directory. Spark will save the table as a collection of Parquet files when working with managed tables, irrespective of the format from which the data were read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing metadata\n",
    "\n",
    "As mentioned, Spark stores lots of information on the databases and tables that are saved. Let's have a look at the databases, tables, and other metadata that are stored in Spark.\n",
    "\n",
    "First, we use the `listDatabases()` method to list the databases created within our Spark environment. Leading on from above, we expect only the `default` and `bitcoin` databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment `spark.catalog.listDatabases()` in the cell below to run to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databases:\n",
    "#spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we list all the tables that we have created within our environment. If we do not pass an argument to the method, it will list all the tables within the environment.  Alternatively, you can pass a database name as an argument, in which case only the tables for that database will be listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='bitcoin_advice', database='bitcoin', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='bitcoin_advice', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='bitcoin_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tables:\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can dive deeper into specific tables as well by looking into the columns of a specific table, using the `listColumns()` method, with the single argument being the name of the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='timestamp', description=None, dataType='timestamp', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='high', description=None, dataType='float', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='low', description=None, dataType='float', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='open', description=None, dataType='float', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='sell_guidance', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having a look at the column metadata stored for a specific table:\n",
    "spark.catalog.listColumns(\"bitcoin_advice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now have a look at all the properties of the created table. We do this by using the `DESCRIBE` command, adding the `FORMATTED` command to return the formatted detail on the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|           timestamp|           timestamp|   null|\n",
      "|                high|               float|   null|\n",
      "|                 low|               float|   null|\n",
      "|                open|               float|   null|\n",
      "|       sell_guidance|              string|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             bitcoin|       |\n",
      "|               Table|      bitcoin_advice|       |\n",
      "|        Created Time|Fri Oct 08 13:28:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.1.2|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/Users/maddy...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE FORMATTED bitcoin.bitcoin_advice').show(25, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great! \n",
    "\n",
    "\n",
    "Information returned on the various fields in the table include:\n",
    "- which database it belongs to;\n",
    "- when it was created; \n",
    "- who created it (in this case Spark); \n",
    "- its format (Provider);  and \n",
    "- the location where it is stored.\n",
    "\n",
    "\n",
    "Let's have a look to see if we have any additional metadata on the table.\n",
    "\n",
    "Additional metadata for tables are stored in the table's `TBLPROPERTIES` (referred to in the above table as `Table Properties`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW TBLPROPERTIES bitcoin.bitcoin_advice').show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not look like it.  \n",
    "So, let’s add some:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties = {'description': 'This is a table that is used to determine whether you should buy Bitcoin or not', \n",
    "              'created_by': 'John Smith'}\n",
    "spark.sql(f'ALTER TABLE bitcoin.bitcoin_advice SET TBLPROPERTIES (PROPERTIES = \"{properties}\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'This is a table that is used to determine whether you should buy Bitcoin or not',\n",
       " 'created_by': 'John Smith'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props = spark.sql('SHOW TBLPROPERTIES bitcoin.bitcoin_advice (PROPERTIES)').collect()[0]['value']\n",
    "eval(props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s consider everything that happened here.\n",
    "\n",
    "We first retrieve the table properties again using the `TBLPROPERTIES` attribute of the table – this time retrieving `PROPERTIES`. We then `collect` the results from the returned DataFrame and retrieve the first value `[0]` (we know there will only be one entry since the keys stored in `TBLPROPERTIES` are unique), finally retrieving the `value` and evaluating for proper formatting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL-like operations\n",
    "*Understand and use unions, joins, windowing, and other modifications in Spark.*\n",
    "\n",
    "Spark also allows SQL-like operations. \n",
    "\n",
    "These include:\n",
    "- unions (sometimes called concatenation); \n",
    "- joins (you should know these quite well); \n",
    "- windowing (a cool new method which is between aggregation and single-row operations); and\n",
    "- other operations that you would normally perform in SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some mock datasets to play with. We are going to use this to practise joining, \n",
    "# create window functions, and concatenate tables.\n",
    "# We create a city info table that describes some static features of cities in South Africa.\n",
    "\n",
    "city_info = spark.createDataFrame([\n",
    "    (0, 'Johannesburg', '2001', '011', 1886, False),\n",
    "    (1, 'Pretoria', '0001', '012', 1855, False),\n",
    "    (2, 'Durban', '4001', '031', 1880, True),\n",
    "    (3, 'Cape Town', '8000', '021', 1652, True),\n",
    "    (4, 'Port Elizabeth', '6001', '041', 1820, True),\n",
    "    (5, 'Bloemfontein', '9300', '051', 1846, False),\n",
    "    (6, 'Kimberley', '8301', '053', 1873, False),\n",
    "    (7, 'East London', '5200', '043', 1847, True),\n",
    "], ['index', 'city', 'postal_code', 'area_code', 'established', 'coastal'])\n",
    "\n",
    "# Let's create a city data table which contains data form cities from a recent census\n",
    "city_data = spark.createDataFrame([\n",
    "    (0, 'Johannesburg', 10500000, 76000000, 1644.98),\n",
    "    (1, 'Pretoria', 2921612, 75600000, 687.54),\n",
    "    (2, 'Durban', 3442361, 83900000, 225.91),\n",
    "    (3, 'Cape Town', 3740026, 78700000, 2461.0),\n",
    "    (4, 'Port Elizabeth', 1152915, 45600000, 251.03),\n",
    "    (5, 'Bloemfontein', 747431, 15600000, 236.17),\n",
    "    (6, 'Kimberley', 225160, 460000, 164.3),\n",
    "    (7, 'East London', 755200, 23400000, 168.86),\n",
    "], ['index', 'city', 'population', 'gdp', 'area'])\n",
    "\n",
    "city_info.createOrReplaceTempView('city_info')\n",
    "city_data.createOrReplaceTempView('city_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+-----------+---------+-----------+-------+\n",
      "|index|          city|postal_code|area_code|established|coastal|\n",
      "+-----+--------------+-----------+---------+-----------+-------+\n",
      "|    0|  Johannesburg|       2001|      011|       1886|  false|\n",
      "|    1|      Pretoria|       0001|      012|       1855|  false|\n",
      "|    2|        Durban|       4001|      031|       1880|   true|\n",
      "|    3|     Cape Town|       8000|      021|       1652|   true|\n",
      "|    4|Port Elizabeth|       6001|      041|       1820|   true|\n",
      "|    5|  Bloemfontein|       9300|      051|       1846|  false|\n",
      "|    6|     Kimberley|       8301|      053|       1873|  false|\n",
      "|    7|   East London|       5200|      043|       1847|   true|\n",
      "+-----+--------------+-----------+---------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting data from city info.\n",
    "spark.sql(\"SELECT * FROM city_info\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+----------+--------+-------+\n",
      "|index|          city|population|     gdp|   area|\n",
      "+-----+--------------+----------+--------+-------+\n",
      "|    0|  Johannesburg|  10500000|76000000|1644.98|\n",
      "|    1|      Pretoria|   2921612|75600000| 687.54|\n",
      "|    2|        Durban|   3442361|83900000| 225.91|\n",
      "|    3|     Cape Town|   3740026|78700000| 2461.0|\n",
      "|    4|Port Elizabeth|   1152915|45600000| 251.03|\n",
      "|    5|  Bloemfontein|    747431|15600000| 236.17|\n",
      "|    6|     Kimberley|    225160|  460000|  164.3|\n",
      "|    7|   East London|    755200|23400000| 168.86|\n",
      "+-----+--------------+----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting data from city data.\n",
    "spark.sql(\"SELECT * FROM city_data\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union\n",
    "\n",
    "Union, as you are aware, is the joining of two tables with the same schema. \n",
    "\n",
    "Let's duplicate and identify the `city_info` table:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are adding a column to the city_info table here, assigning it a value of 1 or 2.\n",
    "\n",
    "city_info_1 = city_info.withColumn('origin', F.lit(1))\n",
    "city_info_2 = city_info.withColumn('origin', F.lit(2))\n",
    "\n",
    "# Remember that Spark queries always return a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_info_1.createOrReplaceTempView('city_info_1')\n",
    "city_info_2.createOrReplaceTempView('city_info_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+-----------+---------+-----------+-------+------+\n",
      "|index|city          |postal_code|area_code|established|coastal|origin|\n",
      "+-----+--------------+-----------+---------+-----------+-------+------+\n",
      "|0    |Johannesburg  |2001       |011      |1886       |false  |1     |\n",
      "|0    |Johannesburg  |2001       |011      |1886       |false  |2     |\n",
      "|4    |Port Elizabeth|6001       |041      |1820       |true   |1     |\n",
      "|2    |Durban        |4001       |031      |1880       |true   |1     |\n",
      "|7    |East London   |5200       |043      |1847       |true   |2     |\n",
      "|3    |Cape Town     |8000       |021      |1652       |true   |1     |\n",
      "|4    |Port Elizabeth|6001       |041      |1820       |true   |2     |\n",
      "|1    |Pretoria      |0001       |012      |1855       |false  |2     |\n",
      "|7    |East London   |5200       |043      |1847       |true   |1     |\n",
      "|2    |Durban        |4001       |031      |1880       |true   |2     |\n",
      "|5    |Bloemfontein  |9300       |051      |1846       |false  |1     |\n",
      "|1    |Pretoria      |0001       |012      |1855       |false  |1     |\n",
      "|6    |Kimberley     |8301       |053      |1873       |false  |2     |\n",
      "|5    |Bloemfontein  |9300       |051      |1846       |false  |2     |\n",
      "|6    |Kimberley     |8301       |053      |1873       |false  |1     |\n",
      "|3    |Cape Town     |8000       |021      |1652       |true   |2     |\n",
      "+-----+--------------+-----------+---------+-----------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM city_info_1 UNION SELECT * FROM city_info_2\").show(18, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same using the PySpark API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+-----------+---------+-----------+-------+------+\n",
      "|index|city          |postal_code|area_code|established|coastal|origin|\n",
      "+-----+--------------+-----------+---------+-----------+-------+------+\n",
      "|0    |Johannesburg  |2001       |011      |1886       |false  |1     |\n",
      "|1    |Pretoria      |0001       |012      |1855       |false  |1     |\n",
      "|2    |Durban        |4001       |031      |1880       |true   |1     |\n",
      "|3    |Cape Town     |8000       |021      |1652       |true   |1     |\n",
      "|4    |Port Elizabeth|6001       |041      |1820       |true   |1     |\n",
      "|5    |Bloemfontein  |9300       |051      |1846       |false  |1     |\n",
      "|6    |Kimberley     |8301       |053      |1873       |false  |1     |\n",
      "|7    |East London   |5200       |043      |1847       |true   |1     |\n",
      "|0    |Johannesburg  |2001       |011      |1886       |false  |2     |\n",
      "|1    |Pretoria      |0001       |012      |1855       |false  |2     |\n",
      "|2    |Durban        |4001       |031      |1880       |true   |2     |\n",
      "|3    |Cape Town     |8000       |021      |1652       |true   |2     |\n",
      "|4    |Port Elizabeth|6001       |041      |1820       |true   |2     |\n",
      "|5    |Bloemfontein  |9300       |051      |1846       |false  |2     |\n",
      "|6    |Kimberley     |8301       |053      |1873       |false  |2     |\n",
      "|7    |East London   |5200       |043      |1847       |true   |2     |\n",
      "+-----+--------------+-----------+---------+-----------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrames just created, we can perform a union.\n",
    "\n",
    "city_info_1.union(city_info_2).show(18, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above syntax is straightforward and resembles the syntax used in Pandas.\n",
    "\n",
    "It is also clear that we joined tables that are from separate origins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins\n",
    "\n",
    "It is a common operation to join two tables together. By default, Spark performs an inner join, with other options, including `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`, `right_outer`, `left_semi`, and `left_anti`.\n",
    "\n",
    "Let's join the city info and data tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-----------+---------+-----------+-------+-----+------------+----------+--------+-------+\n",
      "|index|city        |postal_code|area_code|established|coastal|index|city        |population|gdp     |area   |\n",
      "+-----+------------+-----------+---------+-----------+-------+-----+------------+----------+--------+-------+\n",
      "|1    |Pretoria    |0001       |012      |1855       |false  |1    |Pretoria    |2921612   |75600000|687.54 |\n",
      "|2    |Durban      |4001       |031      |1880       |true   |2    |Durban      |3442361   |83900000|225.91 |\n",
      "|3    |Cape Town   |8000       |021      |1652       |true   |3    |Cape Town   |3740026   |78700000|2461.0 |\n",
      "|0    |Johannesburg|2001       |011      |1886       |false  |0    |Johannesburg|10500000  |76000000|1644.98|\n",
      "|6    |Kimberley   |8301       |053      |1873       |false  |6    |Kimberley   |225160    |460000  |164.3  |\n",
      "+-----+------------+-----------+---------+-----------+-------+-----+------------+----------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM city_info ci JOIN city_data cd ON ci.city = cd.city \").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can do the same in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----------+---------+-----------+-------+-----+----------+--------+-------+\n",
      "|city        |index|postal_code|area_code|established|coastal|index|population|gdp     |area   |\n",
      "+------------+-----+-----------+---------+-----------+-------+-----+----------+--------+-------+\n",
      "|Pretoria    |1    |0001       |012      |1855       |false  |1    |2921612   |75600000|687.54 |\n",
      "|Durban      |2    |4001       |031      |1880       |true   |2    |3442361   |83900000|225.91 |\n",
      "|Cape Town   |3    |8000       |021      |1652       |true   |3    |3740026   |78700000|2461.0 |\n",
      "|Johannesburg|0    |2001       |011      |1886       |false  |0    |10500000  |76000000|1644.98|\n",
      "|Kimberley   |6    |8301       |053      |1873       |false  |6    |225160    |460000  |164.3  |\n",
      "+------------+-----+-----------+---------+-----------+-------+-----+----------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_cities = city_info.join(city_data, on='city')\n",
    "joined_cities.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's just as easy as joining data in Pandas.  Also, note that the column you specify to join on will only be represented once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing\n",
    "\n",
    "A windowing function takes values from rows in a window (which is a limited number of rows) and then returns values, typically as a row. Note that, while taking in a set of rows, it is still possible to return a single value. \n",
    "\n",
    "\n",
    "Windowing may require that we explain it in a bit more detail. Initially, Spark supported two types of transformations: \n",
    "- Functions that take a single row as input and then generate a single value as an output. \n",
    "- Functions that perform aggregations, take in a group of rows, and then calculate a single return value for each of the groups.\n",
    "\n",
    "\n",
    "This left us with a group of operations not yet possible in Spark, operating on a group of rows but then returning a different value for each group of rows. Possible examples include moving averages, ranking groups, cumulative sums, and creating lags in time series.\n",
    "\n",
    "\n",
    "The window function calculates a value for each row in a `Frame` (the group defined for the window).\n",
    "\n",
    "We gain access to the `Window` function through the Spark SQL interface:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate its functionality with a couple of examples:\n",
    "\n",
    "#### Ranking\n",
    "\n",
    "First, let’s use the rank function. Here we use `dense_rank()` to rank the cities by population, windowed by if they are coastal or not.  \n",
    "\n",
    "Read more about the dense_rank vs rank [here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=window#pyspark.sql.functions.dense_rank).\n",
    "\n",
    "Let's first implement it in Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_cities.createOrReplaceTempView(\"joined_cities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-----------+---------+-----------+-------+-----+----------+--------+-------+-------------+\n",
      "|          city|index|postal_code|area_code|established|coastal|index|population|     gdp|   area|pop_rank_area|\n",
      "+--------------+-----+-----------+---------+-----------+-------+-----+----------+--------+-------+-------------+\n",
      "|     Cape Town|    3|       8000|      021|       1652|   true|    3|   3740026|78700000| 2461.0|            1|\n",
      "|        Durban|    2|       4001|      031|       1880|   true|    2|   3442361|83900000| 225.91|            2|\n",
      "|Port Elizabeth|    4|       6001|      041|       1820|   true|    4|   1152915|45600000| 251.03|            3|\n",
      "|   East London|    7|       5200|      043|       1847|   true|    7|    755200|23400000| 168.86|            4|\n",
      "|  Johannesburg|    0|       2001|      011|       1886|  false|    0|  10500000|76000000|1644.98|            1|\n",
      "|      Pretoria|    1|       0001|      012|       1855|  false|    1|   2921612|75600000| 687.54|            2|\n",
      "|  Bloemfontein|    5|       9300|      051|       1846|  false|    5|    747431|15600000| 236.17|            3|\n",
      "|     Kimberley|    6|       8301|      053|       1873|  false|    6|    225160|  460000|  164.3|            4|\n",
      "+--------------+-----+-----------+---------+-----------+-------+-----+----------+--------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT\n",
    "    *,\n",
    "    dense_rank() OVER (PARTITION BY coastal ORDER BY population DESC) as pop_rank_area\n",
    "  FROM joined_cities\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL makes it quite intuitive. We are selecting everything using `SELECT *`, then we call the `dense_rank()` function, which requires that we specify over what we `PARTITION` the data and what we `ORDER` the data by. \n",
    "\n",
    "Now let's do the same in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a `Frame` to partition by, we need to instantiate the `Window` class, \n",
    "# and use the partitionby() and orderby() methods to define the `Frame`.\n",
    "\n",
    "coastal_population_window = Window.partitionBy(joined_cities['coastal']) \\\n",
    "                                  .orderBy(joined_cities['population'].desc()) \n",
    "\n",
    "# We can then use the instantiated Window class to aggregate over.\n",
    "\n",
    "joined_cities = joined_cities.withColumn('pop_rank_area', F.dense_rank().over(coastal_population_window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot is happening here. Let’s look at it part by part.\n",
    "\n",
    "**First**, we call the `partitionBy()` method, which defined the field to partition by (here we select the `coastal` field).\n",
    "\n",
    "*Remember that you can also use F.col('coastal') to reference the fields.*\n",
    "\n",
    "**Next**, we define the field by which we want to order the rows within each `Frame`.\n",
    "\n",
    "**Finally**, we bring it all together by creating a new column using the `withColumn()` method. \n",
    "\n",
    "Here we call the `dense_rank()` function, and it has the `over()` method that takes the window object as an argument. This means that we will apply a dense rank over the specified window frame, effectively aggregating within the window frame.\n",
    "\n",
    "Overall, the result is quite the same between SparkSQL and PySpark, the interface is slightly different, and you can decide which works best for you. Going forward in this section, we are going to use PySpark implementations, which is more in line with the Python environment we have been using until now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-----------+---------+-----------+-------+-----+----------+--------+-------+-------------+\n",
      "|          city|index|postal_code|area_code|established|coastal|index|population|     gdp|   area|pop_rank_area|\n",
      "+--------------+-----+-----------+---------+-----------+-------+-----+----------+--------+-------+-------------+\n",
      "|     Cape Town|    3|       8000|      021|       1652|   true|    3|   3740026|78700000| 2461.0|            1|\n",
      "|        Durban|    2|       4001|      031|       1880|   true|    2|   3442361|83900000| 225.91|            2|\n",
      "|Port Elizabeth|    4|       6001|      041|       1820|   true|    4|   1152915|45600000| 251.03|            3|\n",
      "|   East London|    7|       5200|      043|       1847|   true|    7|    755200|23400000| 168.86|            4|\n",
      "|  Johannesburg|    0|       2001|      011|       1886|  false|    0|  10500000|76000000|1644.98|            1|\n",
      "|      Pretoria|    1|       0001|      012|       1855|  false|    1|   2921612|75600000| 687.54|            2|\n",
      "|  Bloemfontein|    5|       9300|      051|       1846|  false|    5|    747431|15600000| 236.17|            3|\n",
      "|     Kimberley|    6|       8301|      053|       1873|  false|    6|    225160|  460000|  164.3|            4|\n",
      "+--------------+-----+-----------+---------+-----------+-------+-----+----------+--------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_cities.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened here?\n",
    "\n",
    "We calculated the pop_rank_area, which is a rank based on if a town is coastal or not. You will see that coastal cities are ranked 1 to 4, and so also inland cities. This is all because we partitioned by 'coastal' and ordered by 'population'. So you will see that the rank is based on population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's not just rank, but rather get into some analytics. \n",
    "\n",
    "Let’s calculate the area difference between each city and the largest city for inland and coastal cities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again using the Window class, partitioning by coastal and ordering by area. \n",
    "coastal_area_window = Window.partitionBy(F.col('coastal')) \\\n",
    "  .orderBy(F.col('area').desc()) \n",
    "\n",
    "# This time, we are using arithmetic to calculate how much smaller the area is \n",
    "# for the city in question than the largest one in the coastal or inland group.\n",
    "joined_cities = \\\n",
    "  joined_cities.withColumn('coastal_rank_area', F.max(F.col('area')).over(coastal_area_window) - F.col('area'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-----------+---------+-----------+-------+-----+----------+--------+-------+-------------+-----------------+\n",
      "|          city|index|postal_code|area_code|established|coastal|index|population|     gdp|   area|pop_rank_area|coastal_rank_area|\n",
      "+--------------+-----+-----------+---------+-----------+-------+-----+----------+--------+-------+-------------+-----------------+\n",
      "|     Cape Town|    3|       8000|      021|       1652|   true|    3|   3740026|78700000| 2461.0|            1|              0.0|\n",
      "|Port Elizabeth|    4|       6001|      041|       1820|   true|    4|   1152915|45600000| 251.03|            3|          2209.97|\n",
      "|        Durban|    2|       4001|      031|       1880|   true|    2|   3442361|83900000| 225.91|            2|          2235.09|\n",
      "|   East London|    7|       5200|      043|       1847|   true|    7|    755200|23400000| 168.86|            4|          2292.14|\n",
      "|  Johannesburg|    0|       2001|      011|       1886|  false|    0|  10500000|76000000|1644.98|            1|              0.0|\n",
      "|      Pretoria|    1|       0001|      012|       1855|  false|    1|   2921612|75600000| 687.54|            2|           957.44|\n",
      "|  Bloemfontein|    5|       9300|      051|       1846|  false|    5|    747431|15600000| 236.17|            3|          1408.81|\n",
      "|     Kimberley|    6|       8301|      053|       1873|  false|    6|    225160|  460000|  164.3|            4|          1480.68|\n",
      "+--------------+-----+-----------+---------+-----------+-------+-----+----------+--------+-------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_cities.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite clear that Cape Town and Johannesburg are massive! Maybe coastal vs. inland is not the correct group comparison here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\[Exercise 3\\]\n",
    "\n",
    "As an exercise, create another grouping: high vs. low GDP (you can create an arbitrary threshold). And perform similar comparisons as above to see if the differences are smaller or more comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting into complex data types\n",
    "*Spark has an amazing functionality of storing complex data within DataFrames.*\n",
    "\n",
    "Complex data are made up of simpler data types, and as such, there are intuitively two approaches for processing complex data:\n",
    "\n",
    "1. Exploding the data, performing a transformation and recreating the structure.\n",
    "2. Using user-defined functions to process the data.\n",
    "\n",
    "However, both of these methods are computationally expensive, as the first requires you to collect all the items in the transformation and structure recreation, and the second, in turn, requires you to define a UDF, which Spark has limited capacity to optimise.\n",
    "\n",
    "\n",
    "> 💡 &nbsp; **Introducing UDFs**\n",
    ">\n",
    "> UDFs or user-defined functions are functions defined by the user that perform specific transformations. More specifically, UDFs are functions that operate on columns to extend the vocabulary of the Spark DSL for data transformation.\n",
    "\n",
    "\n",
    "Fortunately, Spark has built-in functions for dealing with complex data types. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create a DataFrame with complex data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing complex data types.\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, 'ford', {'length': 2.4, 'height': 1.3, 'color': 'blue'}, [1, 4, 2, 5, 1], [1, 2, 4, 5, 1], [11, 22, 43, 54, 15]),\n",
    "    (1, 'isuzu', {'length': 2.9, 'height': 1.6, 'color': 'white'}, [15, 2, 55, 12, 2], [55, 15, 16, 171, 2], [55, 15, 16, 171, 22]),\n",
    "    (2, 'toyota', {'length': 2.4, 'height': 1.3, 'color': 'white'}, [1, 2, 1, 1, 1], [1, 2, 2, 2, 1], [11, 22, 23, 24, 15]),\n",
    "    (3, 'mini', {'length': 1.2, 'height': 1.3, 'color': 'black'}, [1, 4, 4, 1, 4], [9, 8, 7, 6, 5], [91, 82, 73, 64, 55]),\n",
    "    (4, 'ford', {'length': 2.4, 'height': 1.3, 'color': 'blue'}, [1, 2, 3, 4, 5], [0, 1, 2, 3, 4], [10, 21, 32, 43, 54]),\n",
    "], ['index', 'make', 'description', 'count', 'anti_count', 'index_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------------------------------------------+------------------+--------------------+---------------------+\n",
      "|index|make  |description                                  |count             |anti_count          |index_count          |\n",
      "+-----+------+---------------------------------------------+------------------+--------------------+---------------------+\n",
      "|0    |ford  |{length -> 2.4, color -> null, height -> 1.3}|[1, 4, 2, 5, 1]   |[1, 2, 4, 5, 1]     |[11, 22, 43, 54, 15] |\n",
      "|1    |isuzu |{length -> 2.9, color -> null, height -> 1.6}|[15, 2, 55, 12, 2]|[55, 15, 16, 171, 2]|[55, 15, 16, 171, 22]|\n",
      "|2    |toyota|{length -> 2.4, color -> null, height -> 1.3}|[1, 2, 1, 1, 1]   |[1, 2, 2, 2, 1]     |[11, 22, 23, 24, 15] |\n",
      "|3    |mini  |{length -> 1.2, color -> null, height -> 1.3}|[1, 4, 4, 1, 4]   |[9, 8, 7, 6, 5]     |[91, 82, 73, 64, 55] |\n",
      "|4    |ford  |{length -> 2.4, color -> null, height -> 1.3}|[1, 2, 3, 4, 5]   |[0, 1, 2, 3, 4]     |[10, 21, 32, 43, 54] |\n",
      "+-----+------+---------------------------------------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the code used above.\n",
    "\n",
    "First, we are using the `createDataFrame()` method on the existing `SparkSession`. It requires a list of tuples that will make up the data in the DataFrame, and a list of columns names. \n",
    "\n",
    "In this instance, we are providing it with six columns:\n",
    "- an index (which is an integer);\n",
    "- a car make column (string type);\n",
    "- some attributes of the car (map type – which are dictionaries in Python); and\n",
    "- three miscellaneous columns (containing array types – lists in Python). \n",
    "\n",
    "We can have a look at how Spark inferred the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- description: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: double (valueContainsNull = true)\n",
      " |-- count: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- anti_count: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- index_count: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For us to use Spark SQL to manipulate this DataFrame, let's register this as a temp view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('_temp_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the fun part. We are going to do a range of manipulations:\n",
    "\n",
    "1. Select the distinct elements from each array in the DataFrame in the count field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|array_distinct(count)|\n",
      "+---------------------+\n",
      "|         [1, 4, 2, 5]|\n",
      "|      [15, 2, 55, 12]|\n",
      "|               [1, 2]|\n",
      "|               [1, 4]|\n",
      "|      [1, 2, 3, 4, 5]|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array_distinct is a SparkSQL function that operates on complex data \n",
    "# within DataFrames on type array, getting distinct elements of each array.\n",
    "\n",
    "spark.sql(\"SELECT array_distinct(count) from _temp_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select the maximum for each array in the count field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|array_max(count)|\n",
      "+----------------+\n",
      "|               5|\n",
      "|              55|\n",
      "|               2|\n",
      "|               4|\n",
      "|               5|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array_max is a SparkSQL function that operates on complex data \n",
    "# within DataFrames on type array, getting the numerical maximum for each array.\n",
    "\n",
    "spark.sql(\"SELECT array_max(count) from _temp_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Sort the arrays in the anti_count field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|array_sort(anti_count, lambdafunction((IF(((namedlambdavariable() IS NULL) AND (namedlambdavariable() IS NULL)), 0, (IF((namedlambdavariable() IS NULL), 1, (IF((namedlambdavariable() IS NULL), -1, (IF((namedlambdavariable() < namedlambdavariable()), -1, (IF((namedlambdavariable() > namedlambdavariable()), 1, 0)))))))))), namedlambdavariable(), namedlambdavariable()))|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                                                                                                                                                                                                                                                  [1, 1, 2, 4, 5]|\n",
      "|                                                                                                                                                                                                                                                                                                                                                             [2, 15, 16, 55, 171]|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                  [1, 1, 2, 2, 2]|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                  [5, 6, 7, 8, 9]|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                  [0, 1, 2, 3, 4]|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array_sort is a SparkSQL function that operates on complex data \n",
    "# within DataFrames on type array, sorting the elements in the array.\n",
    "\n",
    "spark.sql(\"SELECT array_sort(anti_count) from _temp_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Get the cardinality of the array fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------------+------------------------+\n",
      "|cardinality(count)|cardinality(anti_count)|cardinality(index_count)|\n",
      "+------------------+-----------------------+------------------------+\n",
      "|                 5|                      5|                       5|\n",
      "|                 5|                      5|                       5|\n",
      "|                 5|                      5|                       5|\n",
      "|                 5|                      5|                       5|\n",
      "|                 5|                      5|                       5|\n",
      "+------------------+-----------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cardinality is a SparkSQL function that operates on complex data \n",
    "# within DataFrames on type array, calculating the cardinality of each array.\n",
    "\n",
    "spark.sql(\"SELECT cardinality(count), cardinality(anti_count), cardinality(index_count) from _temp_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 💡 &nbsp; **Remember**\n",
    ">\n",
    "> Cardinality is a measure of the number of elements in a set, array or list.\n",
    ">\n",
    "> For example, this list [41, 22, 67, 5] will have a cardinality of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Find the intersection between count and anti-count, as well as values in count array and not in anti_count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-------------------------------+\n",
      "|array_intersect(count, anti_count)|array_except(count, anti_count)|\n",
      "+----------------------------------+-------------------------------+\n",
      "|                      [1, 4, 2, 5]|                             []|\n",
      "|                       [15, 2, 55]|                           [12]|\n",
      "|                            [1, 2]|                             []|\n",
      "|                                []|                         [1, 4]|\n",
      "|                      [1, 2, 3, 4]|                            [5]|\n",
      "+----------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array_intersect is a SparkSQL function that operates on complex data \n",
    "# within DataFrames on type array, finding the intersect between two arrays.\n",
    "\n",
    "# array_except is the opposite, selecting values contained in the first \n",
    "# array and not the second.\n",
    "\n",
    "spark.sql(\"SELECT array_intersect(count, anti_count), array_except(count, anti_count) from _temp_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create map types from the index_count and anti_count fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+\n",
      "|map_from_arrays(index_count, anti_count)           |\n",
      "+---------------------------------------------------+\n",
      "|{11 -> 1, 22 -> 2, 43 -> 4, 54 -> 5, 15 -> 1}      |\n",
      "|{55 -> 55, 15 -> 15, 16 -> 16, 171 -> 171, 22 -> 2}|\n",
      "|{11 -> 1, 22 -> 2, 23 -> 2, 24 -> 2, 15 -> 1}      |\n",
      "|{91 -> 9, 82 -> 8, 73 -> 7, 64 -> 6, 55 -> 5}      |\n",
      "|{10 -> 0, 21 -> 1, 32 -> 2, 43 -> 3, 54 -> 4}      |\n",
      "+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# map_from_arrays create a one-to-one mapping between elements in the two\n",
    "# columns.\n",
    "\n",
    "spark.sql(\"SELECT map_from_arrays(index_count, anti_count) from _temp_df\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Have a look at the cardinality of the description map field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|cardinality(description)|\n",
      "+------------------------+\n",
      "|                       3|\n",
      "|                       3|\n",
      "|                       3|\n",
      "|                       3|\n",
      "|                       3|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT cardinality(description) from _temp_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Retrieve the length from each of the map types in the description field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|element_at(description, length)|\n",
      "+-------------------------------+\n",
      "|                            2.4|\n",
      "|                            2.9|\n",
      "|                            2.4|\n",
      "|                            1.2|\n",
      "|                            2.4|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# element_at can decompose map types within a field.\n",
    "\n",
    "spark.sql('SELECT element_at(description, \"length\") from _temp_df').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may not be blatantly obvious where to use this yet, but accessing dictionaries and lists from within fields in Spark can significantly improve your performance and the range of operations you can perform with Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher-order functions\n",
    "\n",
    "The above functions for transforming complex data types are quite useful, but you may require some level of customisation. This is where higher-order functions come in, where you can pass anonymous user-defined or lambda functions as arguments. \n",
    "\n",
    "Let's start with `transform()`, which allows us to apply a lambda function to each element in an array within a Spark field. This is similar to the `map()` Python function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------+\n",
      "|make  |dim_count                     |\n",
      "+------+------------------------------+\n",
      "|ford  |[0.01, 0.04, 0.02, 0.05, 0.01]|\n",
      "|isuzu |[0.15, 0.02, 0.55, 0.12, 0.02]|\n",
      "|toyota|[0.01, 0.02, 0.01, 0.01, 0.01]|\n",
      "|mini  |[0.01, 0.04, 0.04, 0.01, 0.04]|\n",
      "|ford  |[0.01, 0.02, 0.03, 0.04, 0.05]|\n",
      "+------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here, we divide each element in the array within the count field by 100.\n",
    "\n",
    "spark.sql(\"SELECT make, transform(count, t -> t / 100) AS dim_count FROM _temp_df\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter each value in the array by a specific lambda expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|make  |large_count |\n",
      "+------+------------+\n",
      "|ford  |[]          |\n",
      "|isuzu |[15, 55, 12]|\n",
      "|toyota|[]          |\n",
      "|mini  |[]          |\n",
      "|ford  |[]          |\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here, we filter arrays in the count field which are larger than 10.\n",
    "\n",
    "spark.sql(\"SELECT make, filter(count, t -> t > 10) AS large_count FROM _temp_df\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check boolean conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------+\n",
      "|make  |count             |check_count|\n",
      "+------+------------------+-----------+\n",
      "|ford  |[1, 4, 2, 5, 1]   |false      |\n",
      "|isuzu |[15, 2, 55, 12, 2]|true       |\n",
      "|toyota|[1, 2, 1, 1, 1]   |false      |\n",
      "|mini  |[1, 4, 4, 1, 4]   |false      |\n",
      "|ford  |[1, 2, 3, 4, 5]   |false      |\n",
      "+------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here, we check if an array within the count field contains the \n",
    "# value 12 and return a boolean value based on that.\n",
    "\n",
    "spark.sql(\"SELECT make, count, exists(count, t -> t = 12) AS check_count FROM _temp_df\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined functions – when Spark can't help you\n",
    "*Sometimes Spark just does not have the functionality that you require.  This is where UDFs and Pandas UDFs come in.*\n",
    "\n",
    "\n",
    "User-defined functions (UDFs) are processes that you programme yourself in Python and want to perform on your data in a distributed nature. Similarly, you can create Pandas UDFs, which have been greatly enhanced in Spark 3, to allow Spark to peek into these functions and optimise their execution at the DAG level. \n",
    "\n",
    "\n",
    "The benefit of creating UDFs is that you will also be able to use them in Spark SQL. As a data scientist or engineer, you can write functions that other team members later reuse without the need for them to understand the exact logic encapsulated in the function.\n",
    "\n",
    "\n",
    "It is important to note that the UDF does not persist longer than the SparkSession remains active. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply the same logic as we did in the Bitcoin section, but this time using a UDF instead of a SQL CASE statement.\n",
    "\n",
    "We will do this in several steps:\n",
    "\n",
    "\n",
    "1. Define a number of strings that will be used in the lambda function, which we then create to use as a UDF.\n",
    "2. Define a lambda function as a sequence of IF statements.  This can also be represented as a collection of IF statements in a normal function.\n",
    "3. Create a UDF object which can be used in the Python API.\n",
    "4. Register the UDF with the SparkSession to be used in Spark SQL.\n",
    "5. Apply the lambda function to the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define the Bitcoin advice strings.\n",
    "mesg1 = 'Sell now!'\n",
    "mesg2 = 'You should really  sell'\n",
    "mesg3 = 'Should have already sold'\n",
    "mesg4 = 'Buy NOW!'\n",
    "mesg5 = 'Lost it all!'\n",
    "\n",
    "# Define a lambda function that translates the Bitcoin price into the advice strings.\n",
    "func = lambda x : mesg1 if x > 19000 else (mesg2 if x > 17000 else (mesg3 if x > 15000 else (mesg4 if x > 0 else mesg5)))\n",
    "\n",
    "# Instantiate the UDF. \n",
    "udf_mesg = F.udf(func)\n",
    "\n",
    "# Register the UDF to allow reuse in Spark SQL.\n",
    "spark.udf.register('udf_mesg', func, StringType())\n",
    "\n",
    "bitcoin_data = bitcoin_data.withColumn('sell_guidance', udf_mesg('open'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-------------+\n",
      "|          timestamp|open|sell_guidance|\n",
      "+-------------------+----+-------------+\n",
      "|2011-12-31 09:52:00|4.39|     Buy NOW!|\n",
      "|2011-12-31 09:53:00| NaN| Lost it all!|\n",
      "|2011-12-31 09:54:00| NaN| Lost it all!|\n",
      "|2011-12-31 09:55:00| NaN| Lost it all!|\n",
      "|2011-12-31 09:56:00| NaN| Lost it all!|\n",
      "|2011-12-31 09:57:00| NaN| Lost it all!|\n",
      "|2011-12-31 09:58:00| NaN| Lost it all!|\n",
      "|2011-12-31 09:59:00| NaN| Lost it all!|\n",
      "|2011-12-31 10:00:00| NaN| Lost it all!|\n",
      "|2011-12-31 10:01:00| NaN| Lost it all!|\n",
      "+-------------------+----+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bitcoin_data.select('timestamp', 'open', 'sell_guidance').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a similar thing using the temporary view, `bitcoin_tbl`,  that we created earlier. Here we apply the UDF directly to the temporary view using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-------------+\n",
      "|          timestamp|open|sell_guidance|\n",
      "+-------------------+----+-------------+\n",
      "|2011-12-31 09:52:00|4.39|     Buy NOW!|\n",
      "|2011-12-31 09:53:00| NaN| Lost it all!|\n",
      "|2011-12-31 09:54:00| NaN| Lost it all!|\n",
      "|2011-12-31 09:55:00| NaN| Lost it all!|\n",
      "|2011-12-31 09:56:00| NaN| Lost it all!|\n",
      "|2011-12-31 09:57:00| NaN| Lost it all!|\n",
      "|2011-12-31 09:58:00| NaN| Lost it all!|\n",
      "|2011-12-31 09:59:00| NaN| Lost it all!|\n",
      "|2011-12-31 10:00:00| NaN| Lost it all!|\n",
      "|2011-12-31 10:01:00| NaN| Lost it all!|\n",
      "+-------------------+----+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT timestamp, open, udf_mesg(open) AS sell_guidance FROM bitcoin_tbl\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to perform some additional magic:\n",
    "\n",
    "First, we add a `year` column to the DataFrame, then group the newly created categorical field, pivot the table on the year column, and aggregate by retrieving the mean of the open column.  Quite a bit to do, but let’s add one more operation. Instead of only getting the mean,  also round the mean.\n",
    "\n",
    "\n",
    "*This last bit was just to give you something so you can make sense of what the data will look like and draw some inferences from it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+------+------+------+------+--------+--------+-------+--------+\n",
      "|       sell_guidance|2011| 2012|  2013|  2014|  2015|  2016|    2017|    2018|   2019|    2020|\n",
      "+--------------------+----+-----+------+------+------+------+--------+--------+-------+--------+\n",
      "|You should really...|null| null|  null|  null|  null|  null|18006.24|17077.87|   null|18252.32|\n",
      "|Should have alrea...|null| null|  null|  null|  null|  null|16030.03|15957.52|   null| 15881.4|\n",
      "|            Buy NOW!|4.46|10.09|254.14|527.23|274.35|558.79| 3565.59| 7468.63|7426.42| 9525.09|\n",
      "|           Sell now!|null| null|  null|  null|  null|  null|19230.81|    null|   null|22261.03|\n",
      "+--------------------+----+-----+------+------+------+------+--------+--------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a year column, filled by the year values from the timestamp column.\n",
    "bitcoin_data = bitcoin_data.withColumn('year', F.year('timestamp'))\n",
    "\n",
    "# Group by the sell_guidance field created above and pivot on the year field.\n",
    "# Once pivoted, Spark required an aggregation function. Here we get the mean\n",
    "# and round to the nearest two digits.\n",
    "\n",
    "bitcoin_data.dropna().groupBy('sell_guidance').pivot('year').agg(F.round(F.mean('open'), 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the issues specific to PySpark is that UDFs tend to be significantly slower than their Scala counterparts. That is due to the data movement between the Java Virtual Machine (JVM) and Python. This was resolved when Pandas UDFs were introduced, which uses Apache Arrow to transfer data. You can define the function using the `pandas_df` decorator, which will transform the data into the Apache Arrow format, which is consumable by Python. Instead of input rows, you can also operate on a Pandas Series or DataFrame, speeding up transfer and processing.\n",
    "\n",
    "\n",
    "Spark 3 also introduced a further distinction – Pandas UDFs and Pandas Function APIs. \n",
    "\n",
    "With Pandas UDFs, Spark infers the data types from Python type hints, such as `pandas.DataFrame`. Currently, you can then do conversions using Pandas UDFs from Series to Series, Iterator of Series to Iterator of Series, Iterator of Multiple Series to Iterator of Series, and Series to Scalar.\n",
    "\n",
    "For this to work, make sure you have pyarrow >= 1.5.1 installed. [Here are the instructions to follow](https://arrow.apache.org/docs/python/install.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the difference function. Both inputs are Pandas series objects, \n",
    "# similarly the output. This will map to fields in Spark.\n",
    "def difference(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    return a - b\n",
    "\n",
    "# Create the Pandas UDF for the difference function.\n",
    "cubed_udf = F.pandas_udf(difference, returnType=FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the difference UDF function to `close` and `open` fields.\n",
    "\n",
    "bitcoin_data = bitcoin_data.withColumn('change_in_price', cubed_udf('close', 'open'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+----+----+-----+----------+---------------+--------------+-------------+----+---------------+\n",
      "|          timestamp|open|high| low|close|volume_btc|volume_currency|weighted_price|sell_guidance|year|change_in_price|\n",
      "+-------------------+----+----+----+-----+----------+---------------+--------------+-------------+----+---------------+\n",
      "|2011-12-31 18:59:00| 4.5|4.57| 4.5| 4.57| 37.862297|      171.38034|     4.5264115|     Buy NOW!|2011|     0.07000017|\n",
      "|2012-01-04 18:00:00|5.36|5.37|5.36| 5.37| 13.629423|          73.06|     5.3604617|     Buy NOW!|2012|    0.009999752|\n",
      "|2012-01-04 19:51:00|5.37|5.57|5.37| 5.57| 43.312195|      235.74707|      5.442972|     Buy NOW!|2012|     0.20000029|\n",
      "|2012-01-05 09:19:00|5.75|5.79|5.75| 5.79|      14.8|           85.5|      5.777027|     Buy NOW!|2012|     0.03999996|\n",
      "|2012-01-05 12:10:00|6.19|6.23|6.19| 6.23|      16.0|       99.28572|     6.2053576|     Buy NOW!|2012|     0.03999996|\n",
      "|2012-01-05 12:48:00|6.23|6.25|6.23| 6.25|      14.0|          87.42|     6.2442856|     Buy NOW!|2012|     0.01999998|\n",
      "|2012-01-06 08:04:00|6.69|6.73|6.69| 6.73|      6.31|      42.363857|      6.713765|     Buy NOW!|2012|     0.03999996|\n",
      "|2012-01-06 15:20:00| 6.8| 6.9| 6.8|  6.9|  9.310559|        63.6118|     6.8322215|     Buy NOW!|2012|    0.099999905|\n",
      "|2012-01-09 06:40:00|6.99|6.99| 6.9|  6.9|       5.0|       34.69854|      6.939708|     Buy NOW!|2012|   -0.089999676|\n",
      "|2012-01-11 16:25:00|7.22|7.33|7.22| 7.33|  12.92299|       93.98726|     7.2728724|     Buy NOW!|2012|     0.11000013|\n",
      "|2012-01-13 15:35:00|6.99|6.99|6.82| 6.82| 1.4662757|      10.002816|      6.821921|     Buy NOW!|2012|     -0.1699996|\n",
      "|2012-01-13 16:04:00| 7.0|7.25|6.86| 7.25| 5.5749674|        40.0366|      7.181495|     Buy NOW!|2012|           0.25|\n",
      "|2012-01-13 16:05:00|7.25|7.35|7.25| 7.35| 2.7997653|          20.49|     7.3184705|     Buy NOW!|2012|    0.099999905|\n",
      "|2012-01-13 16:06:00|7.35|7.35|6.86| 6.86|  1.457726|      10.578299|      7.256713|     Buy NOW!|2012|    -0.48999977|\n",
      "|2012-01-13 16:26:00| 7.0| 7.0| 6.9|  6.9| 1.4556041|      10.143668|        6.9687|     Buy NOW!|2012|   -0.099999905|\n",
      "|2012-01-13 19:24:00|6.76| 6.9|6.76|  6.9|     0.821|      5.5977993|     6.8182697|     Buy NOW!|2012|     0.13999987|\n",
      "|2012-01-14 03:09:00| 6.5| 6.5| 6.4|  6.4|  1.689531|      10.881951|      6.440812|     Buy NOW!|2012|   -0.099999905|\n",
      "|2012-01-18 00:34:00| 6.3| 6.3| 6.2|  6.2| 1.6233766|      10.164935|        6.2616|     Buy NOW!|2012|    -0.10000038|\n",
      "|2012-01-18 00:35:00| 6.2| 6.2| 6.0|  6.0|  1.776199|      10.832519|     6.0987077|     Buy NOW!|2012|    -0.19999981|\n",
      "|2012-01-18 03:31:00|6.02| 6.3|6.02|  6.3| 31.363214|       191.5384|      6.107104|     Buy NOW!|2012|      0.2800002|\n",
      "+-------------------+----+----+----+-----+----------+---------------+--------------+-------------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bitcoin_data.where(F.col('change_in_price') != 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas Function APIs allow you to apply a local Python function to a PySpark DataFrame, where both the input and output are Pandas instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to relational databases\n",
    "\n",
    "The following information and tables on the connection properties are referenced from the *Learning Spark Textbook* that can be found [here](https://github.com/Explore-AI/Pictures/blob/a700f77ae9331147029cbad145f4293650ac8eab/data_engineering/transform/spark_deeper_concepts/LearningSpark2.0.pdf?raw=true).\n",
    "\n",
    "*While the Hive metastore is a great interface into the data that are underlying Spark, sometimes you want to connect to the data from the outside.*\n",
    "\n",
    "To connect to Spark from the outside world, you have to connect to the Thrift JDBC/ODBC server, also called the Spark Thrift Server (STS). This allows JDBC/ODBC clients to execute SQL queries over JDBC and OBDC protocols on Spark. \n",
    "This is the toolkit you will be using if you want to connect BI tools to Spark, for example, PowerBI or Tableau. \n",
    "\n",
    "\n",
    "In addition to using the Thrift server to connect external tools to Spark, Spark SQL includes a data source API that allows you to read and write data from external databases using JDBC. When running queries, the connector will return a DataFrame object, along with all performance optimisations and the ability to join to datasets already in Spark as DataFrames.\n",
    "\n",
    "To instantiate this, you first have to specify the JDBC driver for your JDBC data source. \n",
    "For example: \n",
    "\n",
    "`cd $SPARK_HOME`\n",
    "\n",
    "`./bin/spark-shell --driver-class-path $database.jar --jars $database.jar`\n",
    "\n",
    "JDBC connection properties will then be specified in the data source options. Options include:\n",
    "\n",
    "<div align=\"center\" style=\"width: 800px; font-size: 100%; text-align: center\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/4a0a9f0f76ab12bb3dc2684a41c901adaec63164/data_engineering/transform/spark_deeper_concepts/common_connection_properties.png?raw=True\"\n",
    "     alt=\"Common Connection Properties\"\n",
    "     style=\"padding-bottom=1em\"\n",
    "     width=750px/>\n",
    "     <em>Learning Spark <a href=\"https://github.com/Explore-AI/Pictures/blob/a700f77ae9331147029cbad145f4293650ac8eab/data_engineering/transform/spark_deeper_concepts/LearningSpark2.0.pdf?raw=true\">here</a>.</em>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on partitioning** \n",
    "\n",
    "Partitioning is especially important when transferring or writing large amounts of data between Spark and an external data source (not just SQL). When transferring data through the JDBC connector, all the data go through a single driver node, which can easily be saturated and slow down the operation. It is advised that you include some of the following properties to speed up the operation. \n",
    "\n",
    "\n",
    "<div align=\"left\" style=\"width: 800px; font-size: 100%; text-align: left\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/4a0a9f0f76ab12bb3dc2684a41c901adaec63164/data_engineering/transform/spark_deeper_concepts/partitioning_connection_properties.png?raw=True\"\n",
    "     alt=\"Common Connection Properties\"\n",
    "     style=\"padding-bottom=1em\"\n",
    "     width=750px/>\n",
    "     <em>Learning Spark <a href=\"https://github.com/Explore-AI/Pictures/blob/a700f77ae9331147029cbad145f4293650ac8eab/data_engineering/transform/spark_deeper_concepts/LearningSpark2.0.pdf?raw=true\">here</a>.</em>\n",
    "</div>\n",
    "<br/><br/>\n",
    "\n",
    "Partitioning will happen by partitioning on the `partitionColumn`. The `numPartitions` will determine the maximum number of concurrent JDBC connections to execute, and `lowerBound` and `upperBound` will determine the maximum and minimum stride for the partitions.\n",
    "\n",
    "\n",
    "For example, if you define the following configuration:\n",
    "- `numPartitions: 10`\n",
    "- `lowerBound: 2011`\n",
    "- `upperBound: 2020`\n",
    "- `partitionColumn: year`\n",
    "\n",
    "The following set of queries will be executed:\n",
    "- `SELECT * FROM table WHERE year is 2011`\n",
    "- `SELECT * FROM table WHERE year is 2012`\n",
    "- ...\n",
    "- `SELECT * FROM table WHERE year is 2020`\n",
    "\n",
    "A total of 10 `SELECT` statements.\n",
    "\n",
    "Some guiding principles:\n",
    "- Start with the same number of partitions as the number of workers you have in your cluster, or a multiple thereof. But keep in mind the concurrency potential of your source/sink system. \n",
    "- Create the `lowerBound` and `upperBound` values based on actual values in the `partitionColumn`. If these values are not accurate, you will exclude some of your data or have empty partitions. \n",
    "- Select a `partitionColumn` that can be partitioned roughly equally (for example, in the above, example year is a great choice since we expect the number of entries to be identical).  Times and dates make excellent partitions in a system where data has been ingested consistently. Alternatively, you can create a hash from your primary key or a set of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include a code example to connect to a database (Microsoft SQL Server). However, the code below is not meant to be run in this notebook unless you have a database connection set upon which you can test this.\n",
    "\n",
    "Jars can be downloaded from MAVEN:\n",
    "`bin/spark-shell --jars mssql-jdbc-7.2.2.jre8.jar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "databaseUrl = <URL-TO-TABLE>\n",
    "tbl = some_table\n",
    "uname = joe_doe\n",
    "pwd = some_intricate_password\n",
    "# Loading data from a JDBC source.\n",
    "readDF = (spark.read\\\n",
    ".format(\"jdbc\")\n",
    ".option(\"url\", databaseUrl)\n",
    ".option(\"dbtable\", \"tbl\")\n",
    ".option(\"user\", \"uname\")\n",
    ".option(\"password\", \"pwd\")\n",
    ".load())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Saving data to a JDBC source.\n",
    "writeDF\n",
    ".write\n",
    ".format(\"jdbc\")\n",
    ".option(\"url\", databaseUrl)\n",
    ".option(\"dbtable\", \"tbl\")\n",
    ".option(\"user\", \"uname\")\n",
    ".option(\"password\", \"pwd\")\n",
    ".save()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's all, folks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Thanks for joining us for lots of Spark SQL.\n",
    "\n",
    "In this train, we dived into the details of what is possible in Spark SQL, how it is possible to replicate the capabilities of the Python API in Spark SQL, and how some things are simpler with Spark SQL than it is in the other APIs.\n",
    "We also looked at how to perform custom functions when you can't find a function you need in Spark, and finally, interacting with external databases through Spark connectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\[Exercise 1\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we have to select all records with a close value larger than 10 000. \n",
    "An additional catch here is that Spark interprets NaN values as very large floats, and they will be included. We thus have to filter them out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+--------+--------+----------+---------------+--------------+\n",
      "|          timestamp|    open|    high|     low|   close|volume_btc|volume_currency|weighted_price|\n",
      "+-------------------+--------+--------+--------+--------+----------+---------------+--------------+\n",
      "|2017-11-29 04:39:00|  9999.0| 10010.0|  9992.0| 10010.0| 165.63174|      1656279.1|      9999.769|\n",
      "|2017-11-29 04:40:00| 10010.0|10035.39| 10010.0|10035.39|  76.33833|      765076.94|     10022.187|\n",
      "|2017-11-29 04:41:00|10035.39| 10059.0|10035.39| 10058.0|  37.38189|      375654.78|     10049.111|\n",
      "|2017-11-29 04:42:00|10058.99|10069.12|10042.87| 10069.0| 12.911021|      129891.89|     10060.544|\n",
      "|2017-11-29 04:43:00| 10068.0|10069.12| 10059.0| 10069.0| 26.123903|       263039.8|     10068.933|\n",
      "|2017-11-29 04:44:00|10069.12|10069.12|10050.75|10054.46| 12.534665|     126085.984|     10058.983|\n",
      "|2017-11-29 04:45:00|10054.47|10054.98| 10050.0|10054.42| 3.3103845|       33275.46|     10051.842|\n",
      "|2017-11-29 04:46:00|10054.41|10069.12| 10050.0|10069.03| 3.7611804|      37833.785|      10059.02|\n",
      "|2017-11-29 04:47:00|10068.98|10069.11|10050.06|10056.01|  5.292985|       53248.05|     10060.117|\n",
      "|2017-11-29 04:48:00| 10056.0|10069.12|10051.12|10056.18| 2.0520806|       20640.55|     10058.354|\n",
      "+-------------------+--------+--------+--------+--------+----------+---------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT *\n",
    "          FROM bitcoin_tbl WHERE close > 10000\n",
    "          AND isNaN(close) = false \n",
    "          \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\[Exercise 2\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we had to find the best month to buy Bitcoin in 2018, in other words, the month with the most occurrences of `Buy NOW!`.\n",
    "\n",
    "First, we use the `BETWEEN` method to filter for records in 2018 only. We also filter for the `Buy NOW!` category in the sell guidance field. \n",
    "Finally, we group by the month using the `month()` function and order according to the same field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----------+\n",
      "|month(CAST(timestamp AS DATE))|count(high)|\n",
      "+------------------------------+-----------+\n",
      "|1                             |37982      |\n",
      "|2                             |40242      |\n",
      "|3                             |44352      |\n",
      "|4                             |43121      |\n",
      "|5                             |44530      |\n",
      "|6                             |42853      |\n",
      "|7                             |44179      |\n",
      "|8                             |43044      |\n",
      "|9                             |39392      |\n",
      "|10                            |37962      |\n",
      "|11                            |39171      |\n",
      "|12                            |42407      |\n",
      "+------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT month(timestamp), COUNT(high)\n",
    "          FROM bitcoin_advice\n",
    "          WHERE timestamp BETWEEN '2018-01-01' and '2019-01-01' AND sell_guidance = 'Buy NOW!'\n",
    "          GROUP BY month(timestamp)\n",
    "          ORDER BY month(timestamp)\"\"\").show(12, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\[Exercise 3\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have to window over a new column, which divides the GDP figures into two or more groups. \n",
    "\n",
    "First, we create those groups based on an arbitrary number that splits the data into two.\n",
    "\n",
    "Next, we window by that new field and order by GDP within that window. \n",
    "\n",
    "Finally, we calculate the difference between the GDP of the group’s largest member and the member in question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-----------+---------+-----------+-------+-----+----------+--------+-------+-------------+-----------------+---------+-------------+\n",
      "|          city|index|postal_code|area_code|established|coastal|index|population|     gdp|   area|pop_rank_area|coastal_rank_area|gdp_group|gdp_rank_area|\n",
      "+--------------+-----+-----------+---------+-----------+-------+-----+----------+--------+-------+-------------+-----------------+---------+-------------+\n",
      "|Port Elizabeth|    4|       6001|      041|       1820|   true|    4|   1152915|45600000| 251.03|            3|          2209.97|      low|            0|\n",
      "|   East London|    7|       5200|      043|       1847|   true|    7|    755200|23400000| 168.86|            4|          2292.14|      low|     22200000|\n",
      "|  Bloemfontein|    5|       9300|      051|       1846|  false|    5|    747431|15600000| 236.17|            3|          1408.81|      low|     30000000|\n",
      "|     Kimberley|    6|       8301|      053|       1873|  false|    6|    225160|  460000|  164.3|            4|          1480.68|      low|     45140000|\n",
      "|        Durban|    2|       4001|      031|       1880|   true|    2|   3442361|83900000| 225.91|            2|          2235.09|     high|            0|\n",
      "|     Cape Town|    3|       8000|      021|       1652|   true|    3|   3740026|78700000| 2461.0|            1|              0.0|     high|      5200000|\n",
      "|  Johannesburg|    0|       2001|      011|       1886|  false|    0|  10500000|76000000|1644.98|            1|              0.0|     high|      7900000|\n",
      "|      Pretoria|    1|       0001|      012|       1855|  false|    1|   2921612|75600000| 687.54|            2|           957.44|     high|      8300000|\n",
      "+--------------+-----+-----------+---------+-----------+-------+-----+----------+--------+-------+-------------+-----------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_cities = joined_cities.withColumn('gdp_group', F.when(F.col('gdp') > 75000000, 'high').otherwise('low'))\n",
    "\n",
    "gdp_area_window = Window.partitionBy(F.col('gdp_group')) \\\n",
    "  .orderBy(F.col('gdp').desc()) \n",
    "\n",
    "joined_cities = \\\n",
    "  joined_cities.withColumn('gdp_rank_area', F.max(F.col('gdp')).over(gdp_area_window) - F.col('gdp'))\n",
    "\n",
    "joined_cities.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb03c4e656ef945a00b59dacec54b501ca48f746723d70a95446e396238f7a19"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
