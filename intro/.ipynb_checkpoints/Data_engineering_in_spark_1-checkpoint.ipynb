{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZJlxkVT47Cc"
   },
   "source": [
    "# Data Engineering in Spark â€“ I\n",
    "Â© Explore Data Science Academy\n",
    "\n",
    "\n",
    "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Processing%20Big%20Data/spark.png\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XcU8qqp47Cf"
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "In this train, you'll learn how to implement common data engineering transformations. These include: \n",
    "\n",
    "  - extraction and parsing; \n",
    "  - translation and mapping, and \n",
    "  - filtering, aggregation, and summarisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqhFjbog47Cg"
   },
   "source": [
    "## The data engineering process\n",
    "\n",
    "Data engineering, at its core, is the foundational practice of preparing an environment in which we can build big data systems. While a large part of the data engineering process is concerned with building out the infrastructure to ingest data and creating pipelines that will transform the data in an automated way, one of the largest components in data engineering is the transformation and cleaning of datasets. \n",
    "\n",
    "In this train, we will take a single dataset through the typical data engineering process, starting with ingesting the data, transforming it, and outputting the data once done. We will touch on several common transformations that you need in your data engineering toolkit. \n",
    "\n",
    "Data transformation is the process of changing the structure, format, and content of a dataset. Each of these steps may have various goals:\n",
    "- **Constructive**: adding, copying, replicating, and creating new fields.\n",
    "- **Destructive**: deleting fields or records, and removing erroneous fields.\n",
    "- **Aesthetic**: standardisation, typecasting, and pivoting.\n",
    "- **Structural**: renaming, moving, joining, and combining fields.\n",
    "\n",
    "In transforming the data, the quality and reliability of the data will be improved, reducing the possibility of errors occurring when working with the datasets, allowing data scientists and analysts to only think about domain complexity. \n",
    "\n",
    "It is important to keep in mind that it is computationally expensive to perform transformations, which can imply real monetary implications. High costs from cloud providers will result in a system administrator or your boss who is not too happy with you.\n",
    "Also, carelessness or a lack of expertise can mean that you paint yourself into a corner when performing transformations, so be sure to plan your transformations out carefully to not break any current business processes or make life difficult for other data team members. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1834J4ou47Cg"
   },
   "source": [
    "## Extraction and parsing\n",
    "\n",
    "Extraction is the process of ingesting the data from a source system (in this case, just a CSV from Kaggle).\n",
    "Parsing, on the other hand, is transforming the data into an appropriate structure to allow compatibility with destination systems. This means changing data into the correct types for your destination and finally writing to a destination system. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FYNxGnCp47Ch"
   },
   "outputs": [],
   "source": [
    "# Import Spark and some auxiliary functions and set up a SparkSession.\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7n22Ud_47Ci"
   },
   "source": [
    "In this train, we will be using a dataset that can be [downloaded from Kaggle](https://www.kaggle.com/conorrot/irish-weather-hourly-data).\n",
    "\n",
    "This dataset contains hourly weather data from 25 weather stations across Ireland, obtained from the Irish Meteorological Service, Met Ã‰ireann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-VqEQ_9j47Ci"
   },
   "outputs": [],
   "source": [
    "# Read into Spark using the SparkSession and DataFrameReader only with header inferred.\n",
    "# Change the below location to match your current working directory.\n",
    "\n",
    "raw_df = spark.read.csv('./data/weather_data/hrly_Irish_weather.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VWe6ly-47Cj",
    "outputId": "db114128-ad76-4f12-fb21-806b4596230e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- county: string (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- rain: string (nullable = true)\n",
      " |-- temp: string (nullable = true)\n",
      " |-- wetb: string (nullable = true)\n",
      " |-- dewpt: string (nullable = true)\n",
      " |-- vappr: string (nullable = true)\n",
      " |-- rhum: string (nullable = true)\n",
      " |-- msl: string (nullable = true)\n",
      " |-- wdsp: string (nullable = true)\n",
      " |-- wddir: string (nullable = true)\n",
      " |-- sun: string (nullable = true)\n",
      " |-- vis: string (nullable = true)\n",
      " |-- clht: string (nullable = true)\n",
      " |-- clamt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `printSchema()` method gives you a breakdown of the schema of the incoming data, detailing the field name, its type, and whether it is nullable or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXZ7xGcC47Cl"
   },
   "source": [
    "From the data ingestion, we can see that there are 18 fields, all of which have been inferred as strings. Let's look at the data and decide on better types for each field.\n",
    "\n",
    "We'll only look at non-null records using the `dropna()` method on the DataFrame to remove all null records.\n",
    "Remember that the methods used on a Spark DataFrame do not change the DataFrame itself, but rather returns a copy of the DataFrame on which we use the `show()` method to visualise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SLnDAmsJ47Cm",
    "outputId": "e42df13e-56fd-490e-8603-6915867a4d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+---------+-----------------+----+----+----+-----+-----+----+------+----+-----+---+-----+----+-----+\n",
      "|county|  station|latitude|longitude|             date|rain|temp|wetb|dewpt|vappr|rhum|   msl|wdsp|wddir|sun|  vis|clht|clamt|\n",
      "+------+---------+--------+---------+-----------------+----+----+----+-----+-----+----+------+----+-----+---+-----+----+-----+\n",
      "|  Mayo|BELMULLET|  54.228|  -10.007|01-jan-1990 00:00| 0.0| 7.6| 7.1|  6.5|  9.7|  93|1003.0|  10|  200|0.0|26000|  16|    8|\n",
      "|  Mayo|BELMULLET|  54.228|  -10.007|01-jan-1990 01:00| 0.0| 7.6| 7.2|  6.7|  9.8|  94|1003.0|  10|  190|0.0|19000|  18|    8|\n",
      "|  Mayo|BELMULLET|  54.228|  -10.007|01-jan-1990 02:00| 0.0| 7.8| 7.2|  6.5|  9.7|  91|1003.1|   8|  200|0.0|22000|  18|    7|\n",
      "|  Mayo|BELMULLET|  54.228|  -10.007|01-jan-1990 03:00| 0.0| 7.7| 6.8|  5.7|  9.2|  87|1003.4|  12|  220|0.0|26000|  18|    6|\n",
      "|  Mayo|BELMULLET|  54.228|  -10.007|01-jan-1990 04:00| 0.5| 7.5| 6.5|  5.3|  8.9|  86|1003.2|  13|  210|0.0|30000| 999|    3|\n",
      "|  Mayo|BELMULLET|  54.228|  -10.007|01-jan-1990 05:00| 0.0| 6.8| 5.9|  4.7|  8.6|  87|1003.9|   9|  220|0.0|30000| 999|    3|\n",
      "|  Mayo|BELMULLET|  54.228|  -10.007|01-jan-1990 06:00| 0.0| 5.8| 4.9|  3.7|  7.9|  86|1004.1|   7|  220|0.0|30000| 999|    3|\n",
      "|  Mayo|BELMULLET|  54.228|  -10.007|01-jan-1990 07:00| 0.0| 5.3| 4.6|  3.6|  7.9|  89|1004.7|   8|  220|0.0|30000| 999|    4|\n",
      "|  Mayo|BELMULLET|  54.228|  -10.007|01-jan-1990 08:00| 0.0| 6.0| 5.4|  4.6|  8.5|  91|1004.9|  10|  190|0.0|30000| 999|    2|\n",
      "|  Mayo|BELMULLET|  54.228|  -10.007|01-jan-1990 09:00| 0.0| 6.1| 5.4|  4.5|  8.4|  89|1005.4|  12|  200|0.1|20000| 999|    3|\n",
      "+------+---------+--------+---------+-----------------+----+----+----+-----+-----+----+------+----+-----+---+-----+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.dropna().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "og12IYBk47Cm"
   },
   "source": [
    "This does give us a good idea, but it's not good enough, yet. Let's use the `describe()` method to get summary statistics of the DataFrame. \n",
    "\n",
    "> ðŸ’¡ &nbsp; **Did you know?**\n",
    ">\n",
    "> If the DataFrame has too many columns to display using the built-in Spark viewer, use the `toPandas()` method to view the DataFrame using the Pandas interface. It is important, however, to note that you should limit the size of the DataFrame in such a case as to not pull a massive DataFrame into memory, causing your computer to crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "ou0eJEzH47Cn",
    "outputId": "215f9397-55a3-4277-cac4-9fe9cd89e6ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>county</th>\n",
       "      <th>station</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>rain</th>\n",
       "      <th>temp</th>\n",
       "      <th>wetb</th>\n",
       "      <th>dewpt</th>\n",
       "      <th>vappr</th>\n",
       "      <th>rhum</th>\n",
       "      <th>msl</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>wddir</th>\n",
       "      <th>sun</th>\n",
       "      <th>vis</th>\n",
       "      <th>clht</th>\n",
       "      <th>clamt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>4660423</td>\n",
       "      <td>4660423</td>\n",
       "      <td>4660423</td>\n",
       "      <td>4660423</td>\n",
       "      <td>4660423</td>\n",
       "      <td>4660423</td>\n",
       "      <td>4660423</td>\n",
       "      <td>4660423</td>\n",
       "      <td>4660423</td>\n",
       "      <td>4660423</td>\n",
       "      <td>4660423</td>\n",
       "      <td>4660423</td>\n",
       "      <td>4431391</td>\n",
       "      <td>4431391</td>\n",
       "      <td>2075256</td>\n",
       "      <td>2075256</td>\n",
       "      <td>2075256</td>\n",
       "      <td>2075256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>53.25453260320943</td>\n",
       "      <td>-8.181231621900034</td>\n",
       "      <td>None</td>\n",
       "      <td>0.12406791040540457</td>\n",
       "      <td>9.967762231294808</td>\n",
       "      <td>8.67506133623306</td>\n",
       "      <td>7.229586436130947</td>\n",
       "      <td>10.578773017025613</td>\n",
       "      <td>83.67801382669663</td>\n",
       "      <td>1013.255110896992</td>\n",
       "      <td>9.672500479374923</td>\n",
       "      <td>199.95195723173794</td>\n",
       "      <td>0.15653534689590937</td>\n",
       "      <td>26897.97343722213</td>\n",
       "      <td>264.2836716962836</td>\n",
       "      <td>5.822756711669644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9898850408372967</td>\n",
       "      <td>1.2206812648919267</td>\n",
       "      <td>None</td>\n",
       "      <td>0.4845203822045314</td>\n",
       "      <td>4.711385132475347</td>\n",
       "      <td>4.31289799183682</td>\n",
       "      <td>4.473829797589463</td>\n",
       "      <td>3.1552922588702934</td>\n",
       "      <td>11.845824244118237</td>\n",
       "      <td>12.593850086665057</td>\n",
       "      <td>6.2040225382377905</td>\n",
       "      <td>90.89429813522699</td>\n",
       "      <td>0.3281719844755069</td>\n",
       "      <td>15665.079051528628</td>\n",
       "      <td>400.4829149145605</td>\n",
       "      <td>2.3707809390027625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>Carlow</td>\n",
       "      <td>ATHENRY</td>\n",
       "      <td>51.476000000000006</td>\n",
       "      <td>-10.007</td>\n",
       "      <td>01-apr-1990 00:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>Wexford</td>\n",
       "      <td>VALENTIA OBSERVATORY</td>\n",
       "      <td>55.372</td>\n",
       "      <td>-9.901</td>\n",
       "      <td>31-oct-2019 23:00</td>\n",
       "      <td>9.9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>99</td>\n",
       "      <td>999.9</td>\n",
       "      <td>97</td>\n",
       "      <td>90</td>\n",
       "      <td>4.9</td>\n",
       "      <td>9000</td>\n",
       "      <td>999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary   county               station            latitude  \\\n",
       "0   count  4660423               4660423             4660423   \n",
       "1    mean     None                  None   53.25453260320943   \n",
       "2  stddev     None                  None  0.9898850408372967   \n",
       "3     min   Carlow               ATHENRY  51.476000000000006   \n",
       "4     max  Wexford  VALENTIA OBSERVATORY              55.372   \n",
       "\n",
       "            longitude               date                 rain  \\\n",
       "0             4660423            4660423              4660423   \n",
       "1  -8.181231621900034               None  0.12406791040540457   \n",
       "2  1.2206812648919267               None   0.4845203822045314   \n",
       "3             -10.007  01-apr-1990 00:00                        \n",
       "4              -9.901  31-oct-2019 23:00                  9.9   \n",
       "\n",
       "                temp              wetb              dewpt               vappr  \\\n",
       "0            4660423           4660423            4660423             4660423   \n",
       "1  9.967762231294808  8.67506133623306  7.229586436130947  10.578773017025613   \n",
       "2  4.711385132475347  4.31289799183682  4.473829797589463  3.1552922588702934   \n",
       "3                                                                               \n",
       "4                9.9               9.9                9.9                 9.9   \n",
       "\n",
       "                 rhum                 msl                wdsp  \\\n",
       "0             4660423             4660423             4431391   \n",
       "1   83.67801382669663   1013.255110896992   9.672500479374923   \n",
       "2  11.845824244118237  12.593850086665057  6.2040225382377905   \n",
       "3                                                               \n",
       "4                  99               999.9                  97   \n",
       "\n",
       "                wddir                  sun                 vis  \\\n",
       "0             4431391              2075256             2075256   \n",
       "1  199.95195723173794  0.15653534689590937   26897.97343722213   \n",
       "2   90.89429813522699   0.3281719844755069  15665.079051528628   \n",
       "3                                                                \n",
       "4                  90                  4.9                9000   \n",
       "\n",
       "                clht               clamt  \n",
       "0            2075256             2075256  \n",
       "1  264.2836716962836   5.822756711669644  \n",
       "2  400.4829149145605  2.3707809390027625  \n",
       "3                                         \n",
       "4                999                   9  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bdk9EFmy47Cn"
   },
   "source": [
    "From that, let's define the schema.\n",
    "\n",
    "Many of the fields are numerical values. Spark has **Integer** types, **Decimal** types, and **Float** types, along with various others, which we have dealt with elsewhere. \n",
    "\n",
    "Parsing the DataFrame, we are not going to parse the date as a **timestamp** or **DateTime** just yet. As of the writing of this train, Spark does not have a very accurate **DateTime** or **timestamp** parser when reading DataFrames. As such, it is better to parse the **DateTime** where we can ensure that the coercion was done correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "__RfxL2z47Co"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType, TimestampType\n",
    "\n",
    "weather_schema = StructType([\n",
    "    StructField('county', StringType()),\n",
    "    StructField('station', StringType()),\n",
    "    StructField('latitude', FloatType()),\n",
    "    StructField('longitude', FloatType()),\n",
    "    StructField('date', StringType()), # While this is a date, it will likely not coerce correctly. Let's parse manually.\n",
    "    StructField('rain', FloatType()),\n",
    "    StructField('temp', FloatType()),\n",
    "    StructField('wetb', FloatType()),\n",
    "    StructField('dewpt', FloatType()),\n",
    "    StructField('vappr', FloatType()),\n",
    "    StructField('rhum', IntegerType()),\n",
    "    StructField('msl', FloatType()),\n",
    "    StructField('wdsp', IntegerType()),\n",
    "    StructField('wddir', IntegerType()),\n",
    "    StructField('sun', FloatType()),\n",
    "    StructField('vis', IntegerType()),\n",
    "    StructField('clht', IntegerType()),\n",
    "    StructField('clamt', IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bHdQBDef47Cp"
   },
   "outputs": [],
   "source": [
    "# Ensure that the below path matches your current working directory.\n",
    "typed_df = spark.read.csv('./data/weather_data/hrly_Irish_weather.csv', header=True, schema=weather_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odoNSWwM47Cq"
   },
   "source": [
    "Let's parse the date column using the `to_timestamp()` function, which takes the field to convert as the first argument and the format as the second (refer to [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) for date conventions). \n",
    "\n",
    "This will allow us to use all kinds of fancy methods on the `date` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eBbRtpn447Cq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "typed_df = typed_df.withColumn('date', F.to_timestamp(F.col('date'), format=\"dd-MMM-yyy HH:mm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- county: string (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- rain: float (nullable = true)\n",
      " |-- temp: float (nullable = true)\n",
      " |-- wetb: float (nullable = true)\n",
      " |-- dewpt: float (nullable = true)\n",
      " |-- vappr: float (nullable = true)\n",
      " |-- rhum: integer (nullable = true)\n",
      " |-- msl: float (nullable = true)\n",
      " |-- wdsp: integer (nullable = true)\n",
      " |-- wddir: integer (nullable = true)\n",
      " |-- sun: float (nullable = true)\n",
      " |-- vis: integer (nullable = true)\n",
      " |-- clht: integer (nullable = true)\n",
      " |-- clamt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "typed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------+---------+-------------------+----+----+----+-----+-----+----+------+----+-----+----+----+----+-----+\n",
      "|county|station|latitude|longitude|               date|rain|temp|wetb|dewpt|vappr|rhum|   msl|wdsp|wddir| sun| vis|clht|clamt|\n",
      "+------+-------+--------+---------+-------------------+----+----+----+-----+-----+----+------+----+-----+----+----+----+-----+\n",
      "|Galway|ATHENRY|  53.289|   -8.786|2011-06-26 01:00:00| 0.0|15.3|14.5| 13.9| 15.8|  90|1016.0|   8|  190|null|null|null| null|\n",
      "|Galway|ATHENRY|  53.289|   -8.786|2011-06-26 02:00:00| 0.0|14.7|13.7| 12.9| 14.9|  89|1015.8|   7|  190|null|null|null| null|\n",
      "|Galway|ATHENRY|  53.289|   -8.786|2011-06-26 03:00:00| 0.0|14.3|13.4| 12.6| 14.6|  89|1015.5|   6|  190|null|null|null| null|\n",
      "|Galway|ATHENRY|  53.289|   -8.786|2011-06-26 04:00:00| 0.0|14.4|13.6| 12.8| 14.8|  90|1015.3|   7|  180|null|null|null| null|\n",
      "|Galway|ATHENRY|  53.289|   -8.786|2011-06-26 05:00:00| 0.0|14.4|13.5| 12.7| 14.7|  89|1015.1|   6|  190|null|null|null| null|\n",
      "|Galway|ATHENRY|  53.289|   -8.786|2011-06-26 06:00:00| 0.0|14.9|13.8| 13.0| 15.0|  88|1015.0|   8|  180|null|null|null| null|\n",
      "|Galway|ATHENRY|  53.289|   -8.786|2011-06-26 07:00:00| 0.0|15.1|13.5| 12.2| 14.2|  83|1014.9|   8|  180|null|null|null| null|\n",
      "|Galway|ATHENRY|  53.289|   -8.786|2011-06-26 08:00:00| 0.0|15.2|13.0| 11.1| 13.2|  76|1014.7|   7|  170|null|null|null| null|\n",
      "|Galway|ATHENRY|  53.289|   -8.786|2011-06-26 09:00:00| 0.0|16.1|13.2| 10.8| 12.9|  70|1014.4|   7|  160|null|null|null| null|\n",
      "|Galway|ATHENRY|  53.289|   -8.786|2011-06-26 10:00:00| 0.0|16.8|13.3| 10.3| 12.5|  65|1014.1|  10|  180|null|null|null| null|\n",
      "+------+-------+--------+---------+-------------------+----+----+----+-----+-----+----+------+----+-----+----+----+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "typed_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "dZn-z9gU47Ct",
    "outputId": "bbb70dad-fb10-4bd6-9032-cd9c3a79a1a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>county</th>\n",
       "      <th>station</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>rain</th>\n",
       "      <th>temp</th>\n",
       "      <th>wetb</th>\n",
       "      <th>dewpt</th>\n",
       "      <th>vappr</th>\n",
       "      <th>rhum</th>\n",
       "      <th>msl</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>wddir</th>\n",
       "      <th>sun</th>\n",
       "      <th>vis</th>\n",
       "      <th>clht</th>\n",
       "      <th>clamt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>4660423</td>\n",
       "      <td>4660423</td>\n",
       "      <td>4660423</td>\n",
       "      <td>4660423</td>\n",
       "      <td>4548758</td>\n",
       "      <td>4627842</td>\n",
       "      <td>4615135</td>\n",
       "      <td>4616264</td>\n",
       "      <td>4472939</td>\n",
       "      <td>4496808</td>\n",
       "      <td>4587140</td>\n",
       "      <td>4349414</td>\n",
       "      <td>4334055</td>\n",
       "      <td>1843896</td>\n",
       "      <td>1805007</td>\n",
       "      <td>1843878</td>\n",
       "      <td>1843878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>53.25453252207271</td>\n",
       "      <td>-8.18123172288137</td>\n",
       "      <td>0.12406791096679713</td>\n",
       "      <td>9.967762233979487</td>\n",
       "      <td>8.675061338530968</td>\n",
       "      <td>7.229586438822406</td>\n",
       "      <td>10.578773020413783</td>\n",
       "      <td>83.67801382669663</td>\n",
       "      <td>1013.2551108660023</td>\n",
       "      <td>9.672500479374923</td>\n",
       "      <td>199.95195723173794</td>\n",
       "      <td>0.15653534733309613</td>\n",
       "      <td>26897.97343722213</td>\n",
       "      <td>264.2836716962836</td>\n",
       "      <td>5.822756711669644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9898850775903211</td>\n",
       "      <td>1.2206812705571317</td>\n",
       "      <td>0.4845203822875591</td>\n",
       "      <td>4.71138513295609</td>\n",
       "      <td>4.312897992278999</td>\n",
       "      <td>4.473829799427647</td>\n",
       "      <td>3.155292257692218</td>\n",
       "      <td>11.845824244118237</td>\n",
       "      <td>12.593850056415128</td>\n",
       "      <td>6.2040225382377905</td>\n",
       "      <td>90.89429813522699</td>\n",
       "      <td>0.32817198472644526</td>\n",
       "      <td>15665.079051528628</td>\n",
       "      <td>400.4829149145605</td>\n",
       "      <td>2.3707809390027625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>Carlow</td>\n",
       "      <td>ATHENRY</td>\n",
       "      <td>51.476</td>\n",
       "      <td>-10.241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.3</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>-92.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14</td>\n",
       "      <td>943.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>Wexford</td>\n",
       "      <td>VALENTIA OBSERVATORY</td>\n",
       "      <td>55.372</td>\n",
       "      <td>-6.241</td>\n",
       "      <td>41.4</td>\n",
       "      <td>31.5</td>\n",
       "      <td>24.9</td>\n",
       "      <td>23.8</td>\n",
       "      <td>29.5</td>\n",
       "      <td>100</td>\n",
       "      <td>1051.2</td>\n",
       "      <td>97</td>\n",
       "      <td>360</td>\n",
       "      <td>4.9</td>\n",
       "      <td>75000</td>\n",
       "      <td>999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary   county               station            latitude  \\\n",
       "0   count  4660423               4660423             4660423   \n",
       "1    mean     None                  None   53.25453252207271   \n",
       "2  stddev     None                  None  0.9898850775903211   \n",
       "3     min   Carlow               ATHENRY              51.476   \n",
       "4     max  Wexford  VALENTIA OBSERVATORY              55.372   \n",
       "\n",
       "            longitude                 rain               temp  \\\n",
       "0             4660423              4548758            4627842   \n",
       "1   -8.18123172288137  0.12406791096679713  9.967762233979487   \n",
       "2  1.2206812705571317   0.4845203822875591   4.71138513295609   \n",
       "3             -10.241                  0.0              -17.3   \n",
       "4              -6.241                 41.4               31.5   \n",
       "\n",
       "                wetb              dewpt               vappr  \\\n",
       "0            4615135            4616264             4472939   \n",
       "1  8.675061338530968  7.229586438822406  10.578773020413783   \n",
       "2  4.312897992278999  4.473829799427647   3.155292257692218   \n",
       "3              -99.9              -92.4                 0.0   \n",
       "4               24.9               23.8                29.5   \n",
       "\n",
       "                 rhum                 msl                wdsp  \\\n",
       "0             4496808             4587140             4349414   \n",
       "1   83.67801382669663  1013.2551108660023   9.672500479374923   \n",
       "2  11.845824244118237  12.593850056415128  6.2040225382377905   \n",
       "3                 -14               943.2                   0   \n",
       "4                 100              1051.2                  97   \n",
       "\n",
       "                wddir                  sun                 vis  \\\n",
       "0             4334055              1843896             1805007   \n",
       "1  199.95195723173794  0.15653534733309613   26897.97343722213   \n",
       "2   90.89429813522699  0.32817198472644526  15665.079051528628   \n",
       "3                   0                  0.0                   5   \n",
       "4                 360                  4.9               75000   \n",
       "\n",
       "                clht               clamt  \n",
       "0            1843878             1843878  \n",
       "1  264.2836716962836   5.822756711669644  \n",
       "2  400.4829149145605  2.3707809390027625  \n",
       "3                  0                   0  \n",
       "4                999                   9  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typed_df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's output this to a destination. In this notebook, we will use `parquet` in the same directory. \n",
    "\n",
    "This has various advantages, the first of which is that it allows us to read it in again while maintaining the fields, field names, and other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BTooh6o147Ct",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ensure that the below path matches your current working directory.\n",
    "typed_df.write.parquet('./data/weather_data/hrly_Irish_weather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5T_mSVBF47Ct"
   },
   "source": [
    "## Translation and mapping\n",
    "\n",
    "Translation is the process of making the data more human-understandable, for example, changing work order codes in a field into human-readable descriptions of the work done. This process is important to ensure the data is understandable for a customer-facing product, but can also assist data scientists and analysts in the process of creating intelligence and analytics. \n",
    "\n",
    "Mapping is a similar process whereby we create a map between the source system and destination system, typically at the field level, for example, changing column names to conform to the destination system. This is especially relevant between different data systems (for example, JSON or XML to a relational database), where different constraints may exist on the field names. \n",
    "\n",
    "Mapping of datasets can be done through metadata, which explains the data fields and attributes and constitutes the data and rules that govern how the data is stored within the database or data repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68EzZfTS47C1"
   },
   "source": [
    "For this particular dataset, the abbreviations of the above field are:\n",
    "\n",
    "- `rain` - Rain (mm) \n",
    "- `temp` - Temperature (Â°C) \n",
    "- `wetb` - Wet Bulb Air Temperature (Â°C) \n",
    "- `dewpt` - Dew Point\n",
    "- `vappr` - Vapour Pressure (hPa) \n",
    "- `rhum` - Relative Humidity (%) \n",
    "- `msl` - Mean Sea Level Pressure (hPa) \n",
    "- `wdsp` - Mean Hourly Wind Speed (kt)\n",
    "- `wddir` - Predominant Hourly Wind Direction (degrees)\n",
    "- `sun` - Sun (hours) \n",
    "- `vis` - Visibility (m)\n",
    "- `clht` - Cloud Ceiling Height \n",
    "- `clamt` - Cloud Amount (Oktas)\n",
    "\n",
    "This will come in very handy when we perform data integrity checks. It may also help us confirm that we have done the typecasting correctly. We got this specifically from the metadata file that was included alongside the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88VYKxt_47C2"
   },
   "source": [
    "## Filtering, aggregation, and summarisation\n",
    "\n",
    "Part of the process of making data more manageable and consistent is filtering, aggregating, and summarising the data. \n",
    "This can be done by filtering out `null` values, invalid records, or irrelevant fields. A large part of this phase is the exploration of the dataset for such errors and challenges, a process that is analogous to a data scientist performing an [Exploratory Data Analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis).\n",
    "\n",
    "It can also be done by aggregating data to a level that is more appropriate for analytical teams, or which will simplify the visualisation of datasets. For example, aggregating data up to a high geographical level (for visualising COVID-19 cases at the level of a country vs. a city) or aggregating time-series data up to the hour instead of the second. \n",
    "\n",
    "There is an almost inexhaustible list of errors that can be present within your dataset when performing these tasks. We will try to cover some of the most prominent issues that you may encounter:\n",
    "\n",
    "- duplicated data;\n",
    "- spurious observations;\n",
    "- structural errors; and\n",
    "- missing observations.\n",
    "\n",
    "We will deal with the first three in this section and take a look at the fourth but only deal with it in the next section.\n",
    "\n",
    "Now that we have the data in parquet format, we can easily read it in again without having to specify the schema again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "trqYdT1R47C2"
   },
   "outputs": [],
   "source": [
    "# Read the data in using the read.parquet() method from existing parquet files.\n",
    "\n",
    "working_df = spark.read.parquet('./data/weather_data/hrly_Irish_weather/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSJ_sasp47C2"
   },
   "source": [
    "Make a list of categorical and continuous variables. This may be useful if we want to filter by these variable fields later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zmDNFWdP47C2"
   },
   "outputs": [],
   "source": [
    "weather_cat = ['county', 'station', 'date']\n",
    "weather_int = ['rhum', 'wdsp', 'wddir', 'vis', 'clht', 'clamt']\n",
    "\n",
    "# Create a list of continuous fields (fields that are not in the categorical list above).\n",
    "weather_cont = [x for x in working_df.columns if x not in weather_cat]\n",
    "\n",
    "# Create a list of float fields (those in the above list, but not in the integer list).\n",
    "weather_float = [x for x in weather_cont if x not in weather_int] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqmNHyAA47C3"
   },
   "source": [
    "### 1. Duplicated data\n",
    "\n",
    "A feature of uncleansed data may sometimes be the occurrence of duplicated data points. Removing these redundant pieces of data forms part of the filtering process.\n",
    "\n",
    "The origin of the duplicates may be the result of an internal process (true duplicates) or due to a failure in the system, such as incorrect data capture or incorrect data processing. \n",
    "\n",
    "To combat such duplications, it is always important to check and remove duplicate entries within a dataset, and this is done through a deduplication process. \n",
    "\n",
    "Deduplication can be simple and is normally based on numerical entries or entries that are well indexed. An example of this might be transactional data in a point-of-sale system, which has a primary key but was processed incorrectly. \n",
    "\n",
    "Table 1. An example of duplicated records.\n",
    "\n",
    "|Index|Date|Name|Value|\n",
    "|--|--|--|--|\n",
    "|1|2021-01-01|Ben|20|\n",
    "|2|2021-01-01|Sarah|12|\n",
    "|3|2021-01-01|Jack|24|\n",
    "|3|2021-01-01|Jack|24|\n",
    "|4|2021-01-01|Ben|20|\n",
    "\n",
    "In the above table, we demonstrate the types of duplications that can occur. Rows with index 3 indicate two duplicates that likely resulted from a system failure, causing the entry to be duplicated within our system. We should remove this type of duplicate from our system. Rows with index 1 and 4 are true duplicates according to the above definition and are likely a manifestation of the underlying process. Ben probably went to the shop twice on the 1st of January 2021 and bought goods to the value of 20. We would want to keep these duplicates in our system as they likely represent real processes.\n",
    "\n",
    "Alternatively, deduplication can be very complex, with the only unique identifier being a string column, in which case, fuzzy logic or some similar technique has to be applied to identify and remove duplicates. An example of this might be names that are in an HR list, some of which have been entered incorrectly, or consolidation between names in multiple systems. \n",
    "\n",
    "Let's start by checking if all entries within the datasets are unique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "toisnTCr47C3",
    "outputId": "41a7c8b4-17c8-4bf7-cdc3-3706aab4182e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------+---------+----+----+----+----+-----+-----+----+---+----+-----+---+---+----+-----+-----+\n",
      "|county|station|latitude|longitude|date|rain|temp|wetb|dewpt|vappr|rhum|msl|wdsp|wddir|sun|vis|clht|clamt|count|\n",
      "+------+-------+--------+---------+----+----+----+----+-----+-----+----+---+----+-----+---+---+----+-----+-----+\n",
      "+------+-------+--------+---------+----+----+----+----+-----+-----+----+---+----+-----+---+---+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df.groupby(working_df.columns) \\\n",
    "  .count() \\\n",
    "  .where('count > 1') \\\n",
    "  .sort('count', ascending=False) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1ewZUHn47C4"
   },
   "source": [
    "Here we are using the `groupby()` method with all the columns as argument, meaning we are grouping by all the columns. We then use the `count()` aggregation method, modify that with the `where()` method, and only retrieving rows where the count is larger than one. Finally, we sort by the count column using the `sort()` method and display the resulting DataFrame.\n",
    "\n",
    "That looks about right... no duplicates across the whole dataset!\n",
    "\n",
    "It does, however, feel like we expected that. It is also just more plausible, statistically. What if we try to create a compound primary index using the date and the station? That should still be unique across the dataset. We expect there to be no double entry per station at any one point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TO46bnLu47C5",
    "outputId": "fbe1b953-f0ed-4a81-9977-1cad3c0a7888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|date|station|count|\n",
      "+----+-------+-----+\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df.groupby(['date', 'station']) \\\n",
    "  .count() \\\n",
    "  .where('count > 1') \\\n",
    "  .sort('count', ascending=False) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8W6C1Dc47C5"
   },
   "source": [
    "Here we are doing exactly the same as above, except we're only grouping by the `date` and `station` columns.\n",
    "\n",
    "No duplicates... nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqNwT5kE47C5"
   },
   "source": [
    "### 2. Spurious observations\n",
    "\n",
    "Another feature of a dataset that has not yet gone through data cleansing may be the occurrence of spurious observations. \n",
    "\n",
    "Spurious observations may have many origins: humans performing data capture may enter incorrect data, an incorrect understanding of the input data may lead to incorrect entries (for example, 0 being entered as O), or automated data capture processes may lead to incorrect entries in a similar way (for example, an OCR system may read an S in as a 5). Other errors may be even more domain-specific (for example, an acoustic sewer level sensor may give negative readings when the water level is above the sensor, or when covered by a large object). It is clear that we have to make sure that all the values within the input data make sense within the bounded context of the domain we are working in. \n",
    "\n",
    "For our dataset, we have defined the field identities above. Let's revisit them and define possible bounds for them:\n",
    "\n",
    "- `rain` â€“ Rain (mm) â€“ any positive decimal number.\n",
    "- `temp` â€“ Temperature (Â°C) â€“ any decimal number above -273.15 (expect to be between -20 and 40).\n",
    "- `wetb` â€“ Wet Bulb Air Temperature (Â°C) â€“ any decimal number above -273.15 (expect to be between -20 and 40).\n",
    "- `dewpt` â€“ Dew Point â€“ any decimal number above -273.15 (expect to be between -20 and 40).\n",
    "- `vappr` â€“ Vapour Pressure (hPa) â€“ any positive decimal number.\n",
    "- `rhum` â€“ Relative Humidity (%) â€“ between 0 and 100.\n",
    "- `msl` â€“ Mean Sea Level Pressure (hPa) â€“ any positive decimal number (expect close to 1,013.25).\n",
    "- `wdsp` â€“ Mean Hourly Wind Speed (kt) â€“ any positive decimal number (maximum at 220 - fastest ever recorded).\n",
    "- `wddir` â€“ Predominant Hourly Wind Direction (degrees) â€“ between 0 and 360.\n",
    "- `sun` â€“ Sun (hours) â€“ between 0 and 24.\n",
    "- `vis` â€“ Visibility (m) â€“ any positive decimal number.\n",
    "- `clht` â€“ Cloud Ceiling Height â€“ between 0 and 999.\n",
    "- `clamt` â€“ Cloud Amount (Oktas) â€“ between 0 and 9.\n",
    "\n",
    "Let's quickly run through checks for all of the above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eMtZGVf47C6"
   },
   "source": [
    "#### 1. `rain` > 0\n",
    "\n",
    "Rain should be positive. Let's see if we have any negative records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W62Sm1pj47C6",
    "outputId": "aa22db8d-02dc-4e77-c7b5-9b8fb0048618",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------+---------+----+----+----+----+-----+-----+----+---+----+-----+---+---+----+-----+\n",
      "|county|station|latitude|longitude|date|rain|temp|wetb|dewpt|vappr|rhum|msl|wdsp|wddir|sun|vis|clht|clamt|\n",
      "+------+-------+--------+---------+----+----+----+----+-----+-----+----+---+----+-----+---+---+----+-----+\n",
      "+------+-------+--------+---------+----+----+----+----+-----+-----+----+---+----+-----+---+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df.where(F.col('rain') < 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kmr0s0Wi47C6"
   },
   "source": [
    "#### 2. `temp` between -20 and 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZdAflL247C6",
    "outputId": "80e06036-5f5d-4733-9727-c3edcde03af6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------+---------+----+----+----+----+-----+-----+----+---+----+-----+---+---+----+-----+\n",
      "|county|station|latitude|longitude|date|rain|temp|wetb|dewpt|vappr|rhum|msl|wdsp|wddir|sun|vis|clht|clamt|\n",
      "+------+-------+--------+---------+----+----+----+----+-----+-----+----+---+----+-----+---+---+----+-----+\n",
      "+------+-------+--------+---------+----+----+----+----+-----+-----+----+---+----+-----+---+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df.where(~F.col('temp').between(-20, 40)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKRINFbb47C6"
   },
   "source": [
    "But... I thought we are filtering records 'where the temp is between -20 and 40'. Why are we not getting anything?\n",
    "\n",
    "This is where the `~` operator comes in, which means 'everything except'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXcyYHj347C6"
   },
   "source": [
    "#### 3. `wetb` between -20 and 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYeNc8Un47C6",
    "outputId": "da9128f5-ccc3-4a84-e31d-e1f869606934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+---------+-------------------+----+----+-----+-----+-----+----+------+----+-----+---+-----+----+-----+\n",
      "|county| station|latitude|longitude|               date|rain|temp| wetb|dewpt|vappr|rhum|   msl|wdsp|wddir|sun|  vis|clht|clamt|\n",
      "+------+--------+--------+---------+-------------------+----+----+-----+-----+-----+----+------+----+-----+---+-----+----+-----+\n",
      "|Dublin|CASEMENT|  53.306|   -6.439|2017-08-01 00:00:00| 0.0|12.5|-49.0| 11.1| 13.3|  92|1009.2|   6|  210|0.0|20000|  70|    7|\n",
      "|Dublin|CASEMENT|  53.306|   -6.439|2017-08-31 21:00:00| 0.0| 8.3|-49.0|  7.0| 10.0|  92|1021.4|   4|  210|0.0|30000| 999|    2|\n",
      "|Dublin|CASEMENT|  53.306|   -6.439|2017-09-01 00:00:00| 0.0| 8.0|-49.0|  6.8|  9.8|  92|1022.5|   4|  220|0.0|20000| 999|    1|\n",
      "|Dublin|CASEMENT|  53.306|   -6.439|2018-01-28 10:00:00| 0.0|11.7|-49.0| 10.3| 12.6|  91|1025.5|  20|  240|0.0| 9000|  11|    8|\n",
      "|Dublin|CASEMENT|  53.306|   -6.439|2019-06-30 08:00:00| 0.0|16.1|-49.0| 12.6| 14.7|  80|1012.5|  14|  230|0.9|40000| 999|    2|\n",
      "|Dublin|CASEMENT|  53.306|   -6.439|2019-06-30 09:00:00| 0.0|16.8|-49.0| 11.7| 13.7|  72|1012.6|  15|  230|0.1|40000|  50|    6|\n",
      "|Dublin|CASEMENT|  53.306|   -6.439|2019-06-30 10:00:00| 0.0|16.7|-49.0| 11.6| 13.6|  72|1013.0|  15|  220|0.0|30000|  30|    6|\n",
      "|Dublin|CASEMENT|  53.306|   -6.439|2019-06-30 11:00:00| 0.0|16.9|-49.0| 12.6| 14.6|  76|1013.2|  15|  230|0.1|30000|  30|    6|\n",
      "|Dublin|CASEMENT|  53.306|   -6.439|2019-06-30 12:00:00| 0.6|15.4|-49.0| 14.7| 16.7|  96|1013.7|  13|  250|0.3|25000|  40|    6|\n",
      "|Dublin|CASEMENT|  53.306|   -6.439|2019-06-30 13:00:00| 0.4|16.6|-49.0| 13.9| 16.0|  85|1013.5|  14|  240|0.7|25000|  50|    6|\n",
      "+------+--------+--------+---------+-------------------+----+----+-----+-----+-----+----+------+----+-----+---+-----+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df.where(~F.col('wetb').between(-20, 40)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-VpX7PQ47C7"
   },
   "source": [
    "**Weird**... let's have a look at how many of these we have and how extreme they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B8Yr8Ejg47C7",
    "outputId": "55a3c247-32b9-4655-e390-4fb5d40dfcb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-----+\n",
      "| wetb|        station|count|\n",
      "+-----+---------------+-----+\n",
      "|-49.0|   ROCHES POINT|   99|\n",
      "|-49.0|   CORK AIRPORT|   92|\n",
      "|-99.9|    CLAREMORRIS|    9|\n",
      "|-49.0|       CASEMENT|   86|\n",
      "|-49.0|SHANNON AIRPORT|  103|\n",
      "|-49.9| DUBLIN AIRPORT|    1|\n",
      "+-----+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df.where(~F.col('wetb').between(-20, 40)).groupBy('wetb', 'station').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsIsigKt47C7"
   },
   "source": [
    "That is quite a lot. It is also only three very specific values spread across many stations. Let's look at where they are located to see if we can find a pattern:\n",
    "\n",
    "\n",
    "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Processing%20Big%20Data/ireland-weather.png\"\n",
    "     alt=\"Ireland\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=600px/>\n",
    "    <p>Figure 1: A map of Ireland.</p>\n",
    "</div>\n",
    "\n",
    "They are not at all clustered or far north. We expect these to be default values that the loggers return if they are faulty.\n",
    "\n",
    "We can note this and handoff to the modelling teams or remove these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TX-W2fwZ47C7"
   },
   "source": [
    "#### 4. `dewpt` between -20 and 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "580Zu_b747C7",
    "outputId": "302022df-6b24-42ca-a4f7-bdccf3defad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+---------+-------------------+----+----+----+-----+-----+----+------+----+-----+----+----+----+-----+\n",
      "|county|    station|latitude|longitude|               date|rain|temp|wetb|dewpt|vappr|rhum|   msl|wdsp|wddir| sun| vis|clht|clamt|\n",
      "+------+-----------+--------+---------+-------------------+----+----+----+-----+-----+----+------+----+-----+----+----+----+-----+\n",
      "|  Mayo|CLAREMORRIS|  53.711|   -8.993|2003-01-30 18:00:00| 0.0| 0.4|-4.0|-22.3|  1.0|  16|1030.6|   8|   10|null|null|null| null|\n",
      "|  Mayo|CLAREMORRIS|  53.711|   -8.993|2003-01-30 19:00:00| 0.0|-0.2|-4.5|-22.9|  0.9|  16|1030.9|   5|  310|null|null|null| null|\n",
      "|  Mayo|CLAREMORRIS|  53.711|   -8.993|2003-01-30 20:00:00| 0.0| 0.4|-4.0|-22.3|  1.0|  16|1031.3|   6|  320|null|null|null| null|\n",
      "|  Mayo|CLAREMORRIS|  53.711|   -8.993|2003-01-30 21:00:00| 0.0|-0.2|-4.5|-22.8|  0.9|  16|1031.7|   5|  310|null|null|null| null|\n",
      "|  Mayo|CLAREMORRIS|  53.711|   -8.993|2003-01-30 22:00:00| 0.0| 0.1|-4.2|-22.1|  1.0|  17|1031.5|   5|  300|null|null|null| null|\n",
      "|  Mayo|CLAREMORRIS|  53.711|   -8.993|2003-01-30 23:00:00| 0.0|-0.6|-4.7|-22.7|  1.0|  18|1031.6|   4|  300|null|null|null| null|\n",
      "|  Mayo|CLAREMORRIS|  53.711|   -8.993|2003-01-31 00:00:00| 0.0| 0.0|-4.3|-22.2|  1.0|  17|1031.9|   5|  270|null|null|null| null|\n",
      "|  Mayo|CLAREMORRIS|  53.711|   -8.993|2003-01-31 01:00:00| 0.0| 0.4|-4.0|-21.8|  1.0|  16|1031.8|   5|  290|null|null|null| null|\n",
      "|  Mayo|CLAREMORRIS|  53.711|   -8.993|2003-01-31 02:00:00| 0.0| 0.5|-3.9|-21.7|  1.1|  17|1031.2|   3|  300|null|null|null| null|\n",
      "|  Mayo|CLAREMORRIS|  53.711|   -8.993|2003-01-31 03:00:00| 0.0| 0.3|-4.0|-21.9|  1.1|  18|1030.5|   3|  270|null|null|null| null|\n",
      "+------+-----------+--------+---------+-------------------+----+----+----+-----+-----+----+------+----+-----+----+----+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df.where(~F.col('dewpt').between(-20, 40)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKOsu3nH47C7",
    "outputId": "ce45b988-9068-4f2a-ac59-c5bf67de6cd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----+\n",
      "|dewpt|    station|count|\n",
      "+-----+-----------+-----+\n",
      "|-21.9|CLAREMORRIS|    4|\n",
      "|-22.9|CLAREMORRIS|    1|\n",
      "|-21.7|CLAREMORRIS|    2|\n",
      "|-22.1|CLAREMORRIS|    1|\n",
      "|-21.8|CLAREMORRIS|    2|\n",
      "|-22.8|CLAREMORRIS|    1|\n",
      "|-22.7|CLAREMORRIS|    1|\n",
      "|-22.2|CLAREMORRIS|    1|\n",
      "|-22.3|CLAREMORRIS|    2|\n",
      "|-21.3|CLAREMORRIS|    1|\n",
      "|-92.4| MOORE PARK|   15|\n",
      "+-----+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df.where(~F.col('dewpt').between(-20, 40)).groupBy('dewpt', 'station').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XiWqlJV47C7"
   },
   "source": [
    "We could have been wrong in our assumption of no readings lower than -20 since the readings at CLAREMORRIS seem to be legitimate readings. However, the MOORE PARK reading seems too low, and as above with `wetb`, we should remove these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njZC3ce847C7"
   },
   "source": [
    "Those were all relative examples, for example, checking if temperatures fell within a specific range.\n",
    "\n",
    "Let's check for dependencies between columns. \n",
    "We would expect the dew point to be lower than the current temperature. Let's see if that is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "APCIEjqb47C8",
    "outputId": "5d653b9b-e118-43ba-da0c-103d22eacfa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+---------+-------------------+----+----+----+-----+-----+----+------+----+-----+----+----+----+-----+\n",
      "|county|   station|latitude|longitude|               date|rain|temp|wetb|dewpt|vappr|rhum|   msl|wdsp|wddir| sun| vis|clht|clamt|\n",
      "+------+----------+--------+---------+-------------------+----+----+----+-----+-----+----+------+----+-----+----+----+----+-----+\n",
      "|Galway|   ATHENRY|  53.289|   -8.786|2012-12-10 06:00:00| 0.0|-3.0|-2.9| -2.9|  4.9| 100|1025.9|   2|  140|null|null|null| null|\n",
      "|Galway|   ATHENRY|  53.289|   -8.786|2013-01-09 09:00:00| 0.0|-1.4|-1.3| -1.3|  5.5| 100|1022.9|   3|   90|null|null|null| null|\n",
      "|Galway|   ATHENRY|  53.289|   -8.786|2013-01-15 08:00:00| 0.0|-1.7|-1.6| -1.6|  5.4| 100|1014.7|   1|  200|null|null|null| null|\n",
      "|Galway|   ATHENRY|  53.289|   -8.786|2013-03-17 06:00:00| 0.0|-2.9|-2.8| -2.8|  5.0| 100| 991.4|   1|   40|null|null|null| null|\n",
      "|Galway|   ATHENRY|  53.289|   -8.786|2013-03-28 05:00:00| 0.0|-1.8|-1.7| -1.7|  5.4| 100|1015.2|   6|   50|null|null|null| null|\n",
      "| Cavan|BALLYHAISE|  54.051|    -7.31|2007-10-25 08:00:00| 0.0|-1.8|-1.7| -1.7|  5.4| 100|1026.0|   3|  140|null|null|null| null|\n",
      "| Cavan|BALLYHAISE|  54.051|    -7.31|2008-02-17 13:00:00| 0.0|-0.1|-0.0| -0.0|  6.1| 100|1039.8|   1|  100|null|null|null| null|\n",
      "| Cavan|BALLYHAISE|  54.051|    -7.31|2011-11-06 04:00:00| 0.0|-1.2|-1.1| -1.1|  5.6| 100|1024.2|   3|  100|null|null|null| null|\n",
      "| Cavan|BALLYHAISE|  54.051|    -7.31|2014-03-12 08:00:00| 0.0|-2.1|-2.0| -2.0|  5.3| 100|1033.4|   1|  100|null|null|null| null|\n",
      "|Dublin|  CASEMENT|  53.306|   -6.439|2008-02-19 00:00:00| 0.0|-0.1|-0.1|  0.0|  6.1| 100|1028.4|   2|  140| 0.0|5000| 999|    0|\n",
      "+------+----------+--------+---------+-------------------+----+----+----+-----+-----+----+------+----+-----+----+----+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df.where(F.col('dewpt') > F.col('temp')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KLco0O-U47C8",
    "outputId": "54f1584a-9b34-431f-b56a-bf922ce642ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|   county|count|\n",
      "+---------+-----+\n",
      "|    Clare|    1|\n",
      "|Roscommon|   31|\n",
      "|   Dublin|  103|\n",
      "|   Galway|    5|\n",
      "|     Cork|  106|\n",
      "|Tipperary|   10|\n",
      "|     Mayo|   54|\n",
      "|    Meath|    7|\n",
      "|   Carlow|   24|\n",
      "|Westmeath|   20|\n",
      "|    Sligo|    2|\n",
      "|    Kerry|    2|\n",
      "|    Cavan|    4|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df.where(F.col('dewpt') > F.col('temp')).groupBy('county').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV80Uidu47C8"
   },
   "source": [
    "You can see that it's possible to do comparisons between columns as well.\n",
    "\n",
    "It does not look like we can use the relationship between temperature and dew point to determine valid entries. If we look at the counties where there are dew points higher than the temperatures, there are quite a few. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0AgjXuh47C8"
   },
   "source": [
    "#### \\[Exercise\\]\n",
    "\n",
    "As an exercise, complete the data checks for the remainder of the expected values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VnoiRI047C8"
   },
   "source": [
    "#### 5. `vappr` a positive decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLcZN0Ni47C8",
    "outputId": "7a03f648-bb6e-4c0f-fea6-b988df92f8f8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhiXpsdG47C8"
   },
   "source": [
    "#### 6. `rhum` between 0 and 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rnrFJuER47C8",
    "outputId": "57f04b0a-5919-40aa-f6a7-e847227db0a3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2rZCtLm47C8"
   },
   "source": [
    "#### 7. `msl` any positive decimal number close to 1013.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sibbSKFA47C8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbz4vhw-47C9"
   },
   "source": [
    "#### 8 `wdsp` any positive decimal number below 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvfEZqL_47C9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FegMQDr47C9"
   },
   "source": [
    "#### 9. `wddir` between 0 and 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWUbjnxD47C9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYENR8R647C9"
   },
   "source": [
    "#### 10. `sun` between 0 and 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNznE3mk47C9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0OF7TIR47C9"
   },
   "source": [
    "#### 11. `vis` any positive decimal number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVz04KCN47C9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LFrJ2B_47C9"
   },
   "source": [
    "#### 12. `clht` integer between 0 and 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSI2c0yF47C-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkIWcYmT47C-"
   },
   "source": [
    "#### 13. `clamt` integer between 0 and 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDv8q6U047C-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b3LA6JI47C-"
   },
   "source": [
    "#### List of errors:\n",
    "\n",
    "1. `wetb` removes values where `wetb` == 49.0, 49.9 and 99.0\n",
    "2. `dewpt` removes values where `dewpt` == 92.4\n",
    "3. `rhum` removes values where `rhum` == -14\n",
    "4. `clamt` removes null values in the `clamt` field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxIHjWsB47C-"
   },
   "source": [
    "### 3. Structural errors\n",
    "\n",
    "Lots of things can go wrong when working with large datasets that come from source systems over which you do not have full control (a common thing when working as a data engineer). \n",
    "\n",
    "Similar to spurious observations, upstream changes can lead to incorrect structures for our processing systems. While addressing structural errors usually happens at the `extraction and parsing` phase of the data engineering process, the diagnosis typically happens during this phase. \n",
    "\n",
    "Usually, structural errors are picked up as malformed columns or value types that are incorrect within the specific fields (for example, strings within numerical columns) or by manual inspection of string fields (for example, having a county name in the station field). \n",
    "\n",
    "To perform this check, we have to look at the dataset before Spark coerced any records into the structure/schema that we specified. To do that, we will refer back to the `raw_df`. Let's have a look again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65VZ2Tkx47C-",
    "outputId": "e2b55dc8-a748-4d36-89ce-8282adb4bd23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------------+---------+-----------------+----+----+----+-----+-----+----+------+----+-----+----+----+----+-----+\n",
      "|county|station|          latitude|longitude|             date|rain|temp|wetb|dewpt|vappr|rhum|   msl|wdsp|wddir| sun| vis|clht|clamt|\n",
      "+------+-------+------------------+---------+-----------------+----+----+----+-----+-----+----+------+----+-----+----+----+----+-----+\n",
      "|Galway|ATHENRY|53.288999999999994|   -8.786|26-jun-2011 01:00| 0.0|15.3|14.5| 13.9| 15.8|  90|1016.0|   8|  190|null|null|null| null|\n",
      "|Galway|ATHENRY|53.288999999999994|   -8.786|26-jun-2011 02:00| 0.0|14.7|13.7| 12.9| 14.9|  89|1015.8|   7|  190|null|null|null| null|\n",
      "|Galway|ATHENRY|53.288999999999994|   -8.786|26-jun-2011 03:00| 0.0|14.3|13.4| 12.6| 14.6|  89|1015.5|   6|  190|null|null|null| null|\n",
      "|Galway|ATHENRY|53.288999999999994|   -8.786|26-jun-2011 04:00| 0.0|14.4|13.6| 12.8| 14.8|  90|1015.3|   7|  180|null|null|null| null|\n",
      "|Galway|ATHENRY|53.288999999999994|   -8.786|26-jun-2011 05:00| 0.0|14.4|13.5| 12.7| 14.7|  89|1015.1|   6|  190|null|null|null| null|\n",
      "+------+-------+------------------+---------+-----------------+----+----+----+-----+-----+----+------+----+-----+----+----+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6aPxEP247C-"
   },
   "source": [
    "Let's have a look at the entries in the string columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wrle0BdA47C-",
    "outputId": "e6db5c80-915a-47a8-8cc2-59cf36f5166b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|   county| count|\n",
      "+---------+------+\n",
      "|    Clare|266617|\n",
      "|  Wexford|135192|\n",
      "|Roscommon|108192|\n",
      "|   Dublin|653378|\n",
      "|  Donegal|460028|\n",
      "|   Galway|216792|\n",
      "|     Cork|802513|\n",
      "|Tipperary|108144|\n",
      "|     Mayo|877279|\n",
      "|    Meath|108887|\n",
      "|   Carlow|147576|\n",
      "|Westmeath|266617|\n",
      "|    Sligo|108888|\n",
      "|    Kerry|266616|\n",
      "|    Cavan|133704|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.groupBy('county').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4_pLJ_R47C-"
   },
   "source": [
    "All the values in the county field are valid county names. Let's look at the stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8M3YO6A47C-",
    "outputId": "49de21b9-92bd-417a-d1fa-1d91d8f7ecae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|             station| count|\n",
      "+--------------------+------+\n",
      "|       KNOCK AIRPORT|208938|\n",
      "|         CLAREMORRIS|266580|\n",
      "|            OAK PARK|147576|\n",
      "|        CORK AIRPORT|266617|\n",
      "|              FINNER|193411|\n",
      "|             GURTEEN|108144|\n",
      "|      DUBLIN AIRPORT|266617|\n",
      "|           BELMULLET|266617|\n",
      "|            CASEMENT|266617|\n",
      "|     SHANNON AIRPORT|266617|\n",
      "|             ATHENRY| 78312|\n",
      "|          BALLYHAISE|133704|\n",
      "|VALENTIA OBSERVATORY|266616|\n",
      "|             DUNSANY|108887|\n",
      "|        PHOENIX PARK|120144|\n",
      "|           MACE HEAD|138480|\n",
      "|        ROCHES POINT|247296|\n",
      "|             NEWPORT|135144|\n",
      "|         JOHNSTOWNII|135192|\n",
      "|           MULLINGAR|266617|\n",
      "|           MT DILLON|108192|\n",
      "|       SherkinIsland|141024|\n",
      "|          MALIN HEAD|266617|\n",
      "|             MARKREE|108888|\n",
      "|          MOORE PARK|147576|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.groupBy('station').count().show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF2_NODg47C-"
   },
   "source": [
    "It all looks fine! All the values in the station field are valid station names.\n",
    "\n",
    "When casting values, Spark does not fail or result in silent overflows when encountering an incompatible type. It rather coerces the value into `NULL`. Thus, we can simply find the incorrectly casted values by casting and then counting the `NULL` values. \n",
    "\n",
    "For this, it is important to know how many null values there were before casting and then compare that with after casting. \n",
    "\n",
    "In the code cell below, we first count the number of NULL values before casting to Integer or Float type. We then cast to the specific type and perform a NULL count again on the resulting DataFrame. We finally do a difference between before and after casting NULL count to determine how many values were incorrectly cast.\n",
    "\n",
    "***Note:*** This code cell might take some time to execute based on your computing environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Ws0986uv47C_"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to hold failures. \n",
    "failures = {}\n",
    "# Create a copy of the raw DataFrame.\n",
    "test_df = raw_df\n",
    "\n",
    "# Loop through columns which hold integer values.\n",
    "for col in weather_int:\n",
    "    # Count the number of nulls before type casting. The isnull() function checks if a value is NULL.\n",
    "    # The when() function returns only values that satisfy the specified condition.\n",
    "    # The count() function returns the count for the DataFrame returned by the when() function.\n",
    "    # We then assign it to an alias using the alias() method.\n",
    "    # We finally return the result using the collect() method.\n",
    "    before_count = test_df.select(F.count(F.when(F.isnull(col), col)).alias(col)).collect() \n",
    "    \n",
    "    # Cast to Integer type, creating a new column using the withColumn() method.\n",
    "    # We use the cast() method to cast and use the IntegerType defined in Spark.\n",
    "    test_df = test_df.withColumn(f'test_type_{col}', F.col(col).cast(IntegerType()))\n",
    "    \n",
    "    # Count the number of nulls after type casting, similar to the before count.\n",
    "    after_count = test_df.select(F.count(F.when(F.isnull(f'test_type_{col}'), f'test_type_{col}')).alias(f'test_type_{col}')).collect() \n",
    "    \n",
    "    # Get the difference between before and after. We need to extract the value from a list \n",
    "    # which the collect() method returns, and then get the first element in the row.\n",
    "    failures[col] = after_count[0][0] - before_count[0][0] \n",
    "    \n",
    "# Loop through columns which hold float values.\n",
    "for col in weather_float:\n",
    "    # Count the number of nulls before type casting, similar to the above counts.\n",
    "    before_count = test_df.select(F.count(F.when(F.isnull(col), col)).alias(col)).collect() \n",
    "    \n",
    "    # Cast to Float type, creating a new column using the withColumn() method.\n",
    "    # We use the cast() method to cast and use the FloatType defined in Spark.\n",
    "    test_df = test_df.withColumn(f'test_type_{col}', F.col(col).cast(FloatType())) \n",
    "    \n",
    "    # Count the number of nulls after type casting, similar to the above counts.\n",
    "    after_count = test_df.select(F.count(F.when(F.isnull(f'test_type_{col}'), f'test_type_{col}')).alias(f'test_type_{col}')).collect() \n",
    "    \n",
    "    # Get the difference between before and after.\n",
    "    failures[col] = after_count[0][0] - before_count[0][0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFXae63k47C_"
   },
   "source": [
    "In the above code, we use the collect method. This method gathers all the data from the execution nodes and creates a single DataFrame on the driver node, which will hold all the data locally and on which operations can be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kTnsnhs47C_",
    "outputId": "ff721aff-da06-480d-f92b-592a0ce26a9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rhum': 163615,\n",
       " 'wdsp': 81977,\n",
       " 'wddir': 97336,\n",
       " 'vis': 270249,\n",
       " 'clht': 231378,\n",
       " 'clamt': 231378,\n",
       " 'latitude': 0,\n",
       " 'longitude': 0,\n",
       " 'rain': 111665,\n",
       " 'temp': 32581,\n",
       " 'wetb': 45288,\n",
       " 'dewpt': 44159,\n",
       " 'vappr': 187484,\n",
       " 'msl': 73283,\n",
       " 'sun': 231360}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tv_ihoRQ47C_"
   },
   "source": [
    "Let's have a closer look at the values that are being cast incorrectly. \n",
    "Here we want to group by values that are NULL after the casting, and not NULL before the casting. That is exactly what we are doing below, and aggregating by count to see how many of each instance we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDUPx2Ls47C_",
    "outputId": "786b94e3-d271-4cf7-dbbb-25c3a13c6699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-----+\n",
      "|latitude|test_type_latitude|count|\n",
      "+--------+------------------+-----+\n",
      "+--------+------------------+-----+\n",
      "\n",
      "+---------+-------------------+-----+\n",
      "|longitude|test_type_longitude|count|\n",
      "+---------+-------------------+-----+\n",
      "+---------+-------------------+-----+\n",
      "\n",
      "+----+--------------+------+\n",
      "|rain|test_type_rain| count|\n",
      "+----+--------------+------+\n",
      "|    |          null|111665|\n",
      "+----+--------------+------+\n",
      "\n",
      "+----+--------------+-----+\n",
      "|temp|test_type_temp|count|\n",
      "+----+--------------+-----+\n",
      "|    |          null|32581|\n",
      "+----+--------------+-----+\n",
      "\n",
      "+----+--------------+-----+\n",
      "|wetb|test_type_wetb|count|\n",
      "+----+--------------+-----+\n",
      "|    |          null|45288|\n",
      "+----+--------------+-----+\n",
      "\n",
      "+-----+---------------+-----+\n",
      "|dewpt|test_type_dewpt|count|\n",
      "+-----+---------------+-----+\n",
      "|     |           null|44159|\n",
      "+-----+---------------+-----+\n",
      "\n",
      "+-----+---------------+------+\n",
      "|vappr|test_type_vappr| count|\n",
      "+-----+---------------+------+\n",
      "|     |           null|187484|\n",
      "+-----+---------------+------+\n",
      "\n",
      "+----+--------------+------+\n",
      "|rhum|test_type_rhum| count|\n",
      "+----+--------------+------+\n",
      "|    |          null|163615|\n",
      "+----+--------------+------+\n",
      "\n",
      "+---+-------------+-----+\n",
      "|msl|test_type_msl|count|\n",
      "+---+-------------+-----+\n",
      "|   |         null|73283|\n",
      "+---+-------------+-----+\n",
      "\n",
      "+----+--------------+-----+\n",
      "|wdsp|test_type_wdsp|count|\n",
      "+----+--------------+-----+\n",
      "|    |          null|81977|\n",
      "+----+--------------+-----+\n",
      "\n",
      "+-----+---------------+-----+\n",
      "|wddir|test_type_wddir|count|\n",
      "+-----+---------------+-----+\n",
      "|     |           null|97336|\n",
      "+-----+---------------+-----+\n",
      "\n",
      "+---+-------------+------+\n",
      "|sun|test_type_sun| count|\n",
      "+---+-------------+------+\n",
      "|   |         null|231360|\n",
      "+---+-------------+------+\n",
      "\n",
      "+---+-------------+------+\n",
      "|vis|test_type_vis| count|\n",
      "+---+-------------+------+\n",
      "|   |         null|270249|\n",
      "+---+-------------+------+\n",
      "\n",
      "+----+--------------+------+\n",
      "|clht|test_type_clht| count|\n",
      "+----+--------------+------+\n",
      "|    |          null|231378|\n",
      "+----+--------------+------+\n",
      "\n",
      "+-----+---------------+------+\n",
      "|clamt|test_type_clamt| count|\n",
      "+-----+---------------+------+\n",
      "|     |           null|231378|\n",
      "+-----+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each of the continuous variable columns.\n",
    "for col in weather_cont:\n",
    "    # Again, we are using the isnull() function to find values that are null.\n",
    "    # We use the where() method to only select records that match our conditions.\n",
    "    # Our conditions are where the casted column is null and the raw df is not null (we use the '~' operator which signifies not).\n",
    "    # Similar to how we aggregate using count and return each DataFrame to standard out.\n",
    "    test_df.where(F.isnull(test_df[f'test_type_{col}']) & ~F.isnull(raw_df[col])).groupby(col, f'test_type_{col}').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rzq7U36a47C_"
   },
   "source": [
    "Looks like we found the culprit. It's empty columns that are incorrectly cast to nulls (those are the values in each of the first cells). We can leave these fields as is, as they represent true missing values, and we will deal with them in the next section when we deal with missing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F24C6fmm47C_"
   },
   "source": [
    "Brilliant! It looks like, for the majority of cases, we do not have any issues in the structure of our dataset and its parsed version in Spark. We can move along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJlENGZN47C_"
   },
   "source": [
    "### 4. Missing observations\n",
    "\n",
    "Let's do a quick check for missing data before we dive into the next section.\n",
    "\n",
    "The reason for bringing it up here is that missing data is a feature of the dataset that we have to deal with or justify. When we find missing data, we have three options for dealing with it:\n",
    "\n",
    "- Filter out the missing data.\n",
    "- Impute or supplement the field (discussed in the next section).\n",
    "- Note the caveat and hand over the data to other analytical teams to deal with the missing data or apply more advanced imputation techniques.\n",
    "\n",
    "Let's see how much missing data we have and how we would remove missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0YXVigd47C_"
   },
   "source": [
    "First, we check for NULL values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYbNaeEp47C_",
    "outputId": "08a1dc73-c62e-4ca7-b4fb-08364757aa57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+-----+-----+-----+------+------+-----+------+------+-------+-------+-------+-------+\n",
      "|latitude|longitude|  rain| temp| wetb|dewpt| vappr|  rhum|  msl|  wdsp| wddir|    sun|    vis|   clht|  clamt|\n",
      "+--------+---------+------+-----+-----+-----+------+------+-----+------+------+-------+-------+-------+-------+\n",
      "|       0|        0|111665|32581|45288|44159|187484|163615|73283|311009|326368|2816527|2855416|2816545|2816545|\n",
      "+--------+---------+------+-----+-----+-----+------+------+-----+------+------+-------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df.select([F.count(F.when(F.isnull(x), x)).alias(x) for x in raw_df.columns if x not in weather_cat]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6S4td8W47DA"
   },
   "source": [
    "We also have to check for NaN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8Cfag3-47DA",
    "outputId": "e5ecfcb4-6050-46a5-e2c0-509e70af58ef",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+----+----+-----+-----+----+---+----+-----+---+---+----+-----+\n",
      "|latitude|longitude|rain|temp|wetb|dewpt|vappr|rhum|msl|wdsp|wddir|sun|vis|clht|clamt|\n",
      "+--------+---------+----+----+----+-----+-----+----+---+----+-----+---+---+----+-----+\n",
      "|       0|        0|   0|   0|   0|    0|    0|   0|  0|   0|    0|  0|  0|   0|    0|\n",
      "+--------+---------+----+----+----+-----+-----+----+---+----+-----+---+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df.select([F.count(F.when(F.isnan(x), x)).alias(x) for x in raw_df.columns if x not in weather_cat]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaGTVzsx47DA"
   },
   "source": [
    "Looks like we have quite a number of rows with missing data. But, luckily, it is only Null values and not NaNs too (two different variants of empty values in Spark), which we will cover later in this train. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WG0Co6r47DI"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this train, we had a look at the first half of the process of performing a data engineering task from a processing perspective. We ingested, created a master table to work from, and performed some filters and checks on the data. In the next train, we are going to continue the data engineering process by actually cleaning the data, performing imputation, additional enrichments, and producing the appropriate output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "w0AgjXuh47C8",
    "3b3LA6JI47C-"
   ],
   "name": "050_data_engineering_transformations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
