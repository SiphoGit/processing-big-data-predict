{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering in Spark – II \n",
    "© Explore Data Science Academy\n",
    "\n",
    "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Processing%20Big%20Data/spark.png\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "In this train, you learn how to implement common data engineering transformations. These include: \n",
    "\n",
    "  - enrichment and imputation;\n",
    "  - indexing and ordering; \n",
    "  - anonymisation and encryption;\n",
    "  - modeling, typecasting, formatting, and renaming; and \n",
    "  - pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this train, we are continuing the work on common transformations in Apache Spark. In the previous train, we fully characterised the dataset for any flaws or shortcomings. Here, we are going to adjust the dataset and correct any flaws identified. We will also introduce more advanced techniques to assist with the necessary transformations. Although this will not be the full scope of what you can achieve when using Spark, it will definitely ensure that you understand the fundamentals. Transforming data is a very detail-orientated task, thus, a thorough understanding of the code snippets throughout this train will go a long way into preparing you for your data engineering journey.\n",
    "\n",
    "Let's start by reading in the dataset that we previously saved as parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark and some auxiliary functions, and set up a SparkSession.\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType, TimestampType\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = spark.read.parquet('./data/weather_data/hrly_Irish_weather/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also define categorical and continuous columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_cat = ['county', 'station', 'date']\n",
    "weather_int = ['rhum', 'wdsp', 'wddir', 'vis', 'clht', 'clamt']\n",
    "\n",
    "# Create a list of continuous fields (fields that are not in the categorical list above).\n",
    "weather_cont = [x for x in working_df.columns if x not in weather_cat]\n",
    "\n",
    "# Create a list of float fields (those in the above list, but not in the integer list).\n",
    "weather_float = [x for x in weather_cont if x not in weather_int] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET8vamIG47DA"
   },
   "source": [
    "\n",
    "## Data enrichment and imputation\n",
    "\n",
    "After we have characterised and summarised the incoming dataset, we can try to improve and enrich the dataset using some data engineering techniques. \n",
    "\n",
    "Missing data can be addressed by infilling or imputation. \n",
    "\n",
    "The dataset can also be enriched by creating new fields, which can be precomputed metrics (think percentages, means, or some windowed function), or cleaning up the current features (think splitting text-dense columns). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVxKG3e747DA"
   },
   "source": [
    "### Enriching data\n",
    "\n",
    "We'll start enriching the dataset by adding new inferred fields. \n",
    "\n",
    "To enhance this dataset, let's try to infer the associated season based on the temperature, and then validate the season using the date.\n",
    "\n",
    "There are a couple of things that we have to do to perform this operation:\n",
    "\n",
    "1. Create a day column to window over.\n",
    "2. Get the **mean** daily temperature.\n",
    "3. Get the **mean**, **minimum**, and **maximum** temperatures per month.\n",
    "4. Estimate the month based on temperature.\n",
    "5. Estimate the season based on month.\n",
    "6. Validate against the real month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LLn0RdQt47DA"
   },
   "outputs": [],
   "source": [
    "# Create day and month column.\n",
    "working_df = working_df.withColumn('day', F.dayofyear('date'))\n",
    "working_df = working_df.withColumn('month', F.month('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3JeERFoF47DA"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "# Create daily and monthly windows using Window class and partitionBy() method,\n",
    "# using the 'day' and 'month' columns respectively.\n",
    "day_window = Window.partitionBy(F.col('day'))\n",
    "month_window = Window.partitionBy(F.col('month'))\n",
    "\n",
    "# First, make a copy of the DataFrame.\n",
    "working_df_season = working_df\n",
    "\n",
    "# Get the daily minimum and maximum using the above defined windows.\n",
    "working_df_season = working_df_season.withColumn('temp_daily_max', F.max('temp').over(day_window))\n",
    "working_df_season = working_df_season.withColumn('temp_daily_min', F.min('temp').over(day_window))\n",
    "\n",
    "# Get the mean for the minimum and maximum temperatures \n",
    "# using the mean() function with the monthly window.\n",
    "working_df_season = working_df_season.withColumn('temp_monthly_min', F.mean('temp_daily_min').over(month_window))\n",
    "working_df_season = working_df_season.withColumn('temp_monthly_max', F.mean('temp_daily_max').over(month_window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kdp2-UVe47DA"
   },
   "source": [
    "To estimate the season, we want to compare our calculated minimum and maximum values against historic values. We retrieve these values from [Wikipedia](https://en.wikipedia.org/wiki/Climate_of_Ireland)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GLVMVQ9E47DA"
   },
   "outputs": [],
   "source": [
    "# Climactic data from Wikipedia.\n",
    "rough_min_max_ireland = {\n",
    "    1: {'min': 2, 'max': 8},\n",
    "    2: {'min': 2, 'max': 8},\n",
    "    3: {'min': 3, 'max': 10},\n",
    "    4: {'min': 5, 'max': 12},\n",
    "    5: {'min': 7, 'max': 15},\n",
    "    6: {'min': 10, 'max': 18},\n",
    "    7: {'min': 12, 'max': 20},\n",
    "    8: {'min': 12, 'max': 19},\n",
    "    9: {'min': 10, 'max': 17},\n",
    "    10: {'min': 7, 'max': 14},\n",
    "    11: {'min': 5, 'max': 10},\n",
    "    12: {'min': 3, 'max': 5}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1cFryyG47DA"
   },
   "source": [
    "Let's group by month and get the mean values for each month, to compare against the values retrieved from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NnBXBb7v47DA",
    "outputId": "45f3c3c9-d928-4770-9ec2-d46b01c54506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+---------------------+\n",
      "|month|avg(temp_monthly_min)|avg(temp_monthly_max)|\n",
      "+-----+---------------------+---------------------+\n",
      "|    1|   -6.294776098179843|   13.973935970171693|\n",
      "|    2|    -5.83057417469433|   14.518216912065954|\n",
      "|    3|   -4.845702851176519|   17.019559528027298|\n",
      "|    4|  -2.9175169930454037|    20.41644317842727|\n",
      "|    5|  -0.8002945385715458|   24.354988555758272|\n",
      "|    6|  -0.4465843441516472|    26.21454659678375|\n",
      "|    7|                  0.0|    27.30916489960508|\n",
      "|    8| -0.00323508493013298|   26.269253851997522|\n",
      "|    9|  -0.2034015738605942|   23.701991028867827|\n",
      "|   10|  -1.9310375362803789|   19.284097329660977|\n",
      "|   11|   -4.177552947793544|    15.99675852985561|\n",
      "|   12|   -9.814020929424249|   14.417645506544467|\n",
      "+-----+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df_season.groupBy('month').mean().select('month', 'avg(temp_monthly_min)', 'avg(temp_monthly_max)').orderBy('month').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00zaVBK947DB"
   },
   "source": [
    "Comparing the retrieved values to that of Wikipedia, the values are clearly more extreme, with minimum values lower than expected and maximum values higher. \n",
    "\n",
    "But still, it seems like the highest maximum temperatures are centred around summer and the lowest around winter. Let's see if we can use this to infer the seasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SWbRTUYn47DB"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame that we can manipulate, similar to the above DataFrame, ordered by temperature.\n",
    "\n",
    "seasons_df = working_df_season.groupBy('month').mean().select('month', 'avg(temp_monthly_min)', 'avg(temp_monthly_max)').orderBy('avg(temp_monthly_max)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5JLqnHe47DB",
    "outputId": "bf1ae5ef-8bec-478c-a7e3-4ab1c06a7df3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+---------------------+\n",
      "|month|avg(temp_monthly_min)|avg(temp_monthly_max)|\n",
      "+-----+---------------------+---------------------+\n",
      "|    1|   -6.294776098179843|   13.973935970171693|\n",
      "|   12|   -9.814020929424249|   14.417645506544467|\n",
      "|    2|    -5.83057417469433|   14.518216912065954|\n",
      "|   11|   -4.177552947793544|    15.99675852985561|\n",
      "|    3|   -4.845702851176519|   17.019559528027298|\n",
      "|   10|  -1.9310375362803789|   19.284097329660977|\n",
      "|    4|  -2.9175169930454037|    20.41644317842727|\n",
      "|    9|  -0.2034015738605942|   23.701991028867827|\n",
      "|    5|  -0.8002945385715458|   24.354988555758272|\n",
      "|    6|  -0.4465843441516472|    26.21454659678375|\n",
      "|    8| -0.00323508493013298|   26.269253851997522|\n",
      "|    7|                  0.0|    27.30916489960508|\n",
      "+-----+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seasons_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvMQNWLD47DB",
    "outputId": "f41b7232-d280-4ffe-f9fa-212300d2876b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+---------------------+---------+\n",
      "|month|avg(temp_monthly_min)|avg(temp_monthly_max)|temp_rank|\n",
      "+-----+---------------------+---------------------+---------+\n",
      "|    1|   -6.294776098179843|   13.973935970171693|        1|\n",
      "|   12|   -9.814020929424249|   14.417645506544467|        2|\n",
      "|    2|    -5.83057417469433|   14.518216912065954|        3|\n",
      "|   11|   -4.177552947793544|    15.99675852985561|        4|\n",
      "|    3|   -4.845702851176519|   17.019559528027298|        5|\n",
      "|   10|  -1.9310375362803789|   19.284097329660977|        6|\n",
      "|    4|  -2.9175169930454037|    20.41644317842727|        7|\n",
      "|    9|  -0.2034015738605942|   23.701991028867827|        8|\n",
      "|    5|  -0.8002945385715458|   24.354988555758272|        9|\n",
      "|    6|  -0.4465843441516472|    26.21454659678375|       10|\n",
      "|    8| -0.00323508493013298|   26.269253851997522|       11|\n",
      "|    7|                  0.0|    27.30916489960508|       12|\n",
      "+-----+---------------------+---------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a window that is ordered by the maximum temperature.\n",
    "wndw = Window.orderBy('avg(temp_monthly_max)')\n",
    "\n",
    "# Create a rank over the window defined above.\n",
    "seasons_df = seasons_df.withColumn('temp_rank', F.dense_rank().over(wndw))\n",
    "\n",
    "seasons_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyeofM5J47DB"
   },
   "source": [
    "Now we have a DataFrame that is ranked by maximum temperature. Using this, we can assign the hottest and coldest months to summer and winter. We write a simple Python function to extract the first and last three months based on temperature rank and assign them as either winter or summer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "o4WZTANW47DB"
   },
   "outputs": [],
   "source": [
    "def summer_or_winter(temp_rank):\n",
    "    if temp_rank in [1, 2, 3]:\n",
    "        return 'winter'\n",
    "    if temp_rank in [10, 11, 12]:\n",
    "        return 'summer'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XtWvl27z47DB"
   },
   "outputs": [],
   "source": [
    "# Convert the Python function to a Spark UDF.\n",
    "\n",
    "udf_season = F.udf(lambda x:summer_or_winter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwD2z8ZG47DB",
    "outputId": "77ea02bc-2465-454a-8ae3-e262cc73ed93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+---------------------+---------+------+\n",
      "|month|avg(temp_monthly_min)|avg(temp_monthly_max)|temp_rank|season|\n",
      "+-----+---------------------+---------------------+---------+------+\n",
      "|    1|   -6.294776098179843|   13.973935970171693|        1|winter|\n",
      "|    2|    -5.83057417469433|   14.518216912065954|        3|winter|\n",
      "|    3|   -4.845702851176519|   17.019559528027298|        5|  null|\n",
      "|    4|  -2.9175169930454037|    20.41644317842727|        7|  null|\n",
      "|    5|  -0.8002945385715458|   24.354988555758272|        9|  null|\n",
      "|    6|  -0.4465843441516472|    26.21454659678375|       10|summer|\n",
      "|    7|                  0.0|    27.30916489960508|       12|summer|\n",
      "|    8| -0.00323508493013298|   26.269253851997522|       11|summer|\n",
      "|    9|  -0.2034015738605942|   23.701991028867827|        8|  null|\n",
      "|   10|  -1.9310375362803789|   19.284097329660977|        6|  null|\n",
      "|   11|   -4.177552947793544|    15.99675852985561|        4|  null|\n",
      "|   12|   -9.814020929424249|   14.417645506544467|        2|winter|\n",
      "+-----+---------------------+---------------------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seasons_df = seasons_df.withColumn('season', udf_season(F.col('temp_rank')))\n",
    "\n",
    "seasons_df.orderBy('month').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdLG-ct447DC"
   },
   "source": [
    "Great! Now we have assignments for summer and winter. Autumn and spring are a little bit more difficult since they will inherently have overlapping temperatures. Let's use the order of the months this time to assign the months to the seasons. \n",
    "\n",
    "For this, we will use a simple function that compares the current month to that of summer and assigns a season based on that. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LjO7ufXx47DC"
   },
   "outputs": [],
   "source": [
    "def autumn_or_spring(month, season):\n",
    "    if not season:\n",
    "        if month < 6:\n",
    "            return 'spring'\n",
    "        elif month > 6:\n",
    "            return 'autumn'\n",
    "    else:\n",
    "        return season\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "EGRzn36b47DC"
   },
   "outputs": [],
   "source": [
    "# Convert the Python function to a Spark UDF.\n",
    "\n",
    "udf_season = F.udf(lambda x,y:autumn_or_spring(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBxhnNq047DC",
    "outputId": "f9147893-1b88-4a36-af51-822e86e0ef95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+---------------------+---------+------+\n",
      "|month|avg(temp_monthly_min)|avg(temp_monthly_max)|temp_rank|season|\n",
      "+-----+---------------------+---------------------+---------+------+\n",
      "|    1|   -6.294776098179843|   13.973935970171693|        1|winter|\n",
      "|    2|    -5.83057417469433|   14.518216912065954|        3|winter|\n",
      "|    3|   -4.845702851176519|   17.019559528027298|        5|spring|\n",
      "|    4|  -2.9175169930454037|    20.41644317842727|        7|spring|\n",
      "|    5|  -0.8002945385715458|   24.354988555758272|        9|spring|\n",
      "|    6|  -0.4465843441516472|    26.21454659678375|       10|summer|\n",
      "|    7|                  0.0|    27.30916489960508|       12|summer|\n",
      "|    8| -0.00323508493013298|   26.269253851997522|       11|summer|\n",
      "|    9|  -0.2034015738605942|   23.701991028867827|        8|autumn|\n",
      "|   10|  -1.9310375362803789|   19.284097329660977|        6|autumn|\n",
      "|   11|   -4.177552947793544|    15.99675852985561|        4|autumn|\n",
      "|   12|   -9.814020929424249|   14.417645506544467|        2|winter|\n",
      "+-----+---------------------+---------------------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seasons_df = seasons_df.withColumn('season', udf_season(F.col('month'), F.col('season')))\n",
    "\n",
    "seasons_df.orderBy('month').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhcGqbBy47DC"
   },
   "source": [
    "There we go! All the seasons are assigned.\n",
    "\n",
    "Let's compare that with seasons purely derived from what we know about the months of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2GDJhRII47DC"
   },
   "outputs": [],
   "source": [
    "season_dict = {\n",
    "    1: 'winter',\n",
    "    2: 'winter',\n",
    "    3: 'spring',\n",
    "    4: 'spring',\n",
    "    5: 'spring',\n",
    "    6: 'summer',\n",
    "    7: 'summer',\n",
    "    8: 'summer',\n",
    "    9: 'autumn',\n",
    "    10: 'autumn',\n",
    "    11: 'autumn',\n",
    "    12: 'winter'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Zg2-Wipv47DC"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# Create a map type to map the above dictionary to the DataFrame. \n",
    "# The create_map() function requires a list of columns to convert into a map type.\n",
    "# We convert each item in the above dictionary into a column using the lit() function.\n",
    "# Using list comprehension and the chain() function, we create a list of all items.\n",
    "mapping_expr = F.create_map([F.lit(x) for x in chain(*season_dict.items())])\n",
    "\n",
    "# We apply the map type to the month column.\n",
    "seasons_df = seasons_df.withColumn(\"season_truth\", mapping_expr.getItem(seasons_df[\"month\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-Uv60Jq47DC"
   },
   "source": [
    "Let's run through what just happened. Within the list comprehension, we first created a list of key-value pairs using the `chain()` function from `itertools`. This creates a list from the dictionary: `[1, 'winter', 2, 'winter'...]`, and then the `lit()` function converts each item in the list into a column object in Spark. \n",
    "Finally, the [`create_map()`](https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.functions.create_map) function converts it to a map type in Spark, which can be used to transform columns based on column values. \n",
    "\n",
    "We then use the map to transform the month column into seasons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DWr2ukb47DC",
    "outputId": "ddde74be-1ae8-4434-fd65-cf3bd3d893c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+---------------------+---------+------+------------+\n",
      "|month|avg(temp_monthly_min)|avg(temp_monthly_max)|temp_rank|season|season_truth|\n",
      "+-----+---------------------+---------------------+---------+------+------------+\n",
      "|    1|   -6.294776098179843|   13.973935970171693|        1|winter|      winter|\n",
      "|   12|   -9.814020929424249|   14.417645506544467|        2|winter|      winter|\n",
      "|    2|    -5.83057417469433|   14.518216912065954|        3|winter|      winter|\n",
      "|   11|   -4.177552947793544|    15.99675852985561|        4|autumn|      autumn|\n",
      "|    3|   -4.845702851176519|   17.019559528027298|        5|spring|      spring|\n",
      "|   10|  -1.9310375362803789|   19.284097329660977|        6|autumn|      autumn|\n",
      "|    4|  -2.9175169930454037|    20.41644317842727|        7|spring|      spring|\n",
      "|    9|  -0.2034015738605942|   23.701991028867827|        8|autumn|      autumn|\n",
      "|    5|  -0.8002945385715458|   24.354988555758272|        9|spring|      spring|\n",
      "|    6|  -0.4465843441516472|    26.21454659678375|       10|summer|      summer|\n",
      "|    8| -0.00323508493013298|   26.269253851997522|       11|summer|      summer|\n",
      "|    7|                  0.0|    27.30916489960508|       12|summer|      summer|\n",
      "+-----+---------------------+---------------------+---------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seasons_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkTvnRAw47DC"
   },
   "source": [
    "The above two methods result in the exact same output.\n",
    "\n",
    "Looking at the above two methods for deriving seasons, it is clear that the second required a lot less code to write and is much more efficient. The first would be more appropriate if we wanted to see if climactic shifts are causing a change in seasonal temperatures. \n",
    "\n",
    "It does, however, make use of all the features of Spark, and thus we include it for illustrative purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now apply the second method to the working DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply the map type to the month column.\n",
    "working_df = working_df.withColumn(\"season\", mapping_expr.getItem(working_df[\"month\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd9WGeYQ47DD"
   },
   "source": [
    "### Types of missing data \n",
    "Missing data causes three main problems:\n",
    "\n",
    "- It can introduce bias in the dataset.\n",
    "- It complicates the analysis of the dataset since missing values have to be treated differently.\n",
    "- It reduces computational efficiency.\n",
    "\n",
    "Not only is missing data the bane of existence for any data engineer or data scientist but Spark also makes it just a little bit more complicated, as it has built-in types as well as having to deal with SQL types and types of the various programming languages it is compatible with. \n",
    "\n",
    "\n",
    "**Null values in Spark**\n",
    "\n",
    "When the value of a specific column in a specific row is not known, it is represented as a `NULL` in SQL. Because Spark SQL is the base engine on which the rest is built, this is the base representation of missing values in Spark. These values can be in any column of any type, depending on if the argument `nullable` was set to `True` or `False`. \n",
    "\n",
    "Let's look at the behaviour of `NULL` values in Spark:\n",
    "- Any comparison with a `NULL` value will return a `NULL` value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQCAgmL547DD"
   },
   "source": [
    "Here's an instance where the 'dewpt' is null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bTkCKNP147DD",
    "outputId": "6367741b-9c2b-4aaa-c52b-3d45e5f31551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+----+\n",
      "|county|station|dewpt|rhum|\n",
      "+------+-------+-----+----+\n",
      "|Galway|ATHENRY| null|  85|\n",
      "|Galway|ATHENRY| null|  88|\n",
      "+------+-------+-----+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df.where(F.isnull('dewpt')).select('county', 'station', 'dewpt', 'rhum').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgCDhdMN47DD"
   },
   "source": [
    "We do a comparison with 'rhum', writing the output to a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F0bHfo9x47DD",
    "outputId": "3ff254b4-fd21-49f1-8e94-1ac150db5a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+----+----------+\n",
      "|county|station|dewpt|rhum|dewpt_rhum|\n",
      "+------+-------+-----+----+----------+\n",
      "|Galway|ATHENRY| null|  85|      null|\n",
      "|Galway|ATHENRY| null|  88|      null|\n",
      "+------+-------+-----+----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df.withColumn('dewpt_rhum', F.col('dewpt') == F.col('rhum')).where(F.isnull('dewpt')).select('county', 'station', 'dewpt', 'rhum', 'dewpt_rhum').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BvzKlmd47DD"
   },
   "source": [
    "As you can see, the output from the comparison is 'null'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4Sf2vvE47DD"
   },
   "source": [
    "*Spark provides a null-safe operator, which returns `False` if one of the operators is `NULL`*.\n",
    "\n",
    "- In logical operations the following table describes the behaviour of Spark:\n",
    "\n",
    "    | Left Operand | Right Operand | OR   | AND   |\n",
    "    |--------------|---------------|------|-------|\n",
    "    | True         | NULL          | True | NULL  |\n",
    "    | False        | NULL          | NULL | False |\n",
    "    | NULL         | True          | True | NULL  |\n",
    "    | NULL         | False         | NULL | NULL  |\n",
    "    | NULL         | NULL          | NULL | NULL  |\n",
    "\n",
    "\n",
    "- Spark supports a set of expressions that are tolerant to null values:\n",
    "  - COALESCE\n",
    "  - NULLIF\n",
    "  - IFNULL\n",
    "  - NVL\n",
    "  - NVL2\n",
    "  - ISNAN\n",
    "  - NANVL\n",
    "  - ISNULL\n",
    "  - ISNOTNULL\n",
    "  - ATLEASTNNONNULLS\n",
    "  - IN\n",
    "  \n",
    "The behaviour of each will depend on the specific function. For more information, refer to the Spark functions documentation.\n",
    "\n",
    "- Some are intolerant to Null values:\n",
    "  - CONCAT\n",
    "  - POSITIVE\n",
    "  - TO_DATE\n",
    "  - TO_TIMESTAMP\n",
    "\n",
    "- Aggregate functions in Spark will ignore values, except for the COUNT function.\n",
    "\n",
    "Only if all values are NULL will the above functions return NULL.\n",
    "\n",
    "- WHERE, HAVING and JOIN clauses follow the same rules where NULL values are excluded, except if specified, for example, SELECT * FROM table WHERE field IS NULL.\n",
    "- When grouping values, NULL values are grouped into a separate group.\n",
    "- When ordering data, NULL values are placed first by default.\n",
    "\n",
    "This is quite a mouthful to get you started on NULLs. The best way to understand is with practice, which we will get to after working with NaNs.\n",
    "\n",
    "\n",
    "**NaN values in Spark**\n",
    "\n",
    "NaN values are representations of a value that cannot be computed referring to `float` or `double` types. It is 'Not a Number'. Some of the specific characteristics when working with NaNs are:\n",
    "\n",
    "- NaN = NaN returns True.\n",
    "- When aggregating data, NaN values are grouped.\n",
    "- NaNs are treated as normal values in joins.\n",
    "- NaNs are placed last when ordering data, being larger than any other value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMVrkw-o47DD"
   },
   "source": [
    "We have no NaNs here (which should hopefully be the case most of the time for data engineers), but there are many `Null` values. This should not come as a surprise to you, but in the preceding code, we have already performed many operations with the `Null` values included in the calculations. In the next section, we will try to impute the values to remove all `Null` values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_T44pxz47DD"
   },
   "source": [
    "### Imputing data\n",
    "\n",
    "As mentioned above, sometimes our datasets have a lot of missing data. While it may seem intuitive and simple to just drop all rows which contain one or more missing values (and, indeed, many statistical packages either allow you to or do this by default), there must be more efficient ways of dealing with missing data.\n",
    "\n",
    "This is where imputation or infilling comes in. Imputation is defined as the process of replacing missing values in a table or dataset with substituted values. These substituted values are normally based on additional data contained within the dataset, such as contextual data. \n",
    "\n",
    "Imputation takes numerous forms:\n",
    "\n",
    "- Hot-deck\n",
    "- Cold-deck\n",
    "- Mean substitution\n",
    "- Non-negative matrix factorization (NNMF)\n",
    "- Regression\n",
    "- Last observation\n",
    "- Stochastic sampling\n",
    "- K-nearest neighbours imputation\n",
    "- Multiple imputations\n",
    "- Deep learning\n",
    "\n",
    "The specific imputation technique applied will depend on the use case. For example, if you are dealing with time series sensor data you may go with the last observation. If dealing with house prices, a mean substitution may be most appropriate, or a regression. \n",
    "\n",
    "Imputation is a whole discipline on its own, and a full treatment of its various techniques are well beyond the scope of this train. To find out more, you can read an overview of the topic [here](https://en.wikipedia.org/wiki/Imputation_(statistics)).\n",
    "\n",
    "For this dataset, it makes a lot of sense to impute the temperature with the mean temperature for that day, since temperature may not be logged continuously. \n",
    "\n",
    "Let's implement this using the Window function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k788mWzk47DE"
   },
   "source": [
    "It is clear that data from a source system is not clean at all, containing many flaws and challenges that first have to be addressed. This step in the data engineering toolkit allows us more opportunities to do just that. Here we move past just removing bad data, but rather to creating and infilling new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "1NFH2Bst47DE"
   },
   "outputs": [],
   "source": [
    "day_window = Window.partitionBy(F.col('day'), F.col('station'))\n",
    "\n",
    "working_df_imputed = working_df.withColumn('temp', F.when(F.isnull(F.col('temp')), F.mean('temp').over(day_window)).otherwise(F.col('temp')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8V3Hmdf47DE"
   },
   "source": [
    "Again, there's quite a bit to unpack here. But by now you should be used to the fact that you can make Spark do pretty much anything you want in a single line. \n",
    "\n",
    "In this example, we first create a window that is per day, per station. This will allow us to create values that are specific to the station for a given day. \n",
    "Next, we create a new column for the temperature (it just so happens that we actually want all the temperature data in one column, so we just recreate `temp`). Within the column, we create a `when()` condition to check if the column is empty. If it is empty, we fill it with the mean, over the window previously defined. If it's not empty, we just fill it with the value already in the temperature column.\n",
    "\n",
    "To check that we are not lying to you, you can run a few tests to validate that this is the case. (Pro-tip: do not put the values into `temp` but rather into an intermediate column to validate.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9D0IdgyE47DE",
    "outputId": "e0dc19b7-c096-43f8-bdd3-6384e98f3880"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(temp=0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check if temp is empty.\n",
    "\n",
    "working_df_imputed.select(F.count(F.when(F.isnull('temp'),'temp')).alias('temp')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfqGyDSg47DE"
   },
   "source": [
    "Great!\n",
    "\n",
    "This, however, is a gross simplification of the actual situation, since temperatures will change during the day. A more intuitive and logical imputation may be to impute the temperature using the mean value for temperature for the specific hour over the month within which the reading was taken.\n",
    "\n",
    "Try and implement that on your own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSsoaMbV47DE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qbni0CR347DE"
   },
   "source": [
    "For the remainder of the dataset, we will stick with a mean imputation over the season per station for each of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 726
    },
    "id": "MOLuDW2e47DE",
    "outputId": "6553ffb0-4dbe-44bd-9bb0-4ccf919fc92c"
   },
   "outputs": [],
   "source": [
    "day_window = Window.partitionBy(F.col('season'), F.col('station'))\n",
    "\n",
    "cols_to_impute = [\n",
    "    'rhum',\n",
    "    'wdsp',\n",
    "    'wddir',\n",
    "    'vis',\n",
    "    'clht',\n",
    "    'clamt',\n",
    "    'rain',\n",
    "    'wetb',\n",
    "    'dewpt',\n",
    "    'vappr',\n",
    "    'msl',\n",
    "    'sun'\n",
    "]\n",
    "\n",
    "for field in cols_to_impute:\n",
    "    working_df_imputed = working_df.withColumn(field, F.when(F.isnull(F.col(field)), \n",
    "                                                F.mean(field).over(day_window)).otherwise(F.col(field)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "2hWXlbuc47DE"
   },
   "outputs": [],
   "source": [
    "# Let's check the failures again.\n",
    "\n",
    "failures = {}\n",
    "test_df = working_df_imputed\n",
    "\n",
    "for col in weather_int:\n",
    "    before_count = test_df.select(F.count(F.when(F.isnull(col), col)).alias(col)).collect()\n",
    "    test_df = test_df.withColumn(f'test_type_{col}', F.col(col).cast(IntegerType()))\n",
    "    after_count = test_df.select(F.count(F.when(F.isnull(f'test_type_{col}'), f'test_type_{col}')).alias(f'test_type_{col}')).collect()\n",
    "    failures[col] = after_count[0][0] - before_count[0][0]\n",
    "    \n",
    "for col in weather_float:\n",
    "    before_count = test_df.select(F.count(F.when(F.isnull(col), col)).alias(col)).collect()\n",
    "    test_df = test_df.withColumn(f'test_type_{col}', F.col(col).cast(FloatType()))\n",
    "    after_count = test_df.select(F.count(F.when(F.isnull(f'test_type_{col}'), f'test_type_{col}')).alias(f'test_type_{col}')).collect()\n",
    "    failures[col] = after_count[0][0] - before_count[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "msTA-gIy47DE",
    "outputId": "ac09ec53-3fc7-472c-8bbe-3cd64c4c123f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rhum': 0,\n",
       " 'wdsp': 0,\n",
       " 'wddir': 0,\n",
       " 'vis': 0,\n",
       " 'clht': 0,\n",
       " 'clamt': 0,\n",
       " 'latitude': 0,\n",
       " 'longitude': 0,\n",
       " 'rain': 0,\n",
       " 'temp': 0,\n",
       " 'wetb': 0,\n",
       " 'dewpt': 0,\n",
       " 'vappr': 0,\n",
       " 'msl': 0,\n",
       " 'sun': 0,\n",
       " 'day': 0,\n",
       " 'month': 0,\n",
       " 'season': 4660423}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urz5ltwa47DF"
   },
   "source": [
    "There are no more missing values. Yay!\n",
    "\n",
    "This allows us to create a dataset that is at least complete. However, for data lineage and documentation it is extremely important to note all ingestion assumptions and imputations that have been performed to ensure that all other teams consuming the data are aware of all changes that were made. \n",
    "\n",
    "It is also important to expose a bronze version of the dataset to other teams who may want to perform a different set of imputations or transformations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuSsvmIv47DF"
   },
   "source": [
    "## Indexing and ordering\n",
    "\n",
    "Sometimes it may be beneficial to order data in such a way that it will logically be structured on a data storage system. This may be done through indexing and partitioning for fast access in applications like Spark. (Remember, Spark always tries to process data that are logically closest to the executors.) Also, when filtering data, Spark is most efficient if it already knows where to look.\n",
    "\n",
    "The same principle applies when working with relational databases, where indexes allow for data to be searched very efficiently and for relationships to be maintained between separate tables.\n",
    "\n",
    "While Spark does not inherently allow for indexes on tables (data are rather physically separated into partitions), it is still possible for us to create indexes for tables that will later be written to SQL or another relational database.\n",
    "\n",
    "Let's index the original DataFrame that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "7QgNBcdq47DF"
   },
   "outputs": [],
   "source": [
    "working_df = working_df.orderBy('date', 'station')\n",
    "\n",
    "working_df = working_df.withColumn('idx', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2wM2zht47DF"
   },
   "source": [
    "Doing the above two operations serves two purposes. First, we order the data by date and station, meaning that all the data points for the first date will be together, then grouped by the station. \n",
    "\n",
    "Next, we added an index. If we are to write this data to SQL, we will be able to use this index. \n",
    "This allows us to then partition the data correctly in Spark (the exact details which are beyond the scope of this train), and also allow lightning-quick access in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1fNHRjT47DF",
    "outputId": "b7de57a4-ac98-40a1-a2b9-612496e531b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------+---------+-------------------+----+----+----+-----+-----+----+------+----+-----+----+-----+----+-----+---+-----+------+---+\n",
      "|   county|             station|latitude|longitude|               date|rain|temp|wetb|dewpt|vappr|rhum|   msl|wdsp|wddir| sun|  vis|clht|clamt|day|month|season|idx|\n",
      "+---------+--------------------+--------+---------+-------------------+----+----+----+-----+-----+----+------+----+-----+----+-----+----+-----+---+-----+------+---+\n",
      "|     Mayo|           BELMULLET|  54.228|  -10.007|1990-01-01 00:00:00| 0.0| 7.6| 7.1|  6.5|  9.7|  93|1003.0|  10|  200| 0.0|26000|  16|    8|  1|    1|winter|  0|\n",
      "|   Dublin|            CASEMENT|  53.306|   -6.439|1990-01-01 00:00:00| 0.0| 9.2| 8.5|  7.8| 10.5|  91|1007.9|  13|  160| 0.0|15000|  14|    7|  1|    1|winter|  1|\n",
      "|     Mayo|         CLAREMORRIS|  53.711|   -8.993|1990-01-01 00:00:00| 0.1| 7.1| 7.0|  6.9|  9.9|  99|1004.5|   7|  160|null| null|null| null|  1|    1|winter|  2|\n",
      "|     Cork|        CORK AIRPORT|  51.847|   -8.486|1990-01-01 00:00:00| 1.4| 7.8| 7.7|  7.6| 10.4|  99|1006.7|   9|  200| 0.0|30000|  40|    7|  1|    1|winter|  3|\n",
      "|   Dublin|      DUBLIN AIRPORT|  53.428|   -6.241|1990-01-01 00:00:00| 0.1| 9.0| 8.1|  7.1| 10.1|  88|1008.3|  10|  140| 0.0| 7000|   7|    8|  1|    1|winter|  4|\n",
      "|  Donegal|          MALIN HEAD|  55.372|   -7.339|1990-01-01 00:00:00| 0.0| 8.6| 8.0|  7.4| 10.2|  92|1005.4|  24|  140| 0.0|10000|  15|    7|  1|    1|winter|  5|\n",
      "|Westmeath|           MULLINGAR|  53.537|   -7.362|1990-01-01 00:00:00| 0.0| 9.5| 8.8|  8.1| 10.8|  91|1005.6|  16|  160|null| null|null| null|  1|    1|winter|  6|\n",
      "|     Cork|        ROCHES POINT|  51.793|   -8.244|1990-01-01 00:00:00| 0.3| 9.1| 9.0|  8.9| 11.4|  99|1006.7|   7|  190|null| null|null| null|  1|    1|winter|  7|\n",
      "|    Clare|     SHANNON AIRPORT|   52.69|   -8.918|1990-01-01 00:00:00| 0.0| 7.8| 7.4|  6.9| 10.0|  94|1005.7|   5|  150| 0.0|25000|  45|    7|  1|    1|winter|  8|\n",
      "|    Kerry|VALENTIA OBSERVATORY|  51.938|  -10.241|1990-01-01 00:00:00| 0.0| 7.6| 6.8|  5.8|  9.2|  89|1005.3|   8|  200| 0.0|20000|  35|    5|  1|    1|winter|  9|\n",
      "+---------+--------------------+--------+---------+-------------------+----+----+----+-----+-----+----+------+----+-----+----+-----+----+-----+---+-----+------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnbsbB9647DF"
   },
   "source": [
    "## Anonymisation and encryption\n",
    "\n",
    "Data is a commodity in the information age. As such, it is our responsibility as data engineers to ensure that personally identifiable information (PII) that we have from individuals do not reach the hands of malicious agents. \n",
    "\n",
    "To ensure this, any PII data should be anonymised before being transferred outside of our systems. Fortunately, encryption is mostly taken care of by modern cloud providers, and the details thereof are beyond the scope of this train. More information on [why it's important](https://aws.amazon.com/blogs/security/importance-of-encryption-and-how-aws-can-help/) and [how cloud providers can help](https://docs.aws.amazon.com/whitepapers/latest/introduction-aws-security/data-encryption.html) can be found [here](https://docs.microsoft.com/en-us/azure/security/fundamentals/encryption-overview). Anonymisation can happen at various levels: cell level, field level, or row level. Luckily for us, no fields in the above dataset contain PII data.\n",
    "\n",
    "\n",
    "If any of the fields did contain PII data, we might have to treat the whole table as sensitive, meaning we would have to restrict access and make sure that when presented or given to analytics teams they have the correct data protection assessment in place. One possible way of dealing with sensitive data includes having a separate data lake or blob store that only deals with sensitive data. Access to this lake will, thus, be restricted and will have directory-based access limitations. Another method for dealing with sensitive data would be to mask or remove the sensitive field. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7zzkyFe47DG"
   },
   "source": [
    "## Modeling, typecasting, formatting, and renaming\n",
    "\n",
    "The last step. Finally. \n",
    "\n",
    "Our users will want to have the data prepared in a specific format, type, and structure. This last step in the data engineering toolkit is all about forming and manipulating the data into a structure and format that is acceptable for our end-users. Let's prepare our data for output onto a PowerBI dashboard. For this, we require the data in a wide format, with fields labelled with the full descriptions, and all field types as either string, float, or integer. \n",
    "\n",
    "> **Definitions** 📝\n",
    "> \n",
    ">More on wide and narrow datasets and their uses can be found [here](https://en.wikipedia.org/wiki/Wide_and_narrow_data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qlMG16x47DG"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8ZiFFYI47DG",
    "outputId": "6265973f-1bd1-48c7-a58a-05292ad24f83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- county: string (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- rain: float (nullable = true)\n",
      " |-- temp: float (nullable = true)\n",
      " |-- wetb: float (nullable = true)\n",
      " |-- dewpt: float (nullable = true)\n",
      " |-- vappr: float (nullable = true)\n",
      " |-- rhum: integer (nullable = true)\n",
      " |-- msl: float (nullable = true)\n",
      " |-- wdsp: integer (nullable = true)\n",
      " |-- wddir: integer (nullable = true)\n",
      " |-- sun: float (nullable = true)\n",
      " |-- vis: integer (nullable = true)\n",
      " |-- clht: integer (nullable = true)\n",
      " |-- clamt: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- idx: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFhqzuYQ47DG"
   },
   "source": [
    "Let's convert the date to a string. It will be parsed back to the timestamp on the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "AclbpilV47DG"
   },
   "outputs": [],
   "source": [
    "working_df = working_df.withColumn('date', F.col('date').cast('STRING')) # We can also use SQL notation in casting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUY4EwcN47DG"
   },
   "source": [
    "Let's convert the names to full names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ZM5a4c_k47DG"
   },
   "outputs": [],
   "source": [
    "convert_dict = {\n",
    "    'county': 'County',\n",
    "    'station': 'Weather Station', \n",
    "    'latitude': 'Latitude',\n",
    "    'longitude': 'Longitude',\n",
    "    'date': 'Date',\n",
    "    'rain': 'Rain (mm)',\n",
    "    'temp': 'Temperature (°C)',\n",
    "    'wetb': 'Wet Bulb Air Temperature (°C)',\n",
    "    'dewpt': 'Dew Point',\n",
    "    'vappr': 'Vapour Pressure (hPa)',\n",
    "    'rhum': 'Relative Humidity (%)',\n",
    "    'msl': 'Mean Sea Level Pressure (hPa)',\n",
    "    'wdsp': 'Mean Hourly Wind Speed (kt)',\n",
    "    'wddir': 'Predominant Hourly Wind Direction (degrees)',\n",
    "    'sun': 'Sun (hours)',\n",
    "    'vis': 'Visibility (m)',\n",
    "    'clht': 'Cloud Ceiling Height',\n",
    "    'clamt': 'Cloud Amount (Oktas)',\n",
    "    'idx': 'Index',\n",
    "    'day': 'Day', \n",
    "    'month': 'Month',\n",
    "    'season': 'Season'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "vWuYDGRY47DG"
   },
   "outputs": [],
   "source": [
    "output_df = working_df\n",
    "for col in working_df.columns:\n",
    "    output_df = output_df.withColumnRenamed(col, convert_dict[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lla8yPrg47DH",
    "outputId": "e3ea8246-ab63-44af-e5fb-4fe738f5cc30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- County: string (nullable = true)\n",
      " |-- Weather Station: string (nullable = true)\n",
      " |-- Latitude: float (nullable = true)\n",
      " |-- Longitude: float (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Rain (mm): float (nullable = true)\n",
      " |-- Temperature (°C): float (nullable = true)\n",
      " |-- Wet Bulb Air Temperature (°C): float (nullable = true)\n",
      " |-- Dew Point: float (nullable = true)\n",
      " |-- Vapour Pressure (hPa): float (nullable = true)\n",
      " |-- Relative Humidity (%): integer (nullable = true)\n",
      " |-- Mean Sea Level Pressure (hPa): float (nullable = true)\n",
      " |-- Mean Hourly Wind Speed (kt): integer (nullable = true)\n",
      " |-- Predominant Hourly Wind Direction (degrees): integer (nullable = true)\n",
      " |-- Sun (hours): float (nullable = true)\n",
      " |-- Visability (m): integer (nullable = true)\n",
      " |-- Cloud Ceiling Height: integer (nullable = true)\n",
      " |-- Cloud Ammount (Oktas): integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- Index: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kp4J0Hig47DH"
   },
   "source": [
    "That looks great! \n",
    "While we do not need this dataset to be in a long format, let's just have a quick look into pivoting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8I8IQZSD47DH"
   },
   "source": [
    "### Pivoting\n",
    "\n",
    "Pivoting is changing the data from a row-orientated to a column-orientated dataset (wide vs. narrow). The inverse (going from column to row-orientated) is known as unpivoting. In other words, it is changing from the narrow format into the wide format. \n",
    "\n",
    "For optimal visualisation in a bespoke application, it may be preferable to have the data in a narrow format, whereas, for data scientists, it may be most appropriate to have the data in a wide format. \n",
    "This will form part of the final formatting section of the data engineering process.\n",
    "\n",
    "Pivoting data may also involve aggregations being performed on the data to get it into a more condensed format, such as performing means, summation, or getting the minimum/maximum. This is specifically true when creating a data warehouse for data analytics.\n",
    "\n",
    "\n",
    "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Processing%20Big%20Data/reshaping_pivot.png\"\n",
    "     alt=\"Pivoting\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     />\n",
    "    <em>Figure 1. Pivoting.</em>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibU0IlVK47DH"
   },
   "source": [
    "Let's look at an example from our dataset and try to get the mean temperature for each county in a pivoted table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SgXkeCCB47DH",
    "outputId": "65248575-dad0-414d-8d75-1c72bd4fbfac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------------------+------+------------------+-------+\n",
      "|               Date|Carlow|              Cork|Galway|              Mayo|Wexford|\n",
      "+-------------------+------+------------------+------+------------------+-------+\n",
      "|1990-02-05 13:00:00|  null|11.300000190734863|  null|10.600000381469727|   null|\n",
      "|1990-03-11 12:00:00|  null|              11.5|  null|10.600000381469727|   null|\n",
      "|1990-05-10 10:00:00|  null|11.599999904632568|  null|12.050000190734863|   null|\n",
      "|1990-06-28 19:00:00|  null|12.950000286102295|  null|13.150000095367432|   null|\n",
      "|1990-07-04 00:00:00|  null|13.050000190734863|  null|11.449999809265137|   null|\n",
      "|1990-12-05 03:00:00|  null| 6.599999904632568|  null| 6.599999904632568|   null|\n",
      "|1992-03-08 21:00:00|  null| 7.099999904632568|  null| 7.950000286102295|   null|\n",
      "|1992-08-30 03:00:00|  null|10.199999809265137|  null|10.050000190734863|   null|\n",
      "|1992-11-22 12:00:00|  null|              12.5|  null|12.150000095367432|   null|\n",
      "|1993-04-13 18:00:00|  null|3.8499999046325684|  null| 9.650000095367432|   null|\n",
      "|1993-08-26 15:00:00|  null|              14.0|  null|15.799999713897705|   null|\n",
      "|1993-12-16 07:00:00|  null| 3.299999952316284|  null| 4.900000095367432|   null|\n",
      "|1995-06-18 18:00:00|  null|              14.0|  null|13.349999904632568|   null|\n",
      "|1998-07-29 05:00:00|  null|              13.5|  null|12.300000190734863|   null|\n",
      "|1998-08-29 09:00:00|  null|              15.5|  null| 13.40000025431315|   null|\n",
      "|1998-09-03 14:00:00|  null|17.149999618530273|  null|16.933333079020183|   null|\n",
      "|1999-07-26 09:00:00|  null|14.550000190734863|  null|15.600000063578287|   null|\n",
      "|1999-08-08 13:00:00|  null|16.149999618530273|  null|17.166666984558105|   null|\n",
      "|2001-06-29 23:00:00|  null|13.850000381469727|  null|13.466666539510092|   null|\n",
      "|2001-12-19 22:00:00|  null| 4.799999952316284|  null|5.3666666348775225|   null|\n",
      "+-------------------+------+------------------+------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We first use the groupby() method to group the data, followed by the pivot() method.\n",
    "# Finally, we call an aggregation function.\n",
    "# Select a subset of counties to improve the display.\n",
    "\n",
    "output_df.groupBy('Date').pivot('County').avg('Temperature (°C)').select('Date', 'Carlow', 'Cork', 'Galway', 'Mayo', 'Wexford').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a20eqVA147DH"
   },
   "source": [
    "That's a wrap!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WG0Co6r47DI"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this train, we looked at the second half of the process of performing a data engineering task from a processing perspective. We re-ingested, learned how to deal with incomplete data, and also looked at outputting to other teams.\n",
    "\n",
    "We provided a lot of information on data cleaning and transformation that is necessary to be performed when doing data engineering and showed how it's implemented in Apache Spark. To stay relevant and maintain fluency, we recommend visiting the [Apache Spark documentation](https://spark.apache.org/docs/latest/) regularly for new functionality and feature releases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
