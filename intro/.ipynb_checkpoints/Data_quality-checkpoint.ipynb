{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality and Testing\n",
    "© Explore Data Science Academy \n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you will: \n",
    "* understand the need for data quality validation and testing, including:\n",
    "   * data quality validation as an assurance of incoming data; and\n",
    "   * data testing as integration testing;\n",
    "* define the six dimensions of data quality;\n",
    "* be able to use Spark to calculate each of the six dimensions; and\n",
    "* understand the tools available for data quality validation and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the need for data quality validation and testing\n",
    "\n",
    "Data quality is a crucial part of data engineering. After ingesting data into your system, the next important thing is to ensure that the data is of sufficient quality to allow for high-quality and robust science and analytics. \n",
    "\n",
    "Imagine you build the most efficient, robust, and elegant data pipelines to process your data. All this work would be for nothing if your data is not an accurate representation of real-world events. Therefore, you must prioritise data quality from the start of your project. \n",
    "\n",
    "There are a few cases that may occur when your data quality is not up to scratch: \n",
    "\n",
    "- Your code checks the input data with expectations based on data that you know is correct. If the data is not up to scratch, then you initiate a plan B. This could include re-running the API to retrieve the missing data or alerting users and data engineers to the problem.\n",
    "\n",
    "- The code runs into an error while processing data and crashes. This may occur when your data pipelines expect a specific schema or data type.\n",
    "\n",
    "- Lastly, your data pipelines could continue to run regardless of the poor data quality, producing outputs that look correct but are based on substandard data.\n",
    "\n",
    "Of these three cases, the first one is the best, as we are at least being alerted to changes in data quality. This is the only case where we have data quality checking in place. The second case is not the end of the world – with a failure you are at least aware that something has failed and can look for a solution or re-run your data pipeline. The last case is very dangerous as you could remain unaware of these data quality issues for months. \n",
    "\n",
    "Today's applications and businesses, in general, are gravitating towards data-driven products and decision-making. As you know, there are huge amounts of data available and this is only increasing. Businesses are becoming more dependent on using data and making informed decisions. Therefore, substandard data is not an option, as it can lead to incorrect modelling or analytics.\n",
    "\n",
    "In the sections below, we are going to take a deeper look at data quality and dive into some of the use cases for data quality checks. We'll also introduce six dimensions of data quality that can be evaluated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data quality validation as an assurance of incoming data\n",
    "\n",
    "Data quality validation means making sure that the incoming data is of sufficient quality.\n",
    "\n",
    "Here we will expand on how to validate the quality of the incoming data and show which mitigation should take place when there is a change in data quality. Two important factors need to be monitored here. The first is the change in data quality, such as missing values or duplicates. The second is a change in data distributions where values are outside the expected range.\n",
    "\n",
    "Let's take a look at how to approach data quality validation. The first step in tracking data quality is deciding on the *data requirements*:\n",
    "* Defining the schema and acceptable data types of a dataset ensures that changes to either of these do not negatively affect downstream processes, such as modelling and analysis.\n",
    "* Data requirements can also include the six dimensions of data quality, which can direct decisions around data requirements.\n",
    "* It could also include defining the bounds of what qualifies as a “normal” value, for example, one standard deviation from the mean historical value.\n",
    "\n",
    "Once you have decided on the data requirements, you must automatically check that these requirements are met. This may look like a data quality pipeline running once a day or running every time new data are ingested which tests for the defined data quality requirements. This process could also include an alerting system that lets you (as the data engineer) know about violations in the data requirements.\n",
    "\n",
    "Data quality is likely to vary depending on the source system. It cannot necessarily be changed, but you can ensure that the correct systems are in place to continuously mitigate the risks associated with substandard data. This may be in the form of a data quality rating displayed on the front-end of your application so that users know when data are unreliable.\n",
    "\n",
    "Another option is creating a system that checks incoming data based on requirements and activates a recovery pipeline that re-ingests data if the faults are due to the ingestion step. These mitigation options aim to ensure awareness around data quality and that you are not falling into the third case mentioned above, where your data pipelines continue to run regardless of substandard data.\n",
    "\n",
    "> Remember that mitigation plans for poor data quality are highly dependent on the requirements. \\\n",
    "> Therefore, you should treat each new dataset as a new problem to ensure no additional faults come into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data testing as integration testing\n",
    "\n",
    "Testing is the second use for data quality. Here we use data quality as a method for integration testing. Thus, monitoring data quality through a processing pipeline on a mock dataset can tell us if the pipelines are performing as they should.\n",
    "\n",
    "This approach takes on a more rigorous design than ensuring that incoming data is accurate. Once we are sure that incoming data is up to the correct standard, there are more places where data testing and validation can be included. Following the logic of ETL, once we have extracted the data and ensured its quality, there are likely many transformations that need to be built into the data pipelines based on various business requirements. \n",
    "\n",
    "*Consider the following example:* \\\n",
    "A business receives data every hour for the number of times a pump within a water distribution network has started in its lifetime. This metric is useful as it can help the business determine how close a pump is to the end of its life (water pumps only effectively pump water for a set amount of pump starts). This data can provide a further benefit by calculating the number of times a pump has started in a day, which can alert the business to where pressure on their system is.\n",
    "\n",
    "To calculate the number of times a pump has started in a day, it is necessary to keep track of the previous days' cumulative number of starts and use this as a baseline for that day's number of starts. This is where data testing can be taken to a new level.\n",
    "\n",
    "If we know the method of calculating the number of starts per day, and we have a mock dataset for which we know the number of starts that occurred, we can create a testing script to validate that the calculation is performing as expected. This involves providing the function that performs the calculation with known inputs and outputs, and testing if it passes or fails the test, so we can know with certainty that our function performs correctly. It is very similar to how we would test general software functionality.\n",
    "\n",
    "> This is an example of integration testing for one function or transformation, but these types of checks should ideally be made at every point that your data changes form. \\\n",
    "> By employing this type of testing, you can safely make changes and refactor your code while still having the tests in place to validate that you haven't broken anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The six dimensions of data quality\n",
    "\n",
    "The six dimensions of data quality are generalised attributes that can be used to measure data quality. These dimensions provide a framework within which you can explore data and understand where pitfalls may be, as well as how to address data quality problems. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/421d8c55ebe6caa30836ba3c5785232d3eab84ad/data_engineering/transform/predict/DataQuality.jpg?raw=True\"\n",
    "     alt=\"six dimensions of data quality\"\n",
    "     style=\"padding-bottom=0.5em\"\n",
    "     width=800px/>\n",
    "     <br>\n",
    "     <em>The six dimensions of data quality. </em>\n",
    "</p>\n",
    "\n",
    "In some cases, specific dimensions will not apply to a dataset and, therefore, may not be useful, however, it is important to go through all six dimensions for each use case. It is, therefore, useful to have a standard approach to apply to new datasets. If there are additional requirements, we can build these on top of the six dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An overview of the six dimensions of data quality \n",
    "\n",
    "Each of the six dimensions should give you an idea of how to assess your data's quality. \n",
    "\n",
    "#### Accuracy \n",
    "\n",
    "The accuracy of data is the degree to which the data represent a real-world event or object.\n",
    "\n",
    "Sometimes IoT devices record data measurements that are incorrect but may look like a real value. An example of this is monitors that measure the level of fluid in a tank or pipe. These monitors often use an ultrasonic pulse that bounces off the fluid to measure the level. However, if the sensor is underwater or obstructed, it can give skewed readings. Another example is an API sending incorrect data or ingesting the data from the source incorrectly, such as a rounding error.\n",
    "\n",
    "Accuracy issues can occur at the field level (one incorrect entry) or the row level (multiple incorrect rows).\n",
    "\n",
    "Another issue that may occur is default values. A typical example of this is where a logger sends back a 0 instead of a null value, which can greatly skew any attempts at modelling. This is where it is instrumental to employ domain knowledge when assessing a dataset. \n",
    "\n",
    "- *Measured by*: how correct a data point is relative to a real-world event\n",
    "- *Units*: specific case-related constraints\n",
    "- *Related to*: validity and completeness\n",
    "\n",
    "#### Consistency \n",
    "\n",
    "Consistency is the absence of difference when comparing two or more representations of something against a reference. If data are recorded or captured in multiple places, consistency becomes very important. One cannot have the same data point recorded in various ways. \n",
    "\n",
    "Data entries that refer to the same record or entity have to be consistent across all of the entries. An example of a consistency error is measuring data in different units but using the same column. For example, temperature data that are recorded in Kelvin and degrees Celsius but in the same column. This could greatly impact data users if they are unaware of what normal temperatures should look like.\n",
    "\n",
    "This is not just within a single table. It becomes more important if you are dealing with relational data, in which case the mappings between tables and systems must be consistent. If not, the relationships will be completely lost between the tables and referential integrity compromised. \n",
    "\n",
    "- *Measured by*: analysis of pattern and/or value frequency\n",
    "- *Units*: percentage\n",
    "- *Related to*: accuracy, validity, and uniqueness\n",
    "\n",
    "#### Timeliness \n",
    "\n",
    "Timeliness is the degree to which data represent reality from the required point in time. Timeliness expects that the data within your dataset is sufficiently up to date. What are the delays between an event happening and the data point being recorded?\n",
    "\n",
    "If you are trying to answer questions that relate to recent problems, having timely data is extremely important. For example, you cannot use current flight patterns to model how many aeroplanes will be needed by a large aeronautics company in the next five to ten years. \n",
    "\n",
    "Similarly, when answering questions that require real-time answers (for example, predicting when a pipe will burst in a manufacturing plant), you have to be set up to receive real-time data from sensors and loggers. \n",
    "\n",
    "An example of untimely data causing problems is where a temperature measuring device on a nuclear reactor only communicates its reading two hours after the temperature is recorded. If the temperature exceeds the safety threshold, it could lead to a nuclear meltdown in the reactor. Therefore, it is crucial that the data are sent as soon as it is recorded and that the data pipelines can communicate this data point as fast as possible. \n",
    "\n",
    "Another important point is that timeliness is sometimes determined by the source system, which may only synchronise once a day. In this case, we may have a fast method of ingesting the data and displaying it, but we are limited by the source system.\n",
    "\n",
    "- *Measured by*: time difference\n",
    "- *Units*: time\n",
    "- *Related to*: accuracy, because it will decay as time progresses\n",
    "\n",
    "#### Validity \n",
    "\n",
    "The validity of a dataset is specific to a certain field. In other words, data is valid if it conforms to the syntax (format, type, range) of its definition. Each field will have a property that makes it valid, such as an \"@\" symbol for an email address.\n",
    "\n",
    "Certain values within a field may have criteria required to make it valid, for example, numerical columns cannot contain alphabetical characters, which can be the result of incorrectly parsed scientific notation. This can be more difficult to determine in strings, in which case you may have to check using regex. This could be the inclusion of a symbol, as mentioned above, or it could be that an integer should not exceed certain limits. An example of this is in a gas separation process that uses extremely cold temperatures. If the system records temperatures below -273 degrees Celsius, which is absolute zero, the data would not be valid.\n",
    "\n",
    "- *Measured by*: comparison between the data and the metadata or documentation for the data item\n",
    "- *Units*: percentage of data items deemed valid or invalid\n",
    "- *Related to*: accuracy, completeness, consistency, and uniqueness\n",
    "\n",
    "#### Completeness\n",
    "\n",
    "The completeness of data relates to how many values may be missing in your dataset. \n",
    "\n",
    "The degree of completeness can be measured by seeing how many theoretical data points should be present, based on timestamps. Once a theoretical maximum of how complete a dataset should be is established, you can determine how complete the dataset is based upon the ratio of missing records to theoretical completeness.\n",
    "\n",
    "An example of incomplete data is if you are receiving pump flow readings every two minutes, but you do not have 720 data points per day. If you have 600 data points associated with a specific timestamp, it means that you are missing 120 data points/timestamps. Being aware of this is particularly important in cases where you are aggregating the data to a less frequent interval, because you can still get an hourly mean with less than 30 two-minute data points but it will not be as accurate as with all 30 data points.\n",
    "\n",
    "The questions one could ask to determine the completeness of a dataset are, 'Does the dataset have missing values, or if it is time-series data, does it have time period gaps? Has a bias been introduced that may change your assumptions or affect your results?'\n",
    "\n",
    "Completeness issues can occur at the field level (one entry missing) or at the row level (gaps within the dataset). This can also happen at the field level with entire fields being empty or more than 80% of a field's data being missing. Another issue that may occur is default values. A typical example of this is where a logger sends back a 0 instead of a null value, which can greatly skew any attempts at modelling. This is where it is instrumental to employ domain knowledge when assessing a dataset. \n",
    "\n",
    "- *Measured by*: a measure of the absence of blank (null) values or the presence of non-blank values\n",
    "- *Units*: percentage\n",
    "- *Related to*: validity and accuracy\n",
    "\n",
    "#### Uniqueness\n",
    "\n",
    "This dimension relates to having a real-world object or event represented only once in a particular dataset. The same object cannot be duplicated. In other words, uniqueness specifies that nothing will be recorded more than once based upon how that thing is identified. It is the inverse of an assessment of the level of duplication.\n",
    "\n",
    "Each entry within the datasets should only relate to one single event which has occurred and, thus, should not be duplicated. This is largely mediated by having the appropriate primary key, which is ensured if you stick to the requirements of a good primary key. All fields in the tables should be non-transitively dependent on the primary key. As such, deduplication of the dataset may be required. \n",
    "\n",
    "This dimension of data quality can lead to very frustrating situations. An example is having a replication of a set of asset IDs that relate to pumps. Each asset has a flow measurement device, and each has a theoretical maximum that it can pump. If an asset's name is recorded as both ADZZ-001 and adzz-001, data could be recorded multiple times for the same pump. The theoretical maximum pumping limit could also be reached for an asset that then kicks off a process to alert users to the problem. This could lead to users receiving double the number of alarms or alerts and potentially causing more stress than is necessary.\n",
    "\n",
    "- *Measured by*: analysis of the number of things assessed in the “real world” compared to the number of records of things in the dataset – it requires a reference dataset which is the ground truth\n",
    "- *Units*: percentage\n",
    "- *Related to*: consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark to calculate the six dimensions \n",
    "\n",
    "In this section, we will look at how we can apply some of the dimensions of data quality on a dataset using Apache Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "46ab92fa-2b91-4efc-8933-b82a252a1d2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The dataset we have chosen for this analysis is based on the daily stock market fluctuation for eight different indexes. Below we have an explanation of the column names and their relevant indexes.\n",
    "\n",
    "* TL BASED ISE: Turkish Lira: Istanbul stock exchange national 100 index\n",
    "* USD BASED ISE: USD Istanbul stock exchange national 100 index\n",
    "* SP: Standard & Poor's 500 return index\n",
    "* DAX: Stock market return index of Germany\n",
    "* FTSE: Stock market return index of UK\n",
    "* NIKKEI: Stock market return index of Japan\n",
    "* BOVESPA: Stock market return index of Brazil\n",
    "* EU: MSCI European index\n",
    "* EM: MSCI emerging markets index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get you started, below are the required imports, SparkSession setup, schema definition, and reading of the `csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the below line and execute the cell if you do not have the 'requests' module installed in your environment.\n",
    "#! pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "17f37996-e9f5-4cd0-aa37-d9c4b7d610e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import Spark and some auxiliary functions.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a53f4260-20c3-434b-84ff-efdcf48a2403",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up a SparkSession.\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema.\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"date\", StringType(),True), \\\n",
    "    StructField(\"TL BASED ISE\", DoubleType(),True), \\\n",
    "    StructField(\"USD BASED ISE\", DoubleType(),True), \\\n",
    "    StructField(\"SP\", DoubleType(),True), \\\n",
    "    StructField(\"DAX\", DoubleType(), True), \\\n",
    "    StructField(\"FTSE\", DoubleType(), True), \\\n",
    "    StructField(\"NIKKEI\", DoubleType(), True), \\\n",
    "    StructField(\"BOVESPA\", DoubleType(), True), \\\n",
    "    StructField(\"EU\", DoubleType(), True), \\\n",
    "    StructField(\"EM\", DoubleType(), True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "03a227ee-793e-4747-b0cb-fb46113a4e26",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the data from a csv file.\n",
    "file_url = 'https://raw.githubusercontent.com/Explore-AI/Public-Data/master/data-engineering/procesing-big-data/istanbul_stock_exchange.csv'\n",
    "response = requests.get(file_url)\n",
    "open('istanbul_stock_exchange.csv','wb').write(response.content)\n",
    "df = spark.read.csv(f'istanbul_stock_exchange.csv', schema=schema, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0716de1d-c442-477f-b5d9-866468b33249",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data accuracy\n",
    "\n",
    "Let's start by looking into *data accuracy*, which is the degree to which a piece of data accurately describes a real-world event.\n",
    "\n",
    "To determine the accuracy of the dataset, let's have a look at the following:\n",
    "1. View the DataFrame to get a general idea of how the dataset looks.\n",
    "2. Summarise the DataFrame to further explore the data, including identifying if we're dealing with missing data.\n",
    "3. Investigate the distribution of the different columns.\n",
    "4. Filter specific columns to explore the outliers in the distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    "> View the DataFrame using `show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you also see that null and zero values are included in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    "> Summarise the DataFrame by applying the `summary()` function, and using the `toPandas()` function to see the summary as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1ec94e1b-c59a-4887-9dff-1f5327c0fae4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "By viewing the DataFrame, we already identified a few null and zero values. In the summary you should have seen that the count of some columns is below 539, indicating that we do have missing values.\n",
    "\n",
    "Next, we investigate the distribution of the different columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    "> Convert the DataFrame to a Pandas DataFrame using the `toPandas()` function so that we can make use of the plotting functions of matplotlib. Plot each of the nine columns using the `hist()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plots, you should see that four of the nine histogram distributions appear to be approximately normal. However, the five other variables have a concentrated distribution with some extreme outliers. Since the dataset is based on stock market fluctuations in a day, it is possible to have large negative and positive values. However, it is unlikely for one variable to get a value indicating either positive or negative shifts of more than 100% (1 on the index). Therefore, we potentially have some questionable data in the above dataset.\n",
    "\n",
    "Next, we filter the variables that are not normally distributed, which are columns `SP`, `FTSE`, `EM`, `NIKKEI`, and `BOVESPA`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    "> Filter the five columns that are not normally distributed by using `where()` and the Spark SQL function `F.col()` to inspect the outliers. Use `<-0.2` to find these outliers in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "22b8f864-c8e3-4936-9c21-e94000551aa1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For variable `SP`, for example, you should have seen that some rows have a value associated with approximately 100% and 300% losses. This is uncommon and would require validation from other data sources to confirm if it is accurate.\n",
    "\n",
    "For the other four variables, you should also have seen values associated with more than 100% change in the stock market prices. These values are indicated by indexes over 1 and will also need to be validated by other sources. In these cases, additional research through validation with other data is required to determine if the large gains and losses are legitimate. If not possible, it would be wise to drop or filter out these abnormal values since they can significantly impact the results of building a machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a59af52a-e4d6-415e-9f10-f2d65899367b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Completeness\n",
    "\n",
    "Let's use the same dataset and determine its completeness, in other words, what the proportion of data against the potential of “100% complete” is.\n",
    "\n",
    "To determine the accuracy of the dataset, let's have a look at the following:\n",
    "1. Determine the percentage of missing (null) and zero values within each column.\n",
    "2. Explore the columns that contain missing and zero values.\n",
    "3. Remedy the incomplete data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d0214c94-e14b-4a9a-ac86-f1a91b5cb351",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We already know that null values are included within the dataset, but let's explore further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    "> Check for missing values within the columns. \\\n",
    "> We've given you a `for` loop below to loop through each of the columns:\n",
    "> * `where()`, `isNull()`, and `count()` are used to count the number of null values in the column.\n",
    "> * `select()` and `count()` are used to count the total number of values in the column.\n",
    "> * The number of null values is divided by the total number of values and multiplied by 100 to calculate the percentage of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "81e79194-65a1-4ac5-809a-9187c71a47ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment, to check for missing values within the columns.\n",
    "\n",
    "#missing_count = {}  # Dictionary to keep track of the results\n",
    "#for column in df.columns:   # loop through each column\n",
    "    #_count = df.where(df[column].isNull()).count()  # null count in column x\n",
    "    #_total_count = df.select(df[column]).count()    # total count of column x \n",
    "    #print(f'There are {_count} ({round(_count/_total_count*100, 3)}%) null values in {column} column')  # print out and calculate results\n",
    "    #missing_count[f'{column}'] = _count # recording results in missing_count dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above summary, you should see that only the variable `DAX` has missing (null) values. Let's explore further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    "> Filter and display the rows of variable `DAX` that have a value of null using `where()`, `isNull()`, and `show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to decide how to deal with this incomplete data.\n",
    "\n",
    "In general, we can remedy incomplete data by:\n",
    "* imputing the missing values;\n",
    "* dropping missing values;\n",
    "* discarding the incomplete column; or\n",
    "* discarding the rows containing missing data.\n",
    "\n",
    "Because the missing data in variable `DAX` seem random, imputation will be difficult. So, in this case, we will simply remove the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    "> Assign the filter for variable `DAX` above to a new DataFrame, and `show()` this new DataFrame that does not contain null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above DataFrame should not contain any null values. If that is the case then you've succeeded in remedying the incomplete data from null values!\n",
    "\n",
    "Next, we follow the same instructions, but now, we're looking for zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    "> Check for zeros within the columns. \\\n",
    "> You can use the previous loop for missing values as a guide, and uncomment the below code block to get going:\n",
    "> * Use `where()`, `F.lit(0)`, and `count()` to count the number of zeros in the column.\n",
    "> * Use `select()` and `count()` to count the total number of values in the column.\n",
    "> * Divide the number of zeros by the total number of values, and multiply that by 100 to calculate the percentage of zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dfe0673b-545d-448a-8171-dadbf3dd1886",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Uncomment the given code lines, and complete the descriptions.\n",
    "# Check for zeroes within the columns.\n",
    "\n",
    "# Define a dictionary to keep track of the results.\n",
    "\n",
    "#for column in df.columns: # loop through each column\n",
    "#    try:\n",
    "        # count the number of zeros in column\n",
    "        # count the total number of values in column\n",
    "        # calculate and print the results\n",
    "        # recording results in the dictionary \n",
    "#    except: # catch exceptions\n",
    "#        print(f'Column is not numeric: {[dtype for name, dtype in df.dtypes if name == column][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that columns `SP`, `DAX`, `FTSE`, and `NIKKEI` contain zeros.\n",
    "\n",
    "Let's have a further look at each of these columns and drop the rows containing zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    "> Filter, show, and drop the rows in each of the columns `SP`, `DAX`, `FTSE`, and `NIKKEI` that have a value equal to zero:\n",
    "> * Filter the column using the `where()` function and `show()` all the rows for that specific column that are equal to zero.\n",
    "> * You can also use `where()`, `groupby()`, `count()`, and `show()` to summarise the number of zeros per date for each column.\n",
    "> * Filter the column using `where()` for rows that are not equal to zero and `show()` to determine if all zeros from that specific column have been dropped successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SP:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f894c439-4c01-40b4-8838-820123a21588",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*DAX:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e6bb8e64-56dd-429b-b169-2185163c0f84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*FTSE:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "31b0effd-074b-4d4a-944b-a5c53141fc04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f900ac79-cd38-431d-9455-00a2de4a4496",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*NIKKEI:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5a940a72-2afa-4936-bb60-2444bfe2ffbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Write your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cf32048e-1c89-4d8c-be21-f89988c9d019",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Uncomment to count the number of zero values for variable `NIKKEI` per date.\n",
    "#df.where(df['NIKKEI'] == 0).groupby('date').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4d2c95a3-add6-4260-93df-cbec343192f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For variable `NIKKEI`, it seems like quite a few zeros are included. Having no change in the index does not make sense unless no trading has happened that day. Therefore, it requires further investigation to know if these values can be dropped.\n",
    "\n",
    "It is possible to have no trading on a day another index was recorded, for example, if there is a public holiday and the stock markets did not open that day. This requires investigation into what happened in the country of origin for that specific index. Once you know what happened on that particular date, it becomes a question of if you want to keep the zeros or drop them. This decision may only be made based on your cases-specific application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "800b03b5-f1b9-48cd-852f-a0dc99741cee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Validity\n",
    "\n",
    "Let's look at the data's validity and if it conforms to the syntax (format, type, range) of its definition.\n",
    "\n",
    "To determine the validity, let us have a look at the following:\n",
    "1. Define what we expect from the dataset and compare it to the current schema.\n",
    "2. Change the `date` column from string to `DateType()`.\n",
    "3. Find the first and last date within the dataset.\n",
    "4. Determine if all dates included in the dataset are in the past.\n",
    "5. Determine if all numeric values fall within the expected range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0a848599-8454-44c6-ac83-081626f0f6b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We need to first define what we expect from our dataset:\n",
    "\n",
    "- date: timestamp (nullable = true)\n",
    "- TL BASED ISE: double (nullable = true) \n",
    "- USD BASED ISE: double (nullable = true) \n",
    "- SP: double (nullable = true) \n",
    "- DAX: double (nullable = true)\n",
    "- FTSE: double (nullable = true)\n",
    "- NIKKEI: double (nullable = true)\n",
    "- BOVESPA: double (nullable = true)\n",
    "- EU: double (nullable = true)\n",
    "- EM: double (nullable = true)\n",
    "\n",
    "The date column should conform to a date format and be in the past, while in the other columns the value should mostly be between -1 and 1 (because over +100% or -100% gain or loss is improbable in the market)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ℹ️ **Instructions** ℹ️\n",
    ">\n",
    "> Display the data schema of the DataFrame using `printSchema()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "94cf755b-7bb3-479a-9aca-017c2f67d258",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "33e5e150-a2ca-4891-9a9b-4e517e239a3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You should note here that the date column is still a string, and not a date type as required. Let's see if we can convert it to a date.\n",
    "\n",
    "Because the dates within the dataset are formatted as day and month as the locale's abbreviated name, and the first two digits of the year, we'd rather use a Spark user-defined function (UDF) to convert `StringType()` into `DateType()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "648c46be-d42e-41ca-8515-1fd1be0ba8af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a function that translates a string date into date type.\n",
    "\n",
    "def format_date(date_column) -> DataFrame:\n",
    "  \"\"\"\n",
    "  Converts the date column from a string to a\n",
    "  timestamp that is compatible with Spark.\n",
    "  \n",
    "  Params\n",
    "  ======\n",
    "  date_column: The target column to be \n",
    "  formatted.\n",
    "  \n",
    "  Returns\n",
    "  ======\n",
    "  date_object: The correctly formatted datetime object\n",
    "  \"\"\"\n",
    "  return datetime.strptime(date_column,\"%d-%b-%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "28c409f1-ac48-4f72-95fe-df2a52df0686",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment.\n",
    "\n",
    "# Instantiate the UDF.\n",
    "#date_format = F.udf(format_date, DateType())\n",
    "\n",
    "# Register the UDF (to allow reuse) - with `df` your current DataFrame.\n",
    "#df_date_format = df.withColumn('date', date_format('date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the format of our dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c504ca8b-5275-4a57-af2d-baee7aa5f68e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment.\n",
    "\n",
    "#df_date_format.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1cae6deb-858b-4ac3-8a9f-539d7a399306",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see that the date column has now successfully become a date type!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    "> Display the first ('min') and last ('max') dates of the dataset using the new DataFrame with column `date` in `DateType()`. Use the `agg()` and `show()` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8fb6936c-0976-4997-a46c-6a7254c43f14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    "> Use `where()`, `F.col()`, `datetime.now()`, and `count()` to determine if all dates within the dataset are in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b226526b-9da2-460e-83fe-0d255066f8c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a value of zero because all the dates within the dataset have occurred in the past.\n",
    "\n",
    "Next, we check that all values in the numerical columns are within the expected range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    "> Complete the below code block to confirm the number of numeric outliers, using `filter()`, `F.col()`, and `count()`. \\\n",
    "> We suggest bounds of +- 0.2, as most indexes do not show more than 20% change, based on the summary statistics completed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bbfee45f-4e9f-423a-aff3-b8fe6f6ea70e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Uncomment the provided code.\n",
    " \n",
    "# Define the boundary values within which the values are expected to fall.\n",
    "lower_bound = -0.2\n",
    "upper_bound = 0.2\n",
    "\n",
    "# Add all the columns except the `date` column to a list.\n",
    "#numerics = [x for x in df.columns if x not in 'date']\n",
    "\n",
    "#for numeric in numerics: #iterate through the numeric columns\n",
    "    # count the number of values less than the lower bound\n",
    "    # count the number of values greater than the upper bound\n",
    "    #print(f'Column {numeric} has {count_lower} less than {lower_bound} and {count_greater} greater than {upper_bound}') #calculate and print the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c2205ec8-a264-48ff-ba4c-f4b34b4f4e77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we can see, this investigation confirms what we initially saw in the distribution plots. We have numeric values, which means we can work with our dataset in its current form but there are outliers that should potentially be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d8d16287-8d34-4f4d-89de-171999abe587",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Try it on your own\n",
    "\n",
    "We've guided you through the application of three of the six dimensions using Spark. We strongly encourage you to investigate and try applying all six dimensions in Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of tools available for data quality validation and testing\n",
    "\n",
    "### Manual testing\n",
    "\n",
    "Manual testing can be the first step in the journey of having robust data quality tests in place. This involves running manual checks, for example, calculating means and missing values using built-in Python, Spark, or Pandas methods. This can help to familiarise you with your data to decide what type of constraints may be useful.\n",
    "\n",
    "*Pros:*\n",
    "\n",
    "- It is the easiest to implement since there is no barrier of entry as you are already familiar with the toolset.\n",
    "- It works from first principles, so you can implement exactly the tests you plan to do.\n",
    "- This method can be useful in informing constraints for automated testing.\n",
    "\n",
    "*Cons:*\n",
    "\n",
    "- This approach may be the most inefficient since it is not being optimised computationally.\n",
    "- It may be time-consuming, and any new dataset will have to be implemented from scratch.\n",
    "- You do not get defined metrics out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated testing\n",
    "\n",
    "#### Deequ\n",
    "\n",
    "Deequ is a library that has been built on top of Apache Spark. Its purpose is to define \"unit tests for data\" so that you can measure the quality of large datasets. \n",
    "\n",
    "If using Deequ within a Python environment, there is a library called [PyDeequ](https://pydeequ.readthedocs.io/en/latest/README.html). There are four main components of Deequ:\n",
    "\n",
    "- Metrics Computations:\n",
    "   * `Profilers` leverage Analyzers to inspect the columns of your dataset.\n",
    "   * `Analyzers` are foundational modules used to compute metrics for the profiling and validation of data at scale.\n",
    "\n",
    "- Constraint Suggestion:\n",
    "   * This is where the rules and constraints for the groups of Analyzers are defined. These are run over the datasets and returned to a Verification Suite.\n",
    "\n",
    "- Constraint Verification:\n",
    "   * Here data are validated based on the constraints that have been set.\n",
    "\n",
    "- Metrics Repository:\n",
    "   * The Metrics Repository allows you to save Deequ metrics and runs over time.\n",
    "\n",
    "So, if we take a look at how these four components fit together into a flow, we will get something like this:\n",
    "1. Analyzers are used to compute metrics for the data.\n",
    "2. The Profiler is run on each column of the data to get a better understanding of the data.\n",
    "3. The Constraint Suggestion step occurs where PyDeequ gives you a set of default constraints that it thinks will be useful. Here you would also modify the constraints to your specific use case as the defaults are likely to be very basic.\n",
    "4. The Constraint Verification step happens when the constraints set in the previous step that make up the Verification Suite are validated. This allows you to see if the data quality passes or fails.\n",
    "5. Once the Constraint Verification step has occurred, you can save the results of various runs to the Metrics Repository. This can be very helpful as you get a historical view of all the data quality checks that have been done on your data. \n",
    "\n",
    "This whole process can be integrated within your ETL pipeline at multiple steps. You could have a system that kicks off to check the source data ingested as well as a system that ensures the transformations are occurring correctly.\n",
    "\n",
    "Deequ has a rich set of checks that are available to choose from, and if you ever find yourself having to implement data quality testing with Deequ, you can find all the methods available [here](https://pydeequ.readthedocs.io/en/latest/pydeequ.html#module-pydeequ.checks). Some useful examples include defining the minimum and maximum allowed values for a column as well as checking for data completeness. You can even do checks for correlation between two columns using the hasCorrelation() or hasMutualInformation() methods. \n",
    "\n",
    "> To start using PyDeequ, all you need to do is run:\n",
    ">\n",
    "> `$ pip install pydeequ`\n",
    "\n",
    "For a closer look at how to use PyDeequ, take a look at the [quick start](https://pydeequ.readthedocs.io/en/latest/README.html#quickstart).\n",
    "\n",
    "*Pros:*\n",
    "\n",
    "- This tool is specifically made for data quality testing and tracking.\n",
    "- Seamless integration with Spark as it is built on top of Spark.\n",
    "- Wide variety of built-in functions for data quality testing.\n",
    "- Deequ provides a Profiler to automatically determine tests for your dataset using historical data.\n",
    "\n",
    "*Cons:*\n",
    "\n",
    "- Requires an Apache Spark environment (which is not really applicable to small datasets).\n",
    "\n",
    "> 🚩️**Student instructions**🚩️\n",
    ">\n",
    "> Install `pydeequ` and run some of the data quality checks that were performed in the above sections. `Pydeequ` is very sensitive to the PySpark version being used. To run `pydeequ` successfully, you are advised to [create a new environment](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html#managing-environments) and install PySpark version 3.0.\n",
    ">\n",
    "> When creating a new environment, make sure the version of Python you are installing matches the version you are running on your machine to avoid version conflicts between your Spark driver and executors. Once you have created a new environment you can execute the below command to install the appropriate version of PySpark:\n",
    ">\n",
    ">`pip install pyspark==3.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great Expectations\n",
    "\n",
    "Great Expectations is one of the leading tools that can be used for validating, documenting, and profiling your data. \n",
    "\n",
    "It is a Python package that can be installed using the following commands:\n",
    "\n",
    "> `$ pip install great_expectations`\n",
    ">\n",
    "> `$ great_expectations init` \n",
    "\n",
    "The first command installs Great Expectations and the second command initialises your Great Expectations deployment for a new project. This means that Great Expectations will create a new directory with the following structure:\n",
    "\n",
    "great_expectations/\n",
    "   - great_expectations.yml\n",
    "   - expectations\n",
    "   - checkpoints\n",
    "   - notebooks\n",
    "   - plugins\n",
    "   - .gitignore\n",
    "   - uncommitted/\n",
    "      - config_variables.yml\n",
    "      - documentations\n",
    "      - validations\n",
    "\n",
    "\n",
    "The team from Great Expectations recommend deploying this setup within a virtual environment. Details on how to set up this virtual environment can be found [here](https://legacy.docs.greatexpectations.io/en/latest/reference/supporting_resources.html).\n",
    "\n",
    "Great Expectations has the goal of helping you understand your data better so that communicating what you have built and expect becomes effortless. Three features are delivered to help you achieve this goal. Firstly, data are automatically profiled to build the Expectations. Next, Expectations are used to validate the data quality, and then this gets pushed automatically to Data Docs to give you an idea of your data's characteristics. \n",
    "\n",
    "Let's clarify how this works in a bit more detail. What exactly is an Expectation? An Expectation is what you will use to define how your data should look. Expectations are built using Profilers which learn about your data. A Profiler is what Great Expectations uses to analyse your dataset and generate a first pass set of Expectations. Once the Expectations have been built, one can validate new data using the Expectations that have been generated. You also have the ability to update the Expectations that have been auto-generated to home in on your dataset's requirements. Data Docs use Expectations to describe your data and diagnose problems by giving you a visual report on which constraints have been met. \n",
    "\n",
    "\n",
    "*Pros:*\n",
    "\n",
    "- Auto-generated Expectations out of the box that can be modified as you learn about your data.\n",
    "- High configurability (allows for very accurate studies of your data).\n",
    "- Configurable data source connections.\n",
    "- Can be run externally in a virtual environment, operating directly on the data and not requiring a Python or Spark environment.\n",
    "\n",
    "*Cons:*\n",
    "\n",
    "- Can be difficult to set up due to the high level of configurability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delta Expectations (Live Delta Tables)\n",
    "\n",
    "Delta Expectations is a data quality tool built specifically for Delta Live Tables. Delta Live Tables is a feature of the latest Databricks Runtime and can be used to build reliable, maintainable, and testable data pipelines. The crucial difference here is that, when using Delta Live Tables, you will not be using a series of separate Apache Spark tasks. All you need to do is define the output schema, and Delta Live Tables will manage how the processing steps occur. \n",
    "\n",
    "Delta Expectations allows you to define the expected data quality as well as specify what to do in situations where the records do not meet the specifications. For more information on how to use Delta Expectations, have a look at the [Delta Live Tables quick start](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-quickstart.html).\n",
    "\n",
    "An example of how you would go about building Expectations with Delta Live Tables would start with creating a new notebook and adding code that would implement a data pipeline. Following this step, you will need to create a new pipeline job, and once the job has been completed, you can view the results of this pipeline job. \n",
    "\n",
    "The Expectations would be created within the notebook that makes up a part of your pipeline and will be used to read in the new data and clean it to the desired level. \n",
    "\n",
    "An Expectation may be defined in a similar way to the example below and includes a description, an invariant, and some sort of action to take when the record fails the invariant. \n",
    "\n",
    "'''\n",
    "@dlt.expect(\"valid timestamp\", \"col(“timestamp”) > '2012-01-01'\")\n",
    "'''\n",
    "\n",
    "Here we are expecting the timestamp column to be greater than the 1st of January 2012. There are many other options available for Expectations such as dropping the values that do not meet the criteria using the .expect_or_drop() method. You can also include multiple Expectations such as in this example:\n",
    "\n",
    "'''\n",
    "@dlt.expect_all({\"valid_count\": \"count > 0\", \"valid_current_page\": \"current_page_id IS NOT NULL AND current_page_title IS NOT NULL\"})\n",
    "'''\n",
    "\n",
    "*Pros:*\n",
    "\n",
    "- Leverages new [Lakehouse](https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html) architecture.\n",
    "- Seamless integration with Spark.\n",
    "- Data pipelines can be integrated easily with Expectations.\n",
    "\n",
    "*Cons:*\n",
    "\n",
    "- Delta Expectations is a new technology and is currently not well known in the data engineering community.\n",
    "- Requires Apache Spark environment and Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repurposed tools –  MLFlow\n",
    "\n",
    "Another option is using a client like MLFlow to log the metrics for the tests. This approach allows for logging the data as it comes into storage, effectively creating a very robust data lineage process. However, MLFlow is not technically the correct tool for the process and, therefore, there will be certain instances where it does not work as expected. This is because MLFlow is traditionally used to add structure and reproducibility to the machine learning lifecycle. If logging is your main purpose, other tools can also be used, such as Splunk and Logstash.\n",
    "\n",
    "*Pros:*\n",
    "\n",
    "- One tool for data quality and model quality tracking.\n",
    "- Very commonly used meaning a large development community, which is helpful when trying to solve issues.\n",
    "\n",
    "*Cons:*\n",
    "\n",
    "- The tool is not optimised for storing data quality metrics.\n",
    "- You do not get defined metrics out of the box.\n",
    "- Using packages with specific requirements may complicate environment setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Well done on completing this train! We introduced the concept of data quality as well as methods of handling your data with automated data quality validation and testing. Including these data quality tests can make or break your project.\n",
    "\n",
    "You should now be familiar with the following topics:\n",
    "- Data quality testing and validation\n",
    "- The six dimensions of data quality\n",
    "- How to calculate the six dimensions of data quality\n",
    "- Some of the tools available for automated data quality testing\n",
    "\n",
    "## Resources:\n",
    "\n",
    "* [Data Quality Management](https://towardsdatascience.com/a-comprehensive-framework-for-data-quality-management-b110a0465e83)\n",
    "* [The 6 Dimensions of Data Quality](https://towardsdatascience.com/the-six-dimensions-of-data-quality-and-how-to-deal-with-them-bdcf9a3dba71)\n",
    "* [Automating DQ Checks with Great Expectations](https://urban-institute.medium.com/automating-data-quality-checks-with-great-expectations-f6b7a8e51201)\n",
    "* [Data Quality Libraries](https://medium.com/datamindedbe/data-quality-libraries-the-right-fit-a6564641dfad)\n",
    "* [Apache Griffim](https://griffin.apache.org/)\n",
    "* [Trust Driven Development](https://medium.com/yotpoengineering/tdd-trust-driven-development-in-data-engineering-84b6e680d6ea)\n",
    "* [LakeFS](https://lakefs.io/)\n",
    "* [Understanding Great Expectations](https://medium.com/hashmapinc/understanding-great-expectations-and-how-to-use-it-7754c78962f4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "testing_dq_2",
   "notebookOrigID": 3813857400265206,
   "widgets": {}
  },
  "interpreter": {
   "hash": "7d29f84658e7040ed75db3cfb75fe4448bdb81512bf5939cdc549089737538b6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
